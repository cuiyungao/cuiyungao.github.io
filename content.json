{"meta":{"title":"Cuiyun Gao's Daily Digest","subtitle":"Work Hard, and Play Harder.","description":null,"author":"Cuiyun Gao","url":"http://cuiyungao.github.io"},"pages":[],"posts":[{"title":"Tweet2Vec:Learning Tweet Embeddings Using Character-Level CNN-LSTM Encoder-Decoder","slug":"tweet2vec","date":"2016-08-03T12:41:08.000Z","updated":"2016-08-03T14:08:02.000Z","comments":true,"path":"2016/08/03/tweet2vec/","link":"","permalink":"http://cuiyungao.github.io/2016/08/03/tweet2vec/","excerpt":"SIGIR’16的文章，来自MIT Media Lab。Vosoughi, Soroush, Prashanth Vijayaraghavan, and Deb Roy. “Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM Encoder-Decoder.”","text":"SIGIR’16的文章，来自MIT Media Lab。Vosoughi, Soroush, Prashanth Vijayaraghavan, and Deb Roy. “Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM Encoder-Decoder.” Major Idea作者利用tweet用户的hashtag作为groundtruth，基于Character-level对每一条的embeddings进行学习，对hashtag进行推测，并与已有的word2vec跟Glove进行对比，发现Tweet2Vec的准确率最高。 Techniques基于Character-level，以及作者曾经提出Bi-directional gated recurrent unit (Bi-GRU)’14来学习tweet的表示形式。 Character-level CNN Tweet Model作者采用temporal convolutional和temporal max-pooling operations，即计算一维的输入和输出之间的convolution和pooling函数。给定一个离散的输入函数$f(x)\\in [1,l]\\mapsto \\mathbb{R}$，一个离散的kernel函数$k(x)\\in [1,m]\\mapsto \\mathbb{R}$，stride $s$，$k(x)$和$f(x)$之间的卷积$g(y)\\in [1,(l-m+1)/s]\\mathbb{R}$, 对$f(x)$的pooling操作$h(y)\\in [1, (l-m+1)/s]\\mapsto \\mathbb{R}$：$$g(y) = \\sum_{x=1}^m k(x)\\cdot f(y\\cdot s - x + c) \\h(y) = max_{x=1}^m f(y\\cdot s - x + c)$$其中，$c=m-s+1$是一个补偿常量。tweet的character set包含英语字母，数字，特殊字符和不明确的字符，总共统计了70个字符。每个字符被encode成一个one-hot vector $x_i\\in {0,1}^{70}$，单个tweet的最大长度是150，所以每个tweet被表示成了一个binary matrix $x_{1…150}\\in {0,1}^{150\\times70}$。表示成matix的tweet输入到一个包含4层一维卷基层的deep model。每个卷基层操作采用filter $w\\in \\mathbb{R}^l$来获取n-gram的character feature。一般来说，对于一个tweet $s$，在层$h$的一个特征$c_i$由下面式子生成：$$c_i^{(h)} (s) = g(w^{(h)}\\cdot \\hat{c}_i^{(h-1)} + b^{(h)})$$其中，$\\hat{c}_i^{(0)}=x_{i…i+l-1}$, $b^{(h)}\\in \\mathbb{R}$是h层的bias，$g$是一个rectified linear unit。整体框架如下图所示： Long-Short Term Memory (LSTM)给定一个输入序列$X=(x_1, x_2, …, x_N)$，LSTM计算hidden vector sequence $h=(h_1, h_2, …, h_N)$和output vector sequence $Y=(y_1, y_2, …, y_N)$。每一个时间步骤，一个模块的输出是由一组gates (前一个hidden state $h_{t1}$组成的函数)和当前时间步骤的输入控制的。forget gate $f_t$，input gate $i_t$和output gate $o_t$。这些gates集中决定了当前memory cess $c_t$的过渡和当前的hidden state $h_t$。LSTM的过渡函数定义如下：$$i_t = \\sigma(W_i\\cdot [h_{t-1}, x_t]+b_i) \\f_t = \\sigma(W_f\\cdot [h_{t-1}, x_t]+b_f) \\l_t = tanh(W_l\\cdot [h_{t-1}, x_t]+b_l) \\o_t = \\sigma(W_o\\cdot [h_{t-1}, x_t]+b_o) \\c_t = f_t\\odot c_{t-1} + i_t\\odot l_t \\h_t = o_t\\odot tanh(c_t)$$其中，$\\odot$表示component-wise multiplicaiton。$f_t$控制过去的memory cell要舍弃的信息，而$i_t$控制新的信息储存在current memory cell的程度，$o_t$是基于memory cell $c_t$的输出。1LSTM是学习`long-term denpendencies`，所以在卷基层之后用LSTM学习获取特征的序列中存在的依赖。 在seq-to-seq中，LSTM定义了output上的分布，之后用softmax来序列预测tokens。$$P(Y|X)=\\prod_{t\\in [1,N]} \\frac{exp(g(h_{t-1}, y_t))}{\\sum_{y^{\\prime} exp(g(h_{t1}, y_t^{\\prime}))}}$$其中，g是activation function。 Combined ModelFigure 1呈现了整个encoder-decoder过程。输入是由matrix呈现的tweet，字符由one-hot vector表示。Encoder部分，在Character-level CNN的较高层卷积之后不经过pooling，直接作为LSTM的输入，encoder过程可以表示为：$$H^{conv} = CharCNN(T) \\h_t = LSTM(g_t, h_{t-1}))$$其中，g=H^{conv}是一个特征矩阵，每一行代表LSTM的一个time step，$h_t$是$t$时刻的hidden representation。LSTM作用于$H^{conv}$的每一层，生成下一个序列的embedding。最终输出结果$enc_N$用来表示整条tweet。 Decoder部分，用两层的LSTM作用于encoded representation。每一时间步骤中，字符的预测是：$$P(C_t|\\cdot) = softmax(T_t, h_{t-1})$$其中，$C_t$指的是时间$t$的字符，$T_t$表示时刻$t$的one-hot vector。最终的结果是一个decoded tweet matrix $T^{dec}$，与实际的tweet进行比较，并学习模型的参数。 Experiments实验用于两个分类任务： tweet semantic relatedness和tweet sentiment classification。3 million tweets。感觉数据量也挺小。准确率在0.6~0.7左右。 SummaryCharacter level的deep learning的优势:占用内存少，不需要存储所有word的表示，只需要存储已有的character；不依赖于语言，只需要字符；而且不需要NLP的预处理，比如word segmentation。劣势是：运行速度慢，在文章的实验中，word2vec的速度是tweet2vec的6~7倍，原因是从word变成character输入意味着GRU的输入量变大。Character level可以用在NLP的很多方面，比如named entity recognition, POS tagging, text classification, and language modeling。 文章highlight了tweet2vec的优势，对word segmentation error, spelling mistakes, special characters, interpret emojis, in-vocab tokens非常有效。 Published paper.","categories":[{"name":"papers","slug":"papers","permalink":"http://cuiyungao.github.io/categories/papers/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"},{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"}]},{"title":"Impactful ML Topics at ICML'16","slug":"memory-network","date":"2016-07-04T07:32:54.000Z","updated":"2016-08-03T03:44:02.000Z","comments":true,"path":"2016/07/04/memory-network/","link":"","permalink":"http://cuiyungao.github.io/2016/07/04/memory-network/","excerpt":"Summary from a talk on ICML’16.","text":"Summary from a talk on ICML’16. Deep Residual NetworksResidual networks可以在深度增加的同时提高准确率，可以产生许多问题的representations。适当的weight initialization和batch normalization使得网络可以。 Memory Networks for Language UnderstandingMemory Networks是结合large memory跟可读可写的learning component的一类模型。结合了Reasoning with attention over memory (RAM)。大部分ML有limited memory，针对”low level” tasks，比如object detection。 得到word Embedding的过程是一个encoding的过程。","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"},{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"}]},{"title":"Find Researchers Here","slug":"researcher","date":"2016-07-04T07:27:10.000Z","updated":"2016-07-04T07:32:39.000Z","comments":true,"path":"2016/07/04/researcher/","link":"","permalink":"http://cuiyungao.github.io/2016/07/04/researcher/","excerpt":"With researchers in some fields related to NLP and deep learning contained.","text":"With researchers in some fields related to NLP and deep learning contained. Natural Language Processing Jason WestonLink. Research Scientist at Facebook.","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"},{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"}]},{"title":"Predicting Amazon Ratings Using Neural Networks","slug":"predictingamazon","date":"2016-06-30T09:41:17.000Z","updated":"2016-06-30T09:51:31.000Z","comments":true,"path":"2016/06/30/predictingamazon/","link":"","permalink":"http://cuiyungao.github.io/2016/06/30/predictingamazon/","excerpt":"The paper compares 3 different methods, bag-of-words model, word2vec (using RandomForests) and word2vec (using K Nearest Neighbors).","text":"The paper compares 3 different methods, bag-of-words model, word2vec (using RandomForests) and word2vec (using K Nearest Neighbors). Word vectors经常被用作features应用于许多NLP领域，有两种模型CBOW和Skip-Gram。Doc2vec是word2vec的改进版，区别在于model architecture。在doc2vec中，algorithms是distributed memory (dm) and distributed bag of word (dbow)，但是其分类效果并不如word2vec。 结果显示word2vec的准确率是最高的，而且运行时间最短。 Original paper can be download here.","categories":[{"name":"papers","slug":"papers","permalink":"http://cuiyungao.github.io/categories/papers/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"},{"name":"Rating Prediction","slug":"Rating-Prediction","permalink":"http://cuiyungao.github.io/tags/Rating-Prediction/"}]},{"title":"User Modeling with Neural Network for Review Rating Prediction","slug":"usermodeling","date":"2016-06-30T03:38:13.000Z","updated":"2016-06-30T09:41:46.000Z","comments":true,"path":"2016/06/30/usermodeling/","link":"","permalink":"http://cuiyungao.github.io/2016/06/30/usermodeling/","excerpt":"[IJCAI’15] User Modeling with Neural Network for Review Rating Prediction. Duyu Tang, Bing Qin, Ting LiuProceeding of The 24th International Joint Conference on Artificial Intelligence.","text":"[IJCAI’15] User Modeling with Neural Network for Review Rating Prediction. Duyu Tang, Bing Qin, Ting LiuProceeding of The 24th International Joint Conference on Artificial Intelligence. 作者Duyu Tang的分析集中于sentiment analysis，一年发了很多NLP的顶会文章（EMNLP, ACL, IJCAI, etc.）。 本文通过考虑user information来改善review rating prediction。启发源于lexical composition model，将compositional modifier作为一个matrix, 用matrix-vector mulplication作为compositon function。本文延伸lexical semantic compositio models，建立了user-word composition vector model (UWCVM)，并将结果作为feature用supervised learning framework来进行review rating prediction。 Problem DefinitionReview rating prediction可以看做是a classificaiton/regression problem，由Pang and Lee 2005最开始做，采用metric labeling framework。Rating的performance很大程度上依赖于feature representation的选择。 Problem: Given a review $r_{k_j}$ comprised of $n$ words $\\{w_1,w_2,…,w_n\\}$ written by user $u_k$ as input, review rating prediction aims at infering the numeric rating (1~4 or 1~5 stars) of $r_{k_j}$. The problem can be regarded as a multi-class classification problem by inferring a discrete rating score. MethodologyThe method includes two composition models, the user-word composition vector model (DWCVM) and the document composition vector model (DCVM). The former model将原始的word vectors加入user information，the latter model则将modified word vectors转化成review representation，并用其作为feature来进行rating prediction。Training的过程采用Pang and Lee提出的supervised metric labeling的方法。整个过程如下图表示： User-Word Composition Vector Model整个model将original word vectors融入user information。融入的方法有两种：additive和multiplicative。给定两个向量$v_1$和$v_2$，Additive的结合方式认为输出向量$p$是a linear function of Cartesian product of $v_1$ and $v_2$，如下： $$p = \\mathbf{A}\\times v_1 + \\mathbf{B}\\times v_2$$其中$\\mathbf{A}$和$\\mathbf{B}$是matrices parameters，用来encode $v_1$, $v_2$对$p$的贡献。Multiplicative的结合方式认为输出向量$p$是a linear function of the tensor product of $v_1$ and $v_2$，如下： $$p = \\mathbf{T}\\times v_1 \\times v_2 = \\mathbf{U}_1 \\times v_2$$其中$\\mathbf{T}$是一个rank为3的tensor，将$v_1,v_2$的tensor product映射到$p$上。$\\mathbf{T}$和$\\v_1$的Partial product可以被看做生成新的矩阵$\\mathbf{U}_1$，这个矩阵可以modify原始的word vectors。这两种结合方式可以用下图表示： 文章选用multiplicative composition，因为这种结合方式符合最初的用user information来改善word vectors的想法。后面的实验也验证了multiplicative composition的结合方式准确率要高于additive composition。 为了减少parameter size，user representation被用low-rank plus diagonal approximation来表示：$\\mathbf{U}_k=\\mathbf{U}_{k1} \\times \\mathbf{U}_{k2} + diag(u^{\\prime})$，其中$\\mathbf{U}_{k1}\\in \\mathbb{R}^{d\\times r}$, $\\mathbf{U}_{k2}\\in \\mathbb{R}^{r\\times d}$, $u^{\\prime}\\in \\mathbb{R}^d$。$u^{\\prime}$是每个user共享的background representation，以应对某些在test set中有而在training set中没有的users，即Out-Of-Vocabulary situation。最终modified word vectors $p_i$： $$p_i = tanh(\\mathbf{e}_{ik}) = tanh(\\mathbf{U}_k\\times \\mathbf{e}_i)\\; = tanh((\\mathbf{U}_{k1}\\times \\mathbf{U}_{k2} + diag(u^{\\prime})) \\times \\mathbf{e}_i)$$ Document Composition Vector Model文中采用一个简单而有效的方法paper, recursivley uses biTanh function来生成document representation: $$biTanh(p) = \\sum_{i=1}^n tanh(p_{i-1} + p_i)$$Each sentence将user-modified word vectors作为输入，得到sentence vectors；然后再将sentence vectors输入到biTanh得到最终的document vector $vec(doc)$。biTanh的Recursive use可以看做是two pairs of bag-of-word convolutional neural network，其中window size是2，parameters通过addition和tanh来定义。 Rating Prediction with Metric LabelingReview representation被用来进行review rating prediction，方法是metric labeling framework。 Model Training获取back-propagation中对于whole set of parameters的derivative of the loss, 然后用stochastic gradient descent with mini-batch来更新这些parameters。文中用dropout来避免neural network being over-fitting。 Note: Cases that rating does not match with review texts is not considered. [Zhang et al., SIGIR’14] Paper Experiment实验数据集有两个：Yelp13和RT05。实验结果说明: Semantic composition可以提高预测的准确度，并且结合metric labeling可以改善结果，因为它基于”similar items, similar labels”的idea。 一个用户有越多的reviews，则这个用户的rating可以被很好地估计。 SSPE [Tang et al., 2014a] paper更适用于short review.","categories":[{"name":"papers","slug":"papers","permalink":"http://cuiyungao.github.io/categories/papers/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"},{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"},{"name":"Rating Prediction","slug":"Rating-Prediction","permalink":"http://cuiyungao.github.io/tags/Rating-Prediction/"},{"name":"Neural Network","slug":"Neural-Network","permalink":"http://cuiyungao.github.io/tags/Neural-Network/"}]},{"title":"UFLDL Tutorial","slug":"UFLDL","date":"2016-06-29T14:39:40.000Z","updated":"2016-06-29T14:43:23.000Z","comments":true,"path":"2016/06/29/UFLDL/","link":"","permalink":"http://cuiyungao.github.io/2016/06/29/UFLDL/","excerpt":"The page records the knowledge learnt from UFLDL tutorial provided by Stanford. The resource can be found here.","text":"The page records the knowledge learnt from UFLDL tutorial provided by Stanford. The resource can be found here. The tutorial主要介绍unsupervised feature learning和deep learning，以及如何实现相关的算法。由于算法实现主要是matlab，所以主要记录里面的知识要点。","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"},{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"}]},{"title":"Good-Turing Estimation 大数据处理平滑算法","slug":"goodturing","date":"2016-06-29T09:22:55.000Z","updated":"2016-08-03T04:34:19.000Z","comments":true,"path":"2016/06/29/goodturing/","link":"","permalink":"http://cuiyungao.github.io/2016/06/29/goodturing/","excerpt":"参数平滑算法","text":"参数平滑算法 参数平滑算法，是在训练数据不足时，采用某种方式对统计结果和概率估计进行必要的调整和修补，以降低由于数据稀疏现象带来的统计误差。 在实际应用中，数据稀疏会产生大量空值，影响后续处理的性能和效果，所以需要平滑算法。 Good-Turing估计经常用在数据平滑方面。基本思想是：将统计参数按出现次数聚类，然后用出现次数加1的类来估计当前类。假定，一个元素在文本$W$中出现$r$次的概率是$\\theta(r)$，$N_r$表示元素在$W$中正好出现$r$次的个数，即$N_r=|{x_j: \\sharp(x_j)=r}|$，满足： $$N = \\sum_{r} rN_r.$$ Good-Turing估计$\\theta(r)$为：$\\hat{\\theta}(r) = \\frac{1}{N} (r+1) \\frac{N_{r+1}}{N_r}$. 具体的分析过程可以看这里。","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"Data Mining","slug":"Data-Mining","permalink":"http://cuiyungao.github.io/tags/Data-Mining/"}]},{"title":"Collaborative Filtering (CF)","slug":"cf","date":"2016-06-29T09:22:55.000Z","updated":"2016-07-19T02:31:24.000Z","comments":true,"path":"2016/06/29/cf/","link":"","permalink":"http://cuiyungao.github.io/2016/06/29/cf/","excerpt":"This webpage contains information and implementation related to collaborative filtering.","text":"This webpage contains information and implementation related to collaborative filtering. 对于recommender system，最常用的两个方法是Content-Based和Collaborative Filtering (CF)。CF基于users对items的态度来推荐items，即&quot;wisdom of the crows&quot;。相反，CB关注items的属性，用items之间的相似度来进行推荐。 CF分为Memory-based CF和Model-based CF。 Implementation StepsWe use scikit-learn library将数据分成testing set和training set。For example, 12345import pandas as pdfrom sklearn import cross_validation as cvheader = [&apos;user_id&apos;,&apos;item_id&apos;,&apos;rating&apos;,&apos;timestamp&apos;]data = pd.read_csv(&apos;ml-100k/u.data&apos;, sep=&apos;\\t&apos;, names=header)train_data, test_data = cv.train_test_split(data, test_size=0.25) 基于cosine similarity，我们得到user_similarity和item_similarity，对于user-based CF,我们用下面的公式： $$\\hat{x}_{k,m} = \\bar{x}_k + \\frac{\\sum_{u_a} sim_u(u_k, u_a)(x_{a,m}-\\bar{x}_{u_a})}{\\sum_{u_a} \\|sim_u(u_k, u_a)\\|}$$ Users $k$ and $a$可以被当做是weights。Normalize确保评分在1~5之间。由于有相似taste的两个users可能在评分上一个偏高，一个偏低，所以前面需要加上user的平均rating作为bias。对于item-based CF,公式如下： $$\\hat{x}_{k,m} = \\frac{\\sum_{i_b} sim_i(i_m,i_b)(x_{k,b})}{\\sum_{i_b} \\|sim_i(i_m,i_b)\\|}$$ EvaluationMetric采用Root Mean Squared Error (RMSE),公式如下： $$RMSE = \\sqrt{\\frac{1}{N}\\sum (x_i-\\hat{x}_i)^2}$$ 实现采用sklearn提供的mean_square_error (MSE), RMSE是MSE的平方根。 1234567from sklearn.metrics import mean_squared_errorfrom math import sqrtdef rmse(prediction, ground_truth): # find nonzeros places in groundtruth and extract from the prediction accordingly prediction = prediction[ground_truth.nonzero()].flatten() ground_truth = ground_truth[ground_truth.nonzero()].flatten() return sqrt(mean_squared_error(prediction, ground_truth)) 12print &apos;User-based CF RMSE: &apos; + str(rmse(user_prediction, test_data_matrix))print &apos;Item-based CF RMSE: &apos; + str(rmse(item_prediction, test_data_matrix)) Drawback: Memory-based CF doesn’t scale to real-world scenarios and cannot solve cold-start problem well. Model-based CF methods are scalabel and can deal with higher sparsity level than memory-based, but also suffers from cold-start problem. Original Link. Also implemented locally. RecosystemRecommender system using parallel matrix factorization Link.. The author’s intro. Fast Recommendation for Activity StreamFast Recommendations for Activity Streams Using Vowpal Wabbit Link.","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"Recommend System","slug":"Recommend-System","permalink":"http://cuiyungao.github.io/tags/Recommend-System/"}]},{"title":"Memorization and Exploration in Recurrent Neural Language Models","slug":"memorization","date":"2016-06-28T14:27:33.000Z","updated":"2016-06-28T14:38:07.000Z","comments":true,"path":"2016/06/28/memorization/","link":"","permalink":"http://cuiyungao.github.io/2016/06/28/memorization/","excerpt":"Deep Learning | Los Angeles Meetup","text":"Deep Learning | Los Angeles Meetup The speaker explains recurrent memory networks.Link LSTM LSTM is a powerful sequential model. It has complicated design of input gate, forget gate, update gate and cell. LSTM achieves SOTA in many NLP tasks: sentence completion, sentiment analysis, parsing…. Recurrent Memory Networks amplify the power of RNN offer a way to interpret and discover dependencies in data outperform TreeLSTM models that explicitly use syntactic information in Sentence Completion Challenge First AttemptCombine linearly source context vector, target context (Memory block) and target hidden state LSTM Did not see any gain in BLEU Potentially bias toward target LM Gating combination might be essential for the model Better MemorizationBetter memorization leads to better translation LSTM does not offer any representation advantage compared to vanilla RNN Gradient in LSTM just flows nicer","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"},{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"}]},{"title":"Data Crawling","slug":"crawler","date":"2016-06-28T13:26:03.000Z","updated":"2016-08-03T04:42:01.000Z","comments":true,"path":"2016/06/28/crawler/","link":"","permalink":"http://cuiyungao.github.io/2016/06/28/crawler/","excerpt":"This webpage contains information about data crawling, such as some useful tools and summaries.","text":"This webpage contains information about data crawling, such as some useful tools and summaries. Data scraper for Facebook PagesLink. Data scraper for Facebook Pages, and also code accompanying the blog post How to Scrape Data From Facebook Page Posts for Statistical Analysis. An example is shown below. Web Crawler System in PythonLink. Write script in Python Powerful WebUI with script editor, task monitor, project manager and result viewer MySQL, MongoDB, Redis, SQLite, Elasticsearch; PostgreSQL with SQLAlchemy as database backend RabbitMQ, Beanstalk, Redis and Kombu as message queue Task priority, retry, periodical, recrawl by age, etc… Distributed architecture, Crawl Javascript pages, Python 2&amp;3, etc…","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"Data Mining","slug":"Data-Mining","permalink":"http://cuiyungao.github.io/tags/Data-Mining/"},{"name":"Crawler","slug":"Crawler","permalink":"http://cuiyungao.github.io/tags/Crawler/"}]},{"title":"Generative Topic Embedding:a Continuous Representation of Documents","slug":"acl16","date":"2016-06-28T12:30:48.000Z","updated":"2016-08-03T03:40:38.000Z","comments":true,"path":"2016/06/28/acl16/","link":"","permalink":"http://cuiyungao.github.io/2016/06/28/acl16/","excerpt":"[ACL’16] From 清华Zhu Jun‘s Group","text":"[ACL’16] From 清华Zhu Jun‘s Group Summary from 洪亮劼 Problem DefinitionGiven the hyperparameters $\\alpha$, $\\gamma$, $\\mu$, the learning objective is to find the embeddings $\\mathbf{V}$, the topics $\\mathbf{T}$, and the word-topic and document-topic distributions $p(\\mathbf{Z}_i, \\phi_i|d_i, \\mathbf{A}, \\mathbf{V}, \\mathbf{T})$. Here, $\\mathbf{Z},\\mathbf{T},\\phi$ denote the collection of the document-specific $\\{\\mathbf{Z}_i\\}_{i=1}^M$, $\\{\\mathbf{T}_i\\}_{i=1}^M$, $\\{\\phi_i\\}_{i=1}^M$. $\\mathbf{V}$, $\\mathbf{A}$, and $\\mathbf{T}_i$ stand for the embeddings, the bigram residuals, and the topics, respectively. As a whole, a generative word embedding model PSDVec (Li et al., 2015, ref), by incorporating topics into it. The new model is named TopicVec. TopicVec Model模型结合了topic modeling和word embeddings。In TopicVec, an embedding link function对一个topic中的word distribution进行建模，而不是LDA中的categorical distribution。这个link function的优势是semantic relatedness在embedding space当中被encoded。其它的过程跟LDA很相似，同样用Dirichlet priors来regularize topic distributions，用variational inference algorithm来进行optimization。这个过程可以得到跟words在同样embedding space的topic embeddings。Topic embeddings的目的是来估计underlying semantic centroids。 The whole process: For the $k$-th topic, draw a tpic embedding uniformly from a hyperball of radius $\\gamma$, i.e., $t_k\\sim Unif(B_{\\gamma})$; For each document $d_i$:(a) Draw the mixing proportions $\\phi_i$ from the Dirichlet prior Dir($\\alpha$);(b) For the $j$-th word: $\\;$ i. Draw topic assignment $z_{ij}$ from the categorical distribution Cat($\\phi_i$); $\\;$ ii. Draw word $w_{ij}$ from $\\mathbf{S}$ according to $P(w_{ij}|w_{i,j-c}:w_{i,j-1},z_{ij},d_i)$. Fig1. Graphical representation of TopicVec The notations are listed in the following table: Word Embeddings &amp; Topic ModelingWord embedding通过一个小的context window里面的local word collocation patterns,将words映射到一个低维连续的embedding space。而topic modeling通过同一文档里面的global word collocation patterns，将documents映射到一个低维的topic space。这两个可以互补，本文由此得到TopicVec Model。其中，topics由embedding vectors来表示，并在documents之间共享。每个word的probability由local context和topic来决定。Topic embedding由variational inference method产生，同时得到每个document的topic mixing probability。结合topic embedding和topic mixing probability，可以在低维连续空间里面得到每个document的representation。 Euclidean distance不是衡量两个embeddings之间相似度的最优方法，已有的方法大部分采用exponentiated cosine similarity作为link function，所以cosine similarity可能是较好的估计语义相似度的方法。 Inspirations 从此方法得到的word embeddings，然后再用DRNN等方法来进行classification？ 可以用于DTM上么？ 本文的code可以在这里下载,Nguyen et al. (2015)的方法与之类似，将word embeddings作为latent features，但是实现速度慢，对large corpus不可行，code可以在这里找到。 The original paper can be found here.","categories":[{"name":"papers","slug":"papers","permalink":"http://cuiyungao.github.io/categories/papers/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"},{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"},{"name":"Topic Modeling","slug":"Topic-Modeling","permalink":"http://cuiyungao.github.io/tags/Topic-Modeling/"}]},{"title":"Study Material for Deep Learning and NLP","slug":"material","date":"2016-06-28T12:21:47.000Z","updated":"2016-06-28T12:46:54.000Z","comments":true,"path":"2016/06/28/material/","link":"","permalink":"http://cuiyungao.github.io/2016/06/28/material/","excerpt":"The website summaries the material for understanding deep learning and NLP.","text":"The website summaries the material for understanding deep learning and NLP. How to Start Learning Deep LearningLink [Book] First Contact with TensorFlowLink The book introduces how to use TensorFlow to implement the basic functions of neural network and also parallelism.","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"},{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"},{"name":"material","slug":"material","permalink":"http://cuiyungao.github.io/tags/material/"}]},{"title":"Useful Source Code for NLP","slug":"code","date":"2016-06-28T09:17:25.000Z","updated":"2016-07-29T03:27:18.000Z","comments":true,"path":"2016/06/28/code/","link":"","permalink":"http://cuiyungao.github.io/2016/06/28/code/","excerpt":"This page contains useful code related to NLP.","text":"This page contains useful code related to NLP. TensorFlow v0.9 now available with improved mobile support.Link TensorFlow now supports mobile invocation. Tensorflow and theano CNN code for insurance QA(question Answer matching).Link. Theano和tensorflow的网络结构都是一致的: word embedings + CNN + max pooling + cosine similarity.目前再insuranceQA的test1数据集上，top-1准确率可以达到62%左右，跟论文上是一致的。这里只提供了CNN的代码，后面测试了LSTM和LSTM+CNN的方法，LSTM+CNN的方法比单纯使用CNN或LSTM效果还要更好一些，在test1上的准确率可以再提示5%-6%. LSTM language model with CNN over characters in TensorFlow.Link.Tensorflow implementation of Character-Aware Neural Language Models paper. Topic Augmented Neural Response Generation with a Joint Attention MechanismLink. Use attention model. Text input with relevant emoji sorted with deep learninglink. After inputing a sentence, the related emojis are shown in the top. I’ve tried the project, and it’s amazing. It uses API from Dango. The webpage recommends deep learning material. I need to read them in detail, which maybe helpful for review analysis. Information Extraction with Reinforcement LearningLink. It’s based on Torch and python. Visualization Toolbox for Long Short Term Memory networks (LSTMs)Link. Visual Analysis for State Changes in RNNs. More information about LSTMVis, an introduction video, and the link to the live demo can be found here. Attention Sum ReaderLink. This is a Theano/Blocks implementation of the Attention Sum Reader model as presented in “Text Comprehension with the Attention Sum Reader Network” available at here. Wider &amp; deep learning: better together with TensorFlowLink. Query的过程就是找出类似的词的过程，这个可以用phrase extraction？是否可以用在rating prediction上面。 Minimal Character-Level Language Model需看~！Link. Minimal character-level language model with a Vanilla Recurrent Neural Network, in Python/numpy. LSTM-ParserLink. Based on C++ language. Transition-based joint syntactic dependency parser and semantic role labeler using stack LSTM RNN architecture. Paper Greedy, joint syntactic-semantic parsing with stack LSTMs can be downloaded here. Pre-trained Word Embedding in KerasLink. Based on Keras library. In this tutorial, we will walk you through the process of solving a text classification problem using pre-trained word embeddings and a convolutional neural network. Building Machine Learning Estimator in TensorFlowLink. The purpose of this post is to help you better understand the underlying principles of estimators in TensorFlow Learn and point out some tips and hints if you ever want to build your own estimator that’s suitable for your particular application. TensorLayer: Deep learning and Reinforcement learning libraryLink. It was designed to provide a higher-level API to TensorFlow in order to speed-up experimentations. Neural Relation ExtractionLink. Neural relation extraction aims to extract relations from plain text with neural models, which has been the state-of-the-art methods for relation extraction. In this project, we provide our implementations of CNN [Zeng et al., 2014] and PCNN [Zeng et al.,2015] and their extended version with sentence-level attention scheme [Lin et al., 2016] Make a Chatting RobotLink. 自己动手做聊天机器人教程. Door to Machine LearningLink. 机器学习精简入门教程. Sequence Classification with LSTM RNN with KerasLink. Python based.实验结果证明加了CNN的效果更好，用dropout避免过拟合，但是实验结果不是那么理想，原因是深度只有3，当层数增加的时候可以实现比不用dropout更好的效果。 Neural Conversation ModelsLink. TensorFlow based.支持simple seq2seq models和attention based seq2seq models. 深度学习主机环境配置Link.","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"},{"name":"code","slug":"code","permalink":"http://cuiyungao.github.io/tags/code/"}]},{"title":"LDA Understanding","slug":"lda","date":"2016-06-28T09:17:25.000Z","updated":"2016-07-20T06:24:45.000Z","comments":true,"path":"2016/06/28/lda/","link":"","permalink":"http://cuiyungao.github.io/2016/06/28/lda/","excerpt":"LDA introduction and summary.","text":"LDA introduction and summary. Topic Model Reading ListLink.详细解释：","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"},{"name":"LDA","slug":"LDA","permalink":"http://cuiyungao.github.io/tags/LDA/"}]},{"title":"Deep Recursive Neural Networks for Compositionality in Language","slug":"drnn","date":"2016-06-28T07:04:35.000Z","updated":"2016-06-28T09:26:20.000Z","comments":true,"path":"2016/06/28/drnn/","link":"","permalink":"http://cuiyungao.github.io/2016/06/28/drnn/","excerpt":"Irsoy, Ozan, and Claire Cardie. “Deep recursive neural networks for compositionality in language.” Advances in Neural Information Processing Systems. 2014.","text":"Irsoy, Ozan, and Claire Cardie. “Deep recursive neural networks for compositionality in language.” Advances in Neural Information Processing Systems. 2014. Recursive Neural NetworkRecursive neural networks (RNNs) comprise a class of architecture that can operate on structured input. The same set of weights is recursively applied within a structural setting. Given a positional directed acyclic graph, it visits the nodes in topological order, and recursively applies transformations to generate further representations from previously computed representations of children. A recurrent neural network is simply a recursive neural network with a particular structure (Figure 1c). Problem: Even though RNNs are deep in structure, they lack the capacity for hierarchical representation that exists in conventional deep feed-forward networks and recurrent neural networks. Recurrent v.s. RecursiveRecurrent neural networks are deep in time, while recursive neural networks are deep in structure (due to the repeated application of recursive connections). Recently, the notions of depth in time - the result of recurrent connections, and depth in space - the result of stacking multiple layers on top of one another, are distinguished for recurrent neural network. Deep recurrent neural networks were proposed for composing these concepts. They are created by stacking multiple recurrent layers on top of each other. This allows the extra notion of depth to be incorporated into temporal processing. Inspired by deep recurrent neural networks, deep recursive neural networks are proposed in this paper. Deep Recursive Neural NetworksAn important benefit of depth is the hierarchy among hidden representations: every hidden layer conceptually lies in a different representation space and potentially is a more abstract representation of the input than the previous layer. The DRNN is constructed by stacking multiple layers of individual recursive nets: where $i$ means the multiple stacked layers, $W_L^{(i)}$, $W_R^{(i)}$, and $b^{(i)}$ are the weight matrices that connect the left and right children to the parent, and a bias vector, respectively. $V^{(i)}$ is the weight matrix that connects the $(i-1)$th hidden layer to the $i$th hidden layer. For the untying shown in Figure 1b, every node is represented in the same space above the first, regardless of their leafness. Figure 2 shows the weights that are untied or shared. For prediction, we connect the output layer to only the final hidden layer. If we connect the output layer to all hidden layers, multiple hidden layers can have synergistic effects on the output and make it more difficult to qualitatively analyze each layer. ResultsData: Stanford Sentiment Treebank (SST) link Result 1Comparing with multiplicative RNN and the more recent Paragraph Vectors, DRNNs outperform their shallow counterparts of the same size. Deep RNN outperforms the baselines, achieving state-of-the-art performance on the task. Reason: The authors attribute an important contribution of the improvement to dropouts. Result 2For searching nearest neighbor phrases, different layers capture different aspects. One-nor distance mearsure is used. Analysis: The first layer is dominated by one of the words that is composed. The seconde layer takes syntactic similarity more into account. The third layer captures the sentiment. Inspiration: Can we apply this into emoji detection?Paper can be download here.Code is written in C++, can be found here.Introduction webpage is here.","categories":[{"name":"papers","slug":"papers","permalink":"http://cuiyungao.github.io/categories/papers/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"},{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"}]},{"title":"How to read:character level deep learning","slug":"howtoread","date":"2016-06-27T08:26:48.000Z","updated":"2016-06-28T02:37:07.000Z","comments":true,"path":"2016/06/27/howtoread/","link":"","permalink":"http://cuiyungao.github.io/2016/06/27/howtoread/","excerpt":"A character level model for sentiment classification is demonstrated with TensorFlow.","text":"A character level model for sentiment classification is demonstrated with TensorFlow. Some NLP applications are impressive, such as Gmail’s auto-reply and FB’s deep-text. The models are built with framework Keras, with TensorFlow as back-end. KerasThe core data structure of Keras is a model, a way to organize layers. The main type of the model is the sequential model, a linear stack of layers. Character-Level ModelThe typical problem of sentiment analysis is, given a text $x_i$ (e.g., a movie review), we need to figure out whether the review is positive(1) or negative(0), denoted as $y_i$. A network $f(x_i)$ is created to predict the lable of the review. Typically, the text is split into a sequence of words, and then learn fixed length embedding of the sequence, which later is used for classification. In a recurrent model, each word is encoded as a vector (word embeddingsChristopher Olah) and also his explanation about LSTMs. The neural network tries to learn the specific sequences of letters from words separated by spaces or other punctuation points. The visualization of some internal processes of char-rnn models can be found in this paper. Building a Sentiment Model Dataset: labelled lebeledTrainData.tsv. Split the text into sentences. This bounds the maximum length of a sequence. Encode each sentence from characters to a fixed length encoding. Use a bi-directional LSTM to read sentence by sentence and create a complete document encoding. Full Model Architecture The model uses two bi-directional LSTM. It starts from reading characters and forming concepts of “words”, then uses a bi-directional LSTM to read “words” as a sequence and account for their position. Then, a second bi-directional LSTM is used for each sentence for the final document encoding. Code Advantage of Character-Level ModellingIt enables us to deal with common miss-spellings, different permutation of words (think run, runs, running). Texts that contain emojis, signaling chars, hashtags, and all the funky annotations that are being used in social media are very interesting directions. Take HomeSome things might improve the generalisation and reduce overfitting: Different hidden layer sizes. Smaller layers will reduce the ability of the model to overfit to the training set. Larger dropout rates. $l2/l1$ regularization. A deeper and/or wider architecture of cnn encoder. Different doc encoder, maybe include an attention model. link","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"},{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"},{"name":"character level","slug":"character-level","permalink":"http://cuiyungao.github.io/tags/character-level/"}]},{"title":"Courses for NLP and Deep Learning","slug":"nlpcourses","date":"2016-06-27T08:01:41.000Z","updated":"2016-06-27T08:06:55.000Z","comments":true,"path":"2016/06/27/nlpcourses/","link":"","permalink":"http://cuiyungao.github.io/2016/06/27/nlpcourses/","excerpt":"The website contains the courses related to NLP and deep learning.","text":"The website contains the courses related to NLP and deep learning. 1. Deep learning for text mining from scratchThis website contains links for optimization and statistics, and also very useful tutorials for NLP, such as UFLDL tutorial.","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"},{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"},{"name":"Courses","slug":"Courses","permalink":"http://cuiyungao.github.io/tags/Courses/"}]},{"title":"Dive into TensorFlow","slug":"diveTensorflow","date":"2016-06-27T03:37:45.000Z","updated":"2016-08-03T04:44:48.000Z","comments":true,"path":"2016/06/27/diveTensorflow/","link":"","permalink":"http://cuiyungao.github.io/2016/06/27/diveTensorflow/","excerpt":"Get started with TensorFlow.","text":"Get started with TensorFlow. TensorFlowTensorFlow is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. TensorFlow programs use a tensor data structure to represent all data - only tensors are passed between operations in the computation graph. USAGEA scalar, vector, or matrix is a ternsor. A tensor has a rank, a shape, and a static type. Tensor’s dimension can be described by rank, shape, and dimension number. 12345scalar = tf.constant(100)vector = tf.constant([1,2,3,4])matrix = tf.constant([[1,2],[3,4]])## get shapescalar.get_shape() Virables must be initialized by running an init Op after haveing launched the graph. 123456789101112var = tf.Variable(10)ten = tf.constant(10)new_var = tf.mul(var, ten)upate = tf.assign(var, new_var)init_op = tf.initialize_all_variables()with tf.Session() as sess: sess.run(init_op) print(sess.run(var)) for _ in range(5): sess.run(update) print(sess.run(var)) Variables are in-memory buffers containing tensors. During model training, variables can be used to hold and update parameters. 12weight = tf.Variable(tf.random_normal([4096,500], stddev=0.25))weight_update = tf.Variable(weight.initialized_value()) To get the results, we need to execute the graph with a run() call on the Session object. Feed temporarily replaces the output of an update with a tensor value. The most common “feed” operations is created by tf.placeholder(). 12345plh1 = tf.placeholder(tf.float32)plh2 = tf.placeholder(tf.float32)result = tf.mul(plh1, plh2)with tf.Session as sess: print(sess.run([result], feed_dict=&#123;plh1:[100.], plh2:[200.]&#125;)) Link How Good is Your Fit?Through the video provided by TensorFlow, here summaries the ways to prevent or reduce overfitting: Split up the data into 3 sets: the training set, test set, and cross validation set. Along with the parameters averaging, the model is not too dependent on any particular subset of the overall data set. For neural networks particularly, regularization is a common way. There are a few different types, such as L1 and L2, but each follows the same general principle - the model is penalized for having weights and biases that are too large. Another method is Max Norm constraints. It directly adds a size limit to the weights or biases. A completely different approach is Dropout. It randomly switches off ceratin neurons in the network, preventing the model from becoming too dependent on a set of neurons and its associated weights and biases.","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://cuiyungao.github.io/tags/TensorFlow/"}]},{"title":"SPINN","slug":"SPINN","date":"2016-06-26T03:58:46.000Z","updated":"2016-06-27T02:59:15.000Z","comments":true,"path":"2016/06/26/SPINN/","link":"","permalink":"http://cuiyungao.github.io/2016/06/26/SPINN/","excerpt":"Hybrid tree-sequence neural networks with SPINN, published at Stanford","text":"Hybrid tree-sequence neural networks with SPINN, published at Stanford - Online Reading Summay - SPINNStack-augmented Parser-Interpreter Neural Network. This is a hybrid tree-sequence architecture, 融合了recursive 和recurrent neural networks，比当中的任何一个要好。 GOALoutput compact, sufficient representations of natural language. IDEACompute representations bottom-up, starting at the leaves and moving to nonterminals. This allows linguistic structure to guide computation. TECHShift-Reduce Parsing, a method for building parse structures from sequence inputs in linear time. It works by exploiting an auxiliary stack structure, which stores partially-parsed subtrees, and a buffer, which stores input tokens which have yet to be parsed. This can generate the constituency tree. For a sentence with $n$ tokens, we can produce its parse with a shift-reduce parser in exactly $2n-1$ transitions. Shift Phase:Pulls the next word embeddings from the buffer and pushes it onto the stack; Recude Phase:Combines top two elements of the stack $\\vec{c_1},\\vec{c_2}$ into a single element $\\vec{p}$ via the standard recursive neural network feedforward: $$\\vec{p} = \\sigma(W[\\vec{c_1}, \\vec{c_2}]).$$ Now we have a shift-reduce parser, deep-learning style. The feedforward speed is up to 25x improvement over the recursive neura network, but 2~5 times slower than a recurrent neural network. Recursive neural networks have often been dissed as too slow and “not batchable”, and this development proves both points wrong. Hybrid Tree-Sequence NetworksVisualization of the post-order tree traversal performed by a shift-reduce parser. Why not have a recurrent neural network follow along this path of arrows? Tracking Memory: At any given timestep $t$, a new tracking value $\\vec{m_t}$ is computed by $\\vec{m_t} = Track(\\vec{m_t-1}, \\vec{c_1}, \\vec{c_2}, \\vec{b_1})$. This tracking memory is then passed noto the recursive composition function, via $\\vec{p} = \\sigma(W[\\vec{c_1};\\vec{c_2};\\vec{m_t}])$. A recurrent neural network has just been interwovened into a recursive neural network. ResultA representation $f(x)$ for an input sentence $x$ is built by a new way. It shows a high-accuracy result on the Stanford Natural Language Inference dataset. Published paper: A fast unified model for parsing and sentence understanding.Code.","categories":[{"name":"papers","slug":"papers","permalink":"http://cuiyungao.github.io/categories/papers/"}],"tags":[{"name":"RNN","slug":"RNN","permalink":"http://cuiyungao.github.io/tags/RNN/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"},{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"}]}]}