{"meta":{"title":"Cuiyun Gao's Daily Digest","subtitle":"Work Hard, and Play Harder. 如果有一天：你不再寻找爱情，只是去爱；你不再渴望成功，只是去做；你不再追求成长，只是去修行；一切才真正开始~！","description":null,"author":"Cuiyun Gao","url":"http://cuiyungao.github.io"},"pages":[],"posts":[{"title":"Softmax Approximations for Learning Word Embeddings and Language Modeling","slug":"softmax","date":"2016-08-19T14:39:25.000Z","updated":"2016-08-21T08:53:20.000Z","comments":true,"path":"2016/08/19/softmax/","link":"","permalink":"http://cuiyungao.github.io/2016/08/19/softmax/","excerpt":"本文主要介绍了softmax在word embedding方面的应用，资料来源于这里。","text":"本文主要介绍了softmax在word embedding方面的应用，资料来源于这里。 Language Modeling Objective语言模型的目标是最大化给定词$w_t$的前$n$个单词预测当前词的概率，即$p(w_t|w_{t-1},…,w_{t-n+1})$。对于n-gram models，即:$$p(w_t|w_{t-1},…,w_{t-n+1})=\\frac{count(w_{t-n+1},…,w_{t-1},w_t)}{count(w_{t-n+1},..,w_{t-1})},$$对于神经网络，即：$$p(w_t|w_{t-1},…,w_{t-n+1})=\\frac{exp(h^Tv_w)}{\\sum_{w_i\\inV}exp(h^Tv_{w_i})}.$$ Maximum entropy models最小化同一个概率分布，即$P_h(y|x)=\\frac{exp(h\\cdot f(x,y))}{\\sum_{y^{\\prime}\\in y}exp(h\\cdot f(x,y^{\\prime}))}$,其中$h$是一个weight vector，$f(x,y)$是一个feature vector。在神经网络中经常用于： Multi-class分类； “Soft” selection, 比如attention, memory retrievals等。 分母经常被叫做partition function,即$Z=\\sum_{w_i\\in V}exp(h^Tv_{w_i})$。Softmax-based approaches保持了softmax层的完整性，使得算法更高效；而sampling-based approaches优化估计softmax的不同的loss函数。 Softmax-based ApproachesHierarchical softmax可以被看做是一个二叉树，最多估计$log_2|V|$个节点，而不是所有的$|V|$个节点，如图Fig.1所示。结构比较重要，可以快速地遍历点，经常使用的结构是Huffman tree，对于frequent words路径短，如图Fig.2所示。 Fig.1 Hierarchical SoftmaxFig.2 Hierarchical Softmax [Mnih and Hinton, 2008] Differentiated Softmax的出发点是我们对于经常出现的词获取的知识较多，但是对于很少出现的词信息较少，这就导致前者可以学习较多的参数，而后者较少，最终导致每个输出单词的embedding的大小不同。频繁出现的词的embedding较大，而很少出现的词的embedding较小。框架图如图Fig.3所示。 Fig.3 Differentiated Softmax [Chen et al., 2015] CNN-Softmax学习产生word embedding的函数，而不是单独学习所有输出单词的embedding，如图Fig.4所示。 Fig.3 CNN-Softmax [Jozefowicz et al., 2016] sampling-based ApproachesMargin-based Hinge Loss的出发点是只学习两种分类，一种是正确的词，一种是不正确的词。正确的词的得分较高，不正确的词的得分较低，即最大化：$$\\sum_{x\\in X}\\sum_{w\\in V}max{0,1-f(x)+f(x^{(w)})},$$其中，$x^{(w)}$是一个corrupted window (目标词由任意的词来代替)，$f(x)$是模型输出的分数。 Noise Contrastive Estimation的目的是将target word跟noise区分开，如图Fig.5所示。这样语言模型被转化为二分类模型，对于每个词按照一定的噪声分布（比如，unigram）获取$k$个噪声采样点，用logistic regression loss最小化cross-entropy函数。随着$k$数目的增长估计softmax。 Fig.5 Noise Contrastive Estimation (NCE) [Mnih and Teh, 2012]","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"}]},{"title":"FAIR Open-Sources fastText","slug":"fasttext","date":"2016-08-19T09:57:42.000Z","updated":"2016-08-19T14:38:25.000Z","comments":true,"path":"2016/08/19/fasttext/","link":"","permalink":"http://cuiyungao.github.io/2016/08/19/fasttext/","excerpt":"fastText是Facebook AI Research (FAIR) lab设计的library，主要用来创建text representation和进行文本分类（详见论文）。","text":"fastText是Facebook AI Research (FAIR) lab设计的library，主要用来创建text representation和进行文本分类（详见论文）。 fastText结合了几个在NLP和machine learning中最成功的概念，包括bag of n-grams, using subword information,以及sharing information across classes with a hidden representation。为了加速计算速度，fastText采用hierarchical softmax来平衡类的分布。 fastText为了有效地处理有大量种类的数据集，采用了hierarchical classifier，而不是flat structure，使得training和testing的复杂度降到linear或者与类的数目成对数关系。为了解决类的分布不均衡的问题，fastText采用了Huffman algorithm建立tree的方法；结果是比较频繁的种类的树的深度较浅。基于fastText，training的时间可以从几天降低到几秒钟，并且在性能上与state-of-the-art的方法接近，如下表所示。 在语言的表示形式上，fastText的表现优于word2vec，结果如下表所示。那么中文的结果怎样？ Reference[1] FAIR open-sources fastText","categories":[{"name":"papers","slug":"papers","permalink":"http://cuiyungao.github.io/categories/papers/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"}]},{"title":"Supervised Sequence Labeling with Recurrent Neural Networks","slug":"supseq","date":"2016-08-19T07:00:56.000Z","updated":"2016-08-19T09:40:03.000Z","comments":true,"path":"2016/08/19/supseq/","link":"","permalink":"http://cuiyungao.github.io/2016/08/19/supseq/","excerpt":"","text":"与传统的supervised pattern recognition不同的是，sequence labeling的单独数据点之间并不是相互独立的，后者的输入和labels都是互相关联的序列。Sequence labeling的目的是确定输出labels的位置和类型。 RNN对sequence labeling比较有效，因为对上下文信息比较灵活（决定forget和remember什么样的信息）；可以接受不同类型的数据和数据的表示形式；可以在序列扭曲中发现sequential patterns。本文主要介绍LSTM和bi-directional RNN。 Generative and Discriminative Methods直接计算类的概率$p(C_k|x)$的方法被认为是discriminative methods；有些情况下，需要先计算类的条件密度$p(x|C_k)$，基于Bayes’ rule来计算，即$p(C_k|x)=\\frac{p(x|C_k)p(C_k)}{p(x)}$，其中$p(x)=\\sum_k p(x|C_k)p(C_k)$，这样的方法被认为是generative的方法。Generative methods的好处是每一个类都可以独立于其它的类来进行训练，而discriminative methods在每一次新的类加入时都要被重新训练。但是，后者的分类结果较好，因为它的目标是找到类的边界。文章集中在discriminative sequence labeling。 Sequence classification通常分为三类，越来越细地分为temporal classification, segment classification和sequence classification。Temporal classification中的数据是weakly labelled在target sequences之外；segment classification则是temporal classification的一个特殊例子，数据必须用target sequences来进行strongly labelled；sequence classification则指的是每个输入序列只能对应一个类，比如识别单个的手写字母。 Neural networks当中的activation functions都是可求导的，e.g.，$$\\frac{\\partial tanh(x)}{\\partial x} = 1-tanh(x)^2 \\\\\\frac{\\partial \\sigma (x)}{\\partial x} = \\sigma(x)(1-\\sigma(x)),$$由于activation functions是将无限的输入域映射到有限的输出域，所以通常也被称为squashing functions。","categories":[{"name":"papers","slug":"papers","permalink":"http://cuiyungao.github.io/categories/papers/"}],"tags":[{"name":"RNN","slug":"RNN","permalink":"http://cuiyungao.github.io/tags/RNN/"},{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"},{"name":"Sequence Labeling","slug":"Sequence-Labeling","permalink":"http://cuiyungao.github.io/tags/Sequence-Labeling/"}]},{"title":"Sequence Labeling with Discriminative Models","slug":"sequence","date":"2016-08-19T03:01:09.000Z","updated":"2016-08-19T06:50:13.000Z","comments":true,"path":"2016/08/19/sequence/","link":"","permalink":"http://cuiyungao.github.io/2016/08/19/sequence/","excerpt":"总结了UIUC CS498JH: Introduction to NLP (Fall 2012)关于sequence labeling的课件。","text":"总结了UIUC CS498JH: Introduction to NLP (Fall 2012)关于sequence labeling的课件。 Sequence labeling包含4个基本任务，即POS tagging，NP chunking，shallow parsing和named entity recognition。这里的discriminative model可以理解为generative model，也就是根据当前word生成其种类，主要用到了maximum entropy classifiers和MEMMs。 Tasks in Sequence LabelingPOS tagging是给每个词定义词性，如Fig.1所示。NP chunking是识别所有的名词词组，如Fig.2所示。 Fig.1 POS Tagging Fig.2 NP Chunking更具体地，我们定义三种新的tag，如图Fig.3：- B-NP:名词词组的开头；- I-NP:名词词组的中间；- O:名词词组的外面。Fig.3 BIO Encoding Shallow parsing识别所有的non-resursive的名词词组，包括动词词组(“VP”)和介词词组(“PP”)。Named entity recognition则找到所有提到的entities，比如people, organizations, locations和dates。这两种tasks都有BIO encoding的表示形式。 Graphical ModelsGraphical models是概率模型的表示形式，Nodes表示任意变量的分布$P(X)$,Arrows表示依赖关系$P(Y)P(X|Y)$,shaded nodes表示观察到的变量，white nodes表示隐含变量。HMMs可以看做是对输入$\\mathbf{w}$的generative models，即$P(\\mathbf{w})=\\prod_i P(t_i|t_{i-1})P(w_i|t_i)$，如图Fig.4所示。 Fig.4 HMMs as Generative Models Sequence Labeling的定义是给定输入序列$\\mathbf{w}=w_1, …, w_n$，预测其最佳的label sequence $\\mathbf{t}=t_1, …, t_n$，即：$${argmax}_{\\substack{t}}P(\\mathbf{t}|\\mathbf{w}).$$我们用conditional maximum likelihood estimation (conditional MLE)来估计$w$，$$\\begin{aligned}\\hat{\\mathbf{w}}&amp;={argmax}_{\\substack{\\mathbf{w}}}\\prod_i P(c_i|\\mathbf{x}_i,\\mathbf{w})\\\\ &amp;= {argmax}_{\\substack{\\mathbf{w}}}\\sum_i log(P(c_i|\\mathbf{x}_i,\\mathbf{w})) \\\\ &amp;= {argmax}_{\\substack{\\mathbf{w}}}\\sum_i log(\\frac{e^{\\sum_j w_j f_j(x_i,c)}}{\\sum_{c^{\\prime}e^{\\sum_j w_j f_j(x_i,c^{\\prime})}}}) \\\\ &amp;= {argmax}_{\\substack{\\mathbf{w}}}(L_{\\mathbf{w}}) \\end{aligned}$$对其进行求导，我们可以得到下式。 假设对$P(\\mathbf{w})$建模，假定是高斯（正态）分布，我们可以得到下式。","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"},{"name":"CRF","slug":"CRF","permalink":"http://cuiyungao.github.io/tags/CRF/"},{"name":"Sequence Labeling","slug":"Sequence-Labeling","permalink":"http://cuiyungao.github.io/tags/Sequence-Labeling/"}]},{"title":"The Unreasonable Confusion of Variational Autoencoders","slug":"vae","date":"2016-08-18T08:03:40.000Z","updated":"2016-08-18T09:47:47.000Z","comments":true,"path":"2016/08/18/vae/","link":"","permalink":"http://cuiyungao.github.io/2016/08/18/vae/","excerpt":"将deep learning和graphical model结合起来理解VAE。","text":"将deep learning和graphical model结合起来理解VAE。 VAE即variational autoencoders，作用是设计复杂的数据的generative models，并fit到大数据集上。下面从两个角度来理解VAE，第一个是neural networks，第二个是概率模型中的variational inference。 Neural Network Perspective在神经网络语言中，一个VAE包含encoder, decoder和loss function，如下图所示。Encoder是一个神经网络，输入是一个数据点$x$，输出是其hidden representation $z$，这个过程中包含权重和biases $\\theta$。Encoder必须有效地压缩数据，并映射到低维空间。由于低维空间是随机的，encoder输出参数为高斯概率密度$q_{\\theta}(z|x)$，我们可以从这个分布采样，得到$z$的噪声值。 Decoder是另外一个神经网络，输入是hidden representation $z$，输出参数为数据的概率分布$p_{\\phi}(x|z)$，这个过程中包含权重和biases $\\phi$。由于从较小的维度得到较大的维度，所以信息会丢失。我们的目标是让信息尽可能丢失得比较少，即encoder要根据hidden representation有效地重建输入$x$。我们定义loss function是带regularizer的negative log-likelihood。因为没有所有点的全局表示形式，我们可以分解这个loss function到仅依赖于单独点$l_i$的项，这样对$N$个点的总的loss为$\\sum_{i=1}^N l_i$，其中：$$l_i(\\theta, \\phi)=-E_{z\\sim q_{\\theta}(z|x_i)}[log p_{\\phi}(x_i|z)] + KL(q_{\\theta}(z|x_i)||p(z)),$$第一项是重建损失，或者是对第i个点的expected negative log-likelihood。这个预期是针对encoder在$z$上的分布，目的是尽量地重建数据。第二项是一个regularizer，作用是衡量encoder的分布$q_{\\theta}(z|x)$和$p(z)$之间的距离，即在用$q$表示$p$的过程中损失了多少信息。 在VAE中，$p$被认为是正态分布，即$p(z)=Normal(0,1)$, regularizer的作用是保证每个数字的hidden representation $z$足够不同，如果不包含这一项，encoder会给每一个点一个欧式空间中不同的区域，这意味着同一个数字的两个图片也会有不同的表示形式。最后，我们用梯度下降的方法来训练VAE，每个step size $\\rho$，参数$\\theta$和$\\phi$都会被更新，比如$\\theta \\, \\gets \\, \\theta-\\rho\\frac{\\partial l}{\\partial \\theta}$。 Probability Model Perspective在概率模型中，一个VAE包含数据$x$的概率模型和隐含变量$z$，即对于每个数据点$i$，generative process可以写成如下形式： 首先计算隐含变量 $z_i \\, \\sim \\, p(z)$； 然后计算数据点的后验概率 $x_i \\, \\sim \\, p(x|z)$。 A Graphical Model 这个模型定义了联合概率$p(x,z)=p(x|z)p(z)$，对于inference，我们的目标是给定观察数据，推测隐含变量的值，即计算后验概率$p(z|x)$，通过Bays，我们得到$p(z|x)=\\frac{p(x|z)p(z)}{p(x)}$。分母$p(x)=\\int p(x,z)p(z)\\mathrm{d}z$的估计需要遍历所有隐含变量，这需要大量的时间，所以我们需要另外想办法来估计这个后验概率。 Variational inference用一组分布$q_{\\lambda}(z|x)$来对$p(z|x)$进行估计，$\\lambda$表示不同的分布，也就是说，假定$q$是高斯分布，那么对每一个数据点的隐含变量$\\lambda_{x_i}=(\\mu_{x_i}, \\sigma_{x_i}^2)$。我们的目标是使得这个估计$q_{\\lambda}(z|x)$与真实的$q(z|x)$尽可能接近，由于都是概率，我们很自然地想到KL divergence，即：$$KL(q_{\\lambda}(z|x)||p(z|x))=E_q[log q_{\\lambda}(z|x)]-E_q[log p(x,z)] + log p(x).$$ 由之前的分析可知，$p(x)$很难得到。我们的目的是最小化这个KL divergence，将上式改写如下：$$log p(x) = KL(q_{\\lambda}(z|x)||p(z|x))+ELBO(\\lambda),$$其中，$ELBO(\\lambda)=E_q[log p(x,z)]-E_q[log q_{\\lambda}(z|x)]$，所以可以转变成最大化ELBO (Evidence Lower BOund)函数。由于数据点之间并没有共享隐含变量，所以总的目标函数可以认为是每个点的目标函数的和，也使得我们可以用梯度下降来更新迭代$\\lambda$。所以单独一个点的ELBO可以写成 (需要进行化简)：$$ELBO_i(\\lambda)=E_{q_{\\lambda}(z|x_i)}[log p(x_i|z)]-KL(q_{\\lambda}(z|x_i)||p(z))。$$ Combination of the Two Models从上面的分析可以看出，VAE主要分为两部分，inference network (encoder)和generative network (decoder)。前者是参数化后验概率$q_{\\theta}(z|x,\\lambda)$，后者是参数化$p(x|z)$。由于没有全局的隐含变量，我们可以用梯度下降(e.g., minibatch)最大化ELBO。统一概率模型跟神经网络的参数，ELBO可以写成：$$ELBO_i(\\theta, \\phi) = E_{q_{\\theta}(z|x_i)}[log p_{phi}(x_i|z)]-KL(q_{\\theta}(z|x_i)||p(z)).$$所以，$ELBO_i(\\theta, \\phi)=-l_i(\\theta, \\phi)$。 我们从概率模型的角度，定义了目标函数（ELBO)和inference algorithm (gradient ascent on the ELBO)。不懂inference到底是哪个过程？？在神经网络中，inference通常指给定新的之前没有遇到的点来预测隐含的表示形式；而在概率模型中，inference意味着给定观察到的点来预测隐含变量的值。 Reference[1] The unreasonable confusion of variational autoencoders.[2] 代码.","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"},{"name":"VAE","slug":"VAE","permalink":"http://cuiyungao.github.io/tags/VAE/"}]},{"title":"Learning Text Representation Using Recurrent Convolutional Neural Network with Highway Layers","slug":"RCNN","date":"2016-08-17T09:50:55.000Z","updated":"2016-08-17T12:58:23.000Z","comments":true,"path":"2016/08/17/RCNN/","link":"","permalink":"http://cuiyungao.github.io/2016/08/17/RCNN/","excerpt":"发表在SIGIR’16上。Wen, Y., Zhang, W., Luo, R., &amp; Wang, J. (2016). Learning text representation using recurrent convolutional neural network with highway layers. arXiv preprint arXiv:1606.06905.","text":"发表在SIGIR’16上。Wen, Y., Zhang, W., Luo, R., &amp; Wang, J. (2016). Learning text representation using recurrent convolutional neural network with highway layers. arXiv preprint arXiv:1606.06905. 作者提出了新的结合RNN和CNN的方式，即在中间加了一层highway，这层的作用是把RNN的输出进行特征选择，其结果作为CNN的输入。 MotivationRNN跟CNN在NLP领域非常流行，但是两者各有优劣势。RNN虽然考虑了单词的顺序，但是它的问题在于后面的单词相比于前面单词的表示形式更能影响结果，对于情感分析这样的task，重要的单词预示着正确的情感，但是可能出现在文档的任何位置，所以RNN的结果并不是很好。而CNN可以很自然地解决这个问题，因为它通过max-pooling层平等地看待所有的词；但是CNN也有自己的问题，比如需要确定window的大小以及众多的filter参数。 RCNN模型旨在结合这两者的优点。模型对整个文档采用bi-directional recurrent neural network，并结合词跟它的上下文来表示这个词，filter用来计算latent semantic vectors。最终，max-pooling用来获取最重要的因素，并形成固定大小的句子表示。 这篇文章改进已有的RCNN模型，在bi-directional RNN和CNN之间插入了一个highway network，形成RCNN-HW模型，这个highway layer的作用是作为中间层来单独选择每个词的表示形式中的features。 Model整个模型框图如下所示。RNN能够处理当前的数据实例并且保留历史信息，常用的比如LSTM和GRU，后者比较简洁且性能较好（really??），所以文章采用GRU。表达式如下：$$\\mathbf{r}_t = \\sigma(\\mathbf{W}_r\\mathbf{x}_t+\\mathbf{U}_r\\mathbf{h}_{t-1}+\\mathbf{b}_r) \\\\\\mathbf{z}_t = \\sigma(\\mathbf{W}_z\\mathbf{x}_t+\\mathbf{U}_z\\mathbf{h}_{t-1}+\\mathbf{b}_z) \\\\\\mathbf{\\tilde{h}}_t = tanh(\\mathbf{W}_h\\mathbf{x}_t+\\mathbf{U}_h(\\mathbf{r}_t\\odot\\mathbf{h}_{t-1})+\\mathbf{b}_h) \\\\\\mathbf{h}_t = \\mathbf{z}_t\\odot\\mathbf{h}_{t-1} + (1-\\mathbf{z}_t)\\odot\\mathbf{\\tilde{h}}_t,$$其中，$\\odot$表示element-wise multiplication，$\\mathbf{W,U,b}$表示输入、recurrent的权重和biases。所以，单词的表示形式$\\mathbf{\\tilde{x}}_t$可以结合上下文得到：$$\\mathbf{\\tilde{x}}_t = [\\overleftarrow{\\mathbf{h}_t}||\\mathbf{x}_t||\\overrightarrow{\\mathbf{h}_t}].$$ One-layer highway network是RNN和CNN的中间层，表示如下：$$\\mathbf{y}_t=\\tau\\odot g(\\mathbf{W}_H\\mathbf{\\tilde{x}}_t+\\mathbf{b}_H)+(1-\\tau)\\odot\\mathbf{\\tilde{x}}_t,$$其中，$g$是非线性函数，$\\tau$为&quot;transform gate&quot;，即$\\tau=\\sigma(\\mathbf{W}_{\\tau}\\mathbf{x}_t+\\mathbf{b}_{\\tau})$。这个highway network的设计跟GRU的update gate$\\mathbf{z}$非常相似，其作用是一部分输入信息被不变地传递到输出，其它的信息则需要进行非线性变换。实验说明这样的设计可以获取重要的信息。 在CNN阶段，我们将window size设置为1，因为bidirectional RNN和highway layer已经获取了每个单词的上下文信息。 Experiments文章提供了源代码。情感分析实验阶段，比较了六种方法，分别是：Sum-of-Word-Vector (COW), LSTM, Bi-LSTM, CNN, CNN+LSTM,和RCNN，结果如下图。这些网络用来学习文本的表示形式，之后用softmax进行预测。实验没有采用pre-training model (e.g., word2vec)或者regularizer (e.g., Dropout)，这些使实验结果更好。情感分类的实验结果表明，CNN的效果要好于RNN，可能是由于RNN很难被训练，并且对超参数和后面的单词比较敏感。RCNN的效果最佳，因为它综合了CNN获取局部信息的优势和RNN获取全局信息的优势。CNN-LSTM是用CNN的局部信息作为RNN的输入，但是局部信息并没有时序关系。 文章还比较了不同的文本长度对实验结果的影响，发现对于短文本，这几种方法都不是很好；而对于较长的文本，RCNN的优势就体现出来，如下图所示。RCNN不光可以保留较长的文本信息，还可以通过CNN的max-pooling减少噪声；RCNN-HW则通过highway layer进一步减少噪声，进行特征选择，所以在较长输入时效果最好。 Reference[1] Learning Text Representation Using Recurrent Convolutional Neural Network with Highway Layers","categories":[{"name":"papers","slug":"papers","permalink":"http://cuiyungao.github.io/categories/papers/"}],"tags":[{"name":"RNN","slug":"RNN","permalink":"http://cuiyungao.github.io/tags/RNN/"},{"name":"CNN","slug":"CNN","permalink":"http://cuiyungao.github.io/tags/CNN/"},{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"}]},{"title":"What to do about Ad Blocking Impact on Customer Experience?","slug":"adblock","date":"2016-08-17T09:02:25.000Z","updated":"2016-08-17T12:52:09.000Z","comments":true,"path":"2016/08/17/adblock/","link":"","permalink":"http://cuiyungao.github.io/2016/08/17/adblock/","excerpt":"这是一篇总结ad blocking对用户体验影响的报道。","text":"这是一篇总结ad blocking对用户体验影响的报道。 用户对ad比较反感的原因，总结有三点： Ads是获取网页的速度减慢； Ads干扰了用户希望浏览的内容； 用户担心Ads会泄露个人隐私信息。 用户使用Ad blocker已经非常普遍，据报道有20%的广告被拦截，从而造成了广告收入的损失。下面分析这三个方面的根本原因。第一点的原因是广告的数量，插件和第三方服务太多。对于如何测量用户体验，作者提出了用response time。","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"Ad","slug":"Ad","permalink":"http://cuiyungao.github.io/tags/Ad/"}]},{"title":"Consensus Attention-based Neural Networks for Chinese Reading Comprehension","slug":"CAS","date":"2016-08-17T06:51:22.000Z","updated":"2016-08-17T08:29:05.000Z","comments":true,"path":"2016/08/17/CAS/","link":"","permalink":"http://cuiyungao.github.io/2016/08/17/CAS/","excerpt":"Cui, Y., Liu, T., Chen, Z., Wang, S., &amp; Hu, G. (2016). Consensus Attention-based Neural Networks for Chinese Reading Comprehension. arXiv preprint arXiv:1607.02250.","text":"Cui, Y., Liu, T., Chen, Z., Wang, S., &amp; Hu, G. (2016). Consensus Attention-based Neural Networks for Chinese Reading Comprehension. arXiv preprint arXiv:1607.02250. 这篇文章是哈工大与科大讯飞合作的作品。文章的主要贡献在于： 提供了中文阅读理解的数据集，包括人民日报新闻 (People Daily news)和儿童童话 (Children’s Fairy Tale, CFT)； 完善了已有的attention-based NN model，提出了consensus attention-based neural network architecture，即consensus attention sum reader (CAS Reader)。 Problem DefinitionCloze-style reading comprehension problem致力于理解给定的上下文或者文档，回答根据文档的属性所得的问题，并且回答是文档当中存在的一个词。可以由下面的三元组表示：$&lt;\\mathcal{D}, \\mathcal{Q}, \\mathcal{A}&gt;$，其中$\\mathcal{D}$为文档，$\\mathcal{Q}$为query，$\\mathcal{A}$为针对query的回答。 这种问题的解决通常采用attention-based neural network的方法，但是这种方法需要大量的训练数据来训练预测的可靠模型。现有的方法是自动地生成大量的训练数据。 Consensus Attention Sum Reader本文的算法是基于[2]的改进，目的是直接从文档中估计答案，而不是从所有的vocabularies估计。作者发现直接将query进行RNN之后的表示形式不足以表示query的整个信息，作者利用query的每一个time slices，建立了不同steps之间的consensus attention。感觉最近是不是attention上的LSTM很火？这个可以改善一些常见的topic么？ 一般形式是，给定一组训练三元组$&lt;\\mathcal{D}, \\mathcal{Q}, \\mathcal{A}&gt;$，首先将文档$\\mathcal{D}$和query $\\mathcal{Q}$的one-hot表示形式转换成共享embedding matrix $W_e$的连续的表示形式。由于query通常比文档短，通过共享embedding weights，query的表示形式可以受益于文档的表示形式，这比分开的embedding matrices有效。 之后，用两个不同的bi-directional RNNs得到文档和query的上下文表示形式。这种方法可以获取之前和之后的上下文信息。文章采用bi-directional Gated Recurrent Unit (GRU)，如下：$$e(x) = W_e\\times x, \\; where \\, x\\in\\mathcal{D,Q} \\\\\\overrightarrow{h_s} = \\overrightarrow{GRU}(e(x)) \\\\\\overleftarrow{h_s} = \\overleftarrow{GRU}(e(x)) \\\\h_s = [\\overrightarrow{h_s};\\overleftarrow{h_s}].$$ 文档的attention即一个概率分布，这里用$h_{doc}$和$h_{query}$分别表示文档和query的上下文表示形式，都是三维的张量。这两个张量的内积表示每个文档单词在时间$t$对query单词的重要性。概率分布用softmax来获得，即：$$\\alpha(t) = softmax(h_{doc}\\odot h_{query}(t)).$$ $\\alpha(t)$即文档的attention，且$\\alpha(t)=[\\alpha(t)_1,\\alpha(t)_2, …, \\alpha(t)_n]$，$\\alpha(t)_i$指的是文档第$i$个词在时间$t$的attention值。Consensus attention由merging function $f$得到，即：$$s=f(\\alpha(1), …,\\alpha(m)),$$其中，$s$是query最终的attention，$m$是query的长度。Merging function有以下几种heuristics，即：$$ s\\propto \\begin{cases} softmax(\\sum_{t=1}^m \\alpha(t)),\\; if \\, mode=sum; \\\\ softmax(\\frac{1}{m}\\sum_{t=1}^m \\alpha(t)), \\; if \\, mode=avg; \\\\ softmax({max}_{t=1,…,m} \\alpha(t)_i), \\; if \\, mode=max. \\end{cases}$$ 最终，将结果$s$映射到vocabulary space $V$，并将在文档中同一个词出现在不同位置的attention value相加，如下式所示。（思考：这个方式跟直接从vocabulary中选取某个词有啥区别？因为文章的一个亮点是从文档当中选取，而不是从vocabularies当中选取。） $$P(w|\\mathcal{D,Q})=\\sum_{i\\in I(w,\\mathcal{D})} s_i, \\, w\\in V.$$其中，$I(w,\\mathcal{D})$为单词$w$出现在文档$\\mathcal{D}$中的位置。整个框架图由下图所示。 Fig.1 Architecture of the Proposed Consensus Attention Sum Reader (CAS Reader). Experiments框架实现采用Theano和Keras，在Tesla K40 GPU上训练。实验结果在某些数据集上要优于传统的不用Consensus的模型，结果不好的原因是以前的模型只是从entity当中挑选答案，而改进之后的模型是从文档当中选取。在中文分词上的结果表明，有Consensus的Model要优于传统的attention model，结果如下图所示。 文章另外一个有意思的点是，机器生成的问题跟人想问的问题不同，对人的问题产生效果比较差。比如，机器问题“I went to the __ this morning .”，而人的问题是“Where did I go this morning ?”。我比较疑惑的是，这两个问题在中文里面不是一样么？只是加了一个语气词而已，那么效果怎么会差这么大？ Reference[1] Consensus Attention-based Neural Networks for Chinese Reading Comprehension[2] Text Understanding with the Attention Sum Reader Network","categories":[{"name":"papers","slug":"papers","permalink":"http://cuiyungao.github.io/categories/papers/"}],"tags":[{"name":"RNN","slug":"RNN","permalink":"http://cuiyungao.github.io/tags/RNN/"},{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"},{"name":"Attention Model","slug":"Attention-Model","permalink":"http://cuiyungao.github.io/tags/Attention-Model/"}]},{"title":"Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond","slug":"arxiv0810","date":"2016-08-16T09:16:51.000Z","updated":"2016-08-17T01:21:43.000Z","comments":true,"path":"2016/08/16/arxiv0810/","link":"","permalink":"http://cuiyungao.github.io/2016/08/16/arxiv0810/","excerpt":"IBM的作品。Nallapati, R., Zhou, B., glar Gulçehre, Ç., &amp; Xiang, B. Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond.","text":"IBM的作品。Nallapati, R., Zhou, B., glar Gulçehre, Ç., &amp; Xiang, B. Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond. 所谓的abstractive text summarization就是压缩原始的文档，并保留原文档的主要概念。 Contribution文章将在MT (Machine Learning)中的Attentional Encoder-Decoder RNN算法应用到abstractive text summarization，并针对在abstractive text summarization中存在的三个问题（即为key-words建模，获取sentence-to-word的结构层级，和在training阶段处理少见的或者没有见过的词）对模型进行改善。 文章的另外一个贡献是提出了新的包含多句子总结的数据集，建立了后面research的benchmarks。 Models模型包含encoder和decoder，encoder是一个bidirectional GRU-RNN，而decoder是一个跟encoder有着相同大小hidden-state size的uni-directional GRU-RNN；在source-hidden states之间是attention mechanism，最终由一个soft-max layer为目标词汇生成单词。文章的model有如下三个改进的方面： Feature-rich encoder; Switching generator-pointer model; Hierarchical attention model. 第一个改进，为encoder的输入增加新的features，比如parts-of-speech tags, named-entity tags (NER tags),和离散后的TF以及IDF数据。 第二个改进，为decoder增加一个switch，决定用generator还是用一个指针指向文档当中的某一个位置。如果switch打开，则模型还是用以前的方法从目标词库当中生成单词；如果关闭的话，则生成一个指针指向源的某一个位置，这个位置的word就会被添加到summary当中。每一个time step，Switch被定义为基于linear layer的sigmoid函数，如下：$$P(s_i=1)=\\sigma(\\mathbf{v}^s\\cdot(\\mathbf{W}_h^sh_i)+\\mathbf{W}_e^s\\mathbf{E}[o_{i-1}]+\\mathbf{W}_c^s\\mathbf{c}_i+\\mathbf{b}^s)),$$其中$P(s_i=1)$表示在第$i$个time step开关被打开的概率，$\\mathbf{c}_i$是attention-weighted context vector，$mathbf{W}_h^s, \\mathbf{W}_e^s，\\mathbf{W}_c^s, \\mathbf{b}^s$和$\\mathbf{v}_s$都是开关的参数。这个是不是就相当于CNN中的filters的概念。改进后的模型图如下图所示。 第三个改进，在encoder采用两层概念，即word level和sentence level。Attention mechanism同时作用在这两层上，即word-level的attention被相应的sentence-level的attention重新校正，定义如下：$$P^a(j) = \\frac{P_w^a(j)P_s^a(s(j))}{\\sum_{k=1}^{N_d}P_w^a(k)P_s^a(s(k))}，$$其中，$P_w^a(j)$是word-level第j个位置的attention weight，$P_s^a(l)$是原文件中第l个位置sentence-level的attention weight。得到的re-scaled attention被用来计算decoder的hidden state的输入（attention-weighted context vector）。而且，将positional embeddings加到sentence-level RNN的hidden state，来表示文档中句子的位置重要性。这个模型因此能够将关键句子跟关键词联系在一起。 Experiments分了好几个实验，比较感兴趣的实验是在CNN/Daily Mail上的实验，实验比较了三个模型，实验结果如图所示。最后一个模型”temp”表示Temporal Attention Model，解决了entity重复出现的问题，这个可以用在tag identification上面。另外，感觉这个Temporal Attention Model跟LSTM类似，只不过是在attention上的LSTM。 Reference[1] Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond","categories":[{"name":"papers","slug":"papers","permalink":"http://cuiyungao.github.io/categories/papers/"}],"tags":[{"name":"RNN","slug":"RNN","permalink":"http://cuiyungao.github.io/tags/RNN/"},{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"}]},{"title":"Enriching Word Vectors with Subword information","slug":"enrich","date":"2016-08-16T02:06:02.000Z","updated":"2016-08-16T03:30:25.000Z","comments":true,"path":"2016/08/16/enrich/","link":"","permalink":"http://cuiyungao.github.io/2016/08/16/enrich/","excerpt":"这篇文章是facebook AI research作品，是fastText的基础。Bojanowski, P., Grave, E., Joulin, A., &amp; Mikolov, T. (2016). Enriching Word Vectors with Subword Information. arXiv preprint arXiv:1607.04606.","text":"这篇文章是facebook AI research作品，是fastText的基础。Bojanowski, P., Grave, E., Joulin, A., &amp; Mikolov, T. (2016). Enriching Word Vectors with Subword Information. arXiv preprint arXiv:1607.04606. 大神Tomas Mikolov的有一篇作品，借鉴参考当中的trick。 Idea这篇文章的主要思路是让word之间可以共享信息，即利用subword的特征。这种表示方法同样对rare words以及不同的语言有效。 Model总的框架类似于Skip-Gram Model，给定一组训练词序集$w_1, …, w_T$，目标是最大化基于$w_t$预测出$w_c$的概率，即：$$\\sum_{t=1}^T\\sum_{c\\in\\mathcal{C}_t} log p(w_c|w_t),$$其中$\\mathcal{C}_t$指$w_t$周围的词的indices。给定一个scoring function $s$，将paris (word, context)映射到$\\mathbb{R}$空间，那么一个context word的概率可以定义为：$$p(w_c|w_t)=\\frac{e^{s(w_t,w_c)}}{\\sum_{j=1}^W e^{s(w_t,j)}}.$$这个scoring function通常定义为$s(w_t,w_c)=\\mathbf{u}_{w_t}^T\\mathbf{v}_{w_c}$。 考虑一个word当中的$n$-grams $\\mathcal{G}_w\\subset{1,…,G}$，每个$n$-gram $g$可以表示成一个向量$z_g$，这个word可以表示成其$n$-grams的和。所以scoring function可以表示成：$$s(w,c)=\\sum_{g\\in\\mathcal{G}_w} \\mathbf{z}_g^T\\mathbf{v}_c.$$为了限制memory，所有的$n$-grams被映射到1~K的整数，论文中K为2 millions，最终每个word被表示成index和$n$-grams的hash values。词的开头和结果加一个special characters，以便于找到前缀和后缀。为了提高模型效率，最频繁的$P$个单词不用$n$-grams来表示。 实验结果在similarity tasks上的效果不错。测量方法是模型评分和人工判断之间的Spearman’s rank correlation coefficient。实现采用C++，在运行时间上比Skip-Gram baseline慢了1.5倍，比较的方法当中Skip-Gram和CBOW由C实现。","categories":[{"name":"papers","slug":"papers","permalink":"http://cuiyungao.github.io/categories/papers/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"}]},{"title":"Query Understanding","slug":"query","date":"2016-08-13T02:36:37.000Z","updated":"2016-08-14T08:24:02.000Z","comments":true,"path":"2016/08/13/query/","link":"","permalink":"http://cuiyungao.github.io/2016/08/13/query/","excerpt":"Query understanding是很多AI应用场景的基础，比如机器人对话，车载导航等等。","text":"Query understanding是很多AI应用场景的基础，比如机器人对话，车载导航等等。 Query understanding的任务有三个，即纠正用户输入query的错误，精确地引导用户以及准确地理解用户query的目的。Query通常分为两种类型，一种是general query，不需要上下文情境；另外一种是contextual questions，比如”Where can I eat cheese cake right now”。第二种query不需要返回一系列的文档，而是需要解析query的目的，用户的地理位置和时间等，提供personal的回答。另外一种分类方法是从答案的角度，一种是精确需求，直接给出结果，比如”Who is the president of USA”；第二种则是给出泛泛的回答，让用户自己选择，即Interactive Question Answering，如下图所示。 Fig.1 Interactive Question Answering General CharacteristicsQuery understanding还有一个非常有意思的应用是个人助理（比如微软Cortana，苹果Siri，Google Now，百度度秘），可以为用户提供各种服务，比如搜索，会议安排，闹铃设置，打电话，发短信，播放音乐等等。这个可以被称为Deep Search，即Search with a Deep Understanding，应用NLP和knowledge context (用户之前的搜索和浏览活动，兴趣爱好，地点，询问的时间以及当时的天气等等)，通常有如下特征： 对query的语义理解。不同语义层次的理解如下图所示，展示了不同level的语义理解。 Fig.2 Matching at Different Semantic Levels 对上下文和前面任务的理解。这个可以有效地理解有歧义的词汇。Fig.3显示了含有interleaved tasks的一个搜索情景。Reference Query表示的是当前用户的query，On-Task Queries表示跟Reference Query相同任务的query；Off-Task Queries表示的是跟当前query任务不同的query。 Fig.3 A Search Context with Interleaved Tasks 用户理解和个性化。不同的用户有不同的兴趣爱好等。 Phases in Deep Query UnderstandingQuery understanding主要有三个过程，如Fig.4所示，包括query refinement, query suggestion和query intent detection，最终给query打tag，有了tag之后进入Answer Generation Module。 Fig.4 Query Understanding Module Broad Components Query refinement主要进行spelling correction, acronym expansion, word splitting, words merger和phrase segmentation。纠正之后的query进入query suggestion来发现和推荐可以得到更好搜索结果的queries。在循环一次或者多次query correction-&gt;query suggestion-&gt;user query这个过程后，完善的query进入到query expansion模块，补充更多近似的queries，以减少由于字符的错误匹配产生的文章结果。这个query set之后输入到query intention detection模块，精确地推测query的意图。Query classification将query粗分类到某一个field里面，比如运动、娱乐、天气等，结果被输入到semantic tagging部分进行意图检测。Fig.5描述了一个例子。 Fig.5 Example of Task Done at Each Component Query classification比document text classification更具有挑战性，因为query相对较短，而且很多query有歧义性。Query classification对精确地判断用户意图非常重要。对于semantic tagging，经常用的features是n-grams, regular regression, POS (Part of Speech), lexicons和transit features,总的来说，分为两种，syntactic和semantic的features。 Reference[1] Techniques for deep query understanding.","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"},{"name":"QU","slug":"QU","permalink":"http://cuiyungao.github.io/tags/QU/"}]},{"title":"CNN总结","slug":"cnn","date":"2016-08-12T03:00:57.000Z","updated":"2016-08-12T03:29:50.000Z","comments":true,"path":"2016/08/12/cnn/","link":"","permalink":"http://cuiyungao.github.io/2016/08/12/cnn/","excerpt":"已经有很多大牛对CNN的算法进行了总结，这里就对这些资料进行汇总。","text":"已经有很多大牛对CNN的算法进行了总结，这里就对这些资料进行汇总。 CNN已经被各种大牛解说得很好了，其中觉得不是很懂的地方主要还是在back propagation。所以这篇文章主要对back propagation方面进行总结，也汇总CNN的一些重要的点。一些很好的资料也在本文进行标注。 Main Steps下面这幅图非常经典，是ConvNet的过程图。在CNN中有4个关键操作： Convolution Non Linearlity (ReLU) Pooling or Sub Sampling Classification (Fully Connected Layer). Convolution这一步是一个linear的操作，主要目的是获取输入图片的特征，通过移动filter（也叫kernel，feature detector）保留pixels之间的空间关系。dot product的结果叫做Convolved Feature或者Activation Map或者Feature Map。filter种类的数量表示convolution之后的深度，一种filter得到一种layer。Feature Map的大小由三个参数决定，即Depth, Stride和Zero-Padding。 ReLU是对Convolution的结果进行非线性变换，是element wise operation，将所有的负值替换成0。因为现实数据通常是非线性的，所以引入ReLU这个非线性函数，也可以采用tanh或者sigmoid，但是ReLU通常情况下性能较好。 Pooling操作主要目的是减少特征维度，同时保留重要信息，避免overfitting，这使得对输入图片的各种变换、扭曲和移动有效。到这一步，我们可以得到输入图片的high-level的feature。最后通过fully connected layer，我们对这些feature进行分类。比如说我们要使得结果分成$k$类，通过softmax这个activation function，使得这$k$类输出概率的和是1。最后通过back propagation，我们更新迭代参数使分类结果最佳。 Reference[1] An Intuitive Explanation of Convolutional Neural Networks[2] Backpropagation[3] CS231n Convolutional Neural Networks Stanford[4] Introduction to Natural Language Processing 2016。这里面包含了各种资料的汇总。","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"CNN","slug":"CNN","permalink":"http://cuiyungao.github.io/tags/CNN/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"}]},{"title":"SVM模型小结","slug":"svmbaidu","date":"2016-08-11T12:06:31.000Z","updated":"2016-08-12T02:00:27.000Z","comments":true,"path":"2016/08/11/svmbaidu/","link":"","permalink":"http://cuiyungao.github.io/2016/08/11/svmbaidu/","excerpt":"在百度每周的NLP课程总结，感谢磊磊哥大神。这节课主要讲的是SVM算法，讲到了其中的精髓，感觉受益匪浅，还是记下来以备复习。","text":"在百度每周的NLP课程总结，感谢磊磊哥大神。这节课主要讲的是SVM算法，讲到了其中的精髓，感觉受益匪浅，还是记下来以备复习。 SVM (Support Vector Machine)是机器学习的基础，虽然基本，但是当中延伸出来一些很有意义的idea，比如kernel。 SVM如图所示，假定在$x$空间，存在两分类的点集，标记实心的点为$x^+$，空心的点为$x^-$，$w$为与$H_3$垂直的向量，则$H_3$表示为$w^Tx+b$。注意：$w$是与分界线垂直的向量。实心和空心区域的点集可以分别表示为：$$w^Tx^++b\\geq 1 \\\\w^Tx^-+b\\leq -1.$$这里的1代表不确定常量（常量在不等式左右两边同除，结果为1）。 现在我们想找到SVM的目标函数。我们的主要目的是maximize两个集合之间的距离，所以取在边界上的两个集合中的点，即$x^+$和$x^-$（注意：边界上的点即为support vector），则主要目的转变成maximize这两个点的距离，公式如下：$$\\begin{cases}w^Tx^+ + b = 1 \\\\w^Tx^- + b = -1 \\\\x^+ - x^- = \\lambda.\\end{cases}$$在第三个式子左右两边同乘$w^T$，得到$\\lambda = \\frac{2}{w^Tw}$。所以，$||x^+ - x^-||=\\frac{2}{w^Tw}\\cdot\\sqrt{w^Tw}=\\frac{2}{\\sqrt{w^Tw}}$，我们想要maximize这个margin值，即minimize这个margin值的倒数。即目标函数为：$$min \\; \\frac{1}{2}w^Tw \\\\s.t. \\; y_i(w^Tx_i+b) \\geq 1, \\; \\forall i.$$ 其中$i$表示所有样本。这是primal的表示形式，我们现在用拉格朗日求其dual，则有：$$L(w,b,\\lambda) = \\frac{1}{2}w^Tw+\\sum_i \\lambda_i [1-y_i(w^Tx_i+b)], \\\\s.t. \\; \\sum_i\\lambda_i \\geq 0.$$所以有，$\\frac{\\partial L}{\\partial w} = w-\\sum_i \\lambda_iy_ix_i=0$，得到$w=\\sum_i\\lambda_iy_ix_i$。同理，我们对$b$进行求导，有$\\frac{\\partial L}{\\partial b} = \\sum_i\\lambda_iy_i=0$。所以化简上面的公式，我们有：$$H({\\lambda_i}) = -\\frac{1}{2}(\\sum_i\\lambda_iy_ix_i)^T(\\sum_j\\lambda_jy_jx_j) + \\sum_i\\lambda_i, \\\\s.t. \\; \\sum_i\\lambda_i\\geq 0.$$ 我们把这个式子写成向量形式，即$H=-\\frac{1}{2}\\overrightarrow{\\lambda}^TK\\overrightarrow{\\lambda}+\\overrightarrow{1}\\lambda$，其中$K=(x_iy_i)^T(x_jy_j)$。注意：由定义可以看出，$\\lambda$在不是边界上的点是为0，所以只有边界上的点定义了$w$。这里就引出来一个非常重要的概念kernel，什么样的矩阵可以写成kernel形式呢？Mercer’s theorem对其进行了阐述。简单来说，就是一个对称的半正定的矩阵就可以写成kernel的表示形式，即symetric, PSD matrix $\\implies$ kernel ($x\\to\\phi(x)$)。这里的kernel有三种形式： linear polynomial RBF (类似Gaussian). 最后一个RBF相当于映射到无穷维的空间$\\phi(x)$，表示形式是$K(x_i, x_j)=\\lambda exp(-\\frac{||x_i-x_j||_2^2}{\\sigma^2})$。总的kernel形式可以表示为$K(i,j)=\\phi(x_i)^T\\phi(x_j)$。 对于SVM，我们可以将目标函数引入一个slack variable $\\zeta_i$，则目标函数为：$$min \\; \\frac{1}{2}w^Tw+c\\sum_i\\zeta_i \\\\s.t. \\; y_i(w^Tx_i+b)\\geq 1-\\zeta_i, \\; \\forall_i \\zeta_i\\geq 0.$$ SVRSVR即support vector regression，目的跟regression一样，即用一条线拟合一堆点。其目标函数为：$$min \\; \\frac{1}{2}w^Tw \\\\s.t. \\; w^Tx_i + b -y \\leq \\epsilon, \\; w^Tx_i+b-y\\geq -\\epsilon.$$推导过程同SVM。 SVCSVC即support vector clustering，目的是将点集聚类。它将点的类想象成不同的小山谷，分类即找出山谷的分布$P(x)$，投影到高维空间后，点集分布在一个半径为$R$的球内，球外的点即outliers，点集这时可以用一个高斯分布来描述。 目标函数为：$$min \\; R^2 \\\\s.t. \\; ||\\phi(x_i)-\\mu||_2^2 \\leq R^2.$$其中，$\\mu=\\frac{1}{N}\\sum_i\\phi(x_i)$。要注意两点： $dist(\\hat{x}, \\mu)$可以写成kernel的形式，即$\\phi(x_i)^T\\phi(x_i)$; $R$的设置可以进行分类。","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"SVM","slug":"SVM","permalink":"http://cuiyungao.github.io/tags/SVM/"}]},{"title":"fastText与Word2Vec之间的比较","slug":"fastvsword","date":"2016-08-09T01:29:03.000Z","updated":"2016-08-09T02:08:42.000Z","comments":true,"path":"2016/08/09/fastvsword/","link":"","permalink":"http://cuiyungao.github.io/2016/08/09/fastvsword/","excerpt":"本篇文章总结了试验fastText跟Word2Vec在embedding上的performance，源于这篇文章。","text":"本篇文章总结了试验fastText跟Word2Vec在embedding上的performance，源于这篇文章。 Dataset训练embedding的数据集有两个，一个是text8 corpus，一个是nltk自带的brown corpus。Groundtruth是questtions-words文本，可以从这里下载。 text8 Corpus Download1wget http://mattmahoney.net/dc/text8.zip brown Corpus Download12345678import nltk# 从当中选择brown corpus进行下载nltk.download()# Generate brown corpus text filewith open(&apos;brown_corp.txt&apos;, &apos;w+&apos;) as f: for word in nltk.corpus.brown.words(): f.write(&apos;&#123;word&#125; &apos;.format(word=word)) Model Training用fastText和Word2Vec分别对上述两个数据集进行训练，得到word embeddings。 fastText Training下载fastText源码，对上述两个数据集进行训练。12./fasttext skipgram -input brown_corp.txt -output brown_ft./fasttext skipgram -input text8 -output text8_ft Word2Vec TrainingWord2Vec的训练基于gensim，采用logging来对过程进行输出。123456789101112131415from nltk.corpus import brownfrom gensim.models import Word2Vecfrom gensim.models.word2vec import Text8Corpusimport logginglogging.basicConfig(format=&apos;%(asctime)s : %(levelname)s : %(message)s&apos;)logging.root.setLevel(level=logging.INFO)MODELS_DIR = &apos;models/&apos;brown_gs = Word2Vec(brown.sents())brown_gs.save_word2vec_format(MODELS_DIR + &apos;brown_gs.vec&apos;)text8_gs = Word2Vec(Text8Corpus(&apos;text8&apos;))text8_gs.save_word2vec_format(MODELS_DIR + &apos;text8_gs.vec&apos;) Comparison用questions-words.txt提供的数据作为Groundtruth，从semantic和syntactic两方面来对两种embedding的方法进行比较。 Based on Brown Corpus12345678910111213141516171819202122232425262728293031from gensim.models import Word2Vecdef print_accuracy(model, questions_file): print(&apos;Evaluating...\\n&apos;) acc = model.accuracy(questions_file) for section in acc: correct = len(section[&apos;correct&apos;]) total = len(section[&apos;correct&apos;]) + len(section[&apos;incorrect&apos;]) total = total if total else 1 accuracy = 100*float(correct)/total print(&apos;&#123;:d&#125;/&#123;:d&#125;, &#123;:.2f&#125;%, Section: &#123;:s&#125;&apos;.format(correct, total, accuracy, section[&apos;section&apos;])) sem_correct = sum((len(acc[i][&apos;correct&apos;]) for i in range(5))) sem_total = sum((len(acc[i][&apos;correct&apos;]) + len(acc[i][&apos;incorrect&apos;])) for i in range(5)) print(&apos;\\nSemantic: &#123;:d&#125;/&#123;:d&#125;, Accuracy: &#123;:.2f&#125;%&apos;.format(sem_correct, sem_total, 100*float(sem_correct)/sem_total)) syn_correct = sum((len(acc[i][&apos;correct&apos;]) for i in range(5, len(acc)-1))) syn_total = sum((len(acc[i][&apos;correct&apos;]) + len(acc[i][&apos;incorrect&apos;])) for i in range(5,len(acc)-1)) print(&apos;Syntactic: &#123;:d&#125;/&#123;:d&#125;, Accuracy: &#123;:.2f&#125;%\\n&apos;.format(syn_correct, syn_total, 100*float(syn_correct)/syn_total))MODELS_DIR = &apos;models/&apos;word_analogies_file = &apos;questions-words.txt&apos;print(&apos;\\nLoading FastText embeddings&apos;)ft_model = Word2Vec.load_word2vec_format(MODELS_DIR + &apos;brown_ft.vec&apos;)print(&apos;Accuracy for FastText:&apos;)print_accuracy(ft_model, word_analogies_file)print(&apos;\\nLoading Gensim embeddings&apos;)gs_model = Word2Vec.load_word2vec_format(MODELS_DIR + &apos;brown_gs.vec&apos;)print(&apos;Accuracy for word2vec:&apos;)print_accuracy(gs_model, word_analogies_file) 结果如下： Loading FastText embeddingsAccuracy for FastText:Evaluating…0/1, 0.00%, Section: capital-common-countries0/1, 0.00%, Section: capital-world0/1, 0.00%, Section: currency0/1, 0.00%, Section: city-in-state36/182, 19.78%, Section: family498/702, 70.94%, Section: gram1-adjective-to-adverb110/132, 83.33%, Section: gram2-opposite675/1056, 63.92%, Section: gram3-comparative140/210, 66.67%, Section: gram4-superlative426/650, 65.54%, Section: gram5-present-participle0/1, 0.00%, Section: gram6-nationality-adjective153/1260, 12.14%, Section: gram7-past-tense318/552, 57.61%, Section: gram8-plural245/342, 71.64%, Section: gram9-plural-verbs2601/5086, 51.14%, Section: totalSemantic: 36/182, Accuracy: 19.78%Syntactic: 2565/4904, Accuracy: 52.30%Loading Gensim embeddingsAccuracy for word2vec:Evaluating…0/1, 0.00%, Section: capital-common-countries0/1, 0.00%, Section: capital-world0/1, 0.00%, Section: currency0/1, 0.00%, Section: city-in-state54/182, 29.67%, Section: family8/702, 1.14%, Section: gram1-adjective-to-adverb0/132, 0.00%, Section: gram2-opposite72/1056, 6.82%, Section: gram3-comparative0/210, 0.00%, Section: gram4-superlative14/650, 2.15%, Section: gram5-present-participle0/1, 0.00%, Section: gram6-nationality-adjective28/1260, 2.22%, Section: gram7-past-tense4/552, 0.72%, Section: gram8-plural8/342, 2.34%, Section: gram9-plural-verbs188/5086, 3.70%, Section: totalSemantic: 54/182, Accuracy: 29.67%Syntactic: 134/4904, Accuracy: 2.73% 从运行结果可以看到，fastText的semantic accuracy比Word2Vec要稍微差一点儿，但是Syntactic accuracy的效果明显优于Word2Vec。这是因为1中提到，fastText中word embeddings是由他们的n-gram embeddings来表示，所以形态上相似的词的embeddings也会比较类似。比如：$$embedding(amazing)-embedding(amazingly) = embedding(calm)-embedding(calmly).$$ Based on text8 Corpus123456789print(&apos;Loading FastText embeddings&apos;)ft_model = Word2Vec.load_word2vec_format(MODELS_DIR + &apos;text8_ft.vec&apos;)print(&apos;Accuracy for FastText:&apos;)print_accuracy(ft_model, word_analogies_file)print(&apos;Loading Gensim embeddings&apos;)gs_model = Word2Vec.load_word2vec_format(MODELS_DIR + &apos;text8_gs.vec&apos;)print(&apos;Accuracy for word2vec:&apos;)print_accuracy(gs_model, word_analogies_file) 结果如下： Loading FastText embeddingsAccuracy for FastText:Evaluating…322/506, 63.64%, Section: capital-common-countries609/1452, 41.94%, Section: capital-world36/268, 13.43%, Section: currency286/1520, 18.82%, Section: city-in-state134/306, 43.79%, Section: family556/756, 73.54%, Section: gram1-adjective-to-adverb186/306, 60.78%, Section: gram2-opposite838/1260, 66.51%, Section: gram3-comparative270/506, 53.36%, Section: gram4-superlative556/992, 56.05%, Section: gram5-present-participle1293/1371, 94.31%, Section: gram6-nationality-adjective490/1332, 36.79%, Section: gram7-past-tense888/992, 89.52%, Section: gram8-plural365/650, 56.15%, Section: gram9-plural-verbs6829/12217, 55.90%, Section: totalSemantic: 1387/4052, Accuracy: 34.23%Syntactic: 5442/8165, Accuracy: 66.65%Loading Gensim embeddingsAccuracy for word2vec:Evaluating…153/506, 30.24%, Section: capital-common-countries248/1452, 17.08%, Section: capital-world27/268, 10.07%, Section: currency172/1571, 10.95%, Section: city-in-state218/306, 71.24%, Section: family88/756, 11.64%, Section: gram1-adjective-to-adverb45/306, 14.71%, Section: gram2-opposite716/1260, 56.83%, Section: gram3-comparative179/506, 35.38%, Section: gram4-superlative325/992, 32.76%, Section: gram5-present-participle702/1371, 51.20%, Section: gram6-nationality-adjective343/1332, 25.75%, Section: gram7-past-tense401/992, 40.42%, Section: gram8-plural219/650, 33.69%, Section: gram9-plural-verbs3836/12268, 31.27%, Section: totalSemantic: 818/4103, Accuracy: 19.94%Syntactic: 3018/8165, Accuracy: 36.96% 实验结果可以看出，用在较大的数据集上，fastText的优势表现得更加明显，当然word2vec的Syntactic accuracy提高得也比较明显。所以总的来看，fastText比word2vec在word embedding上更好，特别是对于syntactic information。 实验中用到的HyperparametersGensim word2vec和fastText用了相似的参数，dim_size = 100, window_size = 5, num_epochs = 5。但是它们的模型完全不同，尽管有很多相似性。 Reference[1]Enriching Word Vectors with Subword Information[2]Efficient Estimation of Word Representations in Vector Space","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"},{"name":"Word Embedding","slug":"Word-Embedding","permalink":"http://cuiyungao.github.io/tags/Word-Embedding/"},{"name":"fastText","slug":"fastText","permalink":"http://cuiyungao.github.io/tags/fastText/"}]},{"title":"用户在线广告点击行为的预测","slug":"adclick","date":"2016-08-04T05:46:40.000Z","updated":"2016-08-17T09:21:40.000Z","comments":true,"path":"2016/08/04/adclick/","link":"","permalink":"http://cuiyungao.github.io/2016/08/04/adclick/","excerpt":"本篇是对ad点击行为进行预测的总结，包括张伟楠在携程技术中心的一个演讲以及北大硕士Kintocai的总结。集中于深度学习在Multi-field Categorical这类数据集上的应用。","text":"本篇是对ad点击行为进行预测的总结，包括张伟楠在携程技术中心的一个演讲以及北大硕士Kintocai的总结。集中于深度学习在Multi-field Categorical这类数据集上的应用。 总结了FM和FNN算法在处理多值分类数据方面的优势，并把这两种算法与神经网络在特征变量处理方面的差异做了对比，最后通过用户在线广告点击行为预测比较了LR、FM、FNN、CCPM、PNN-I等算法的效果。 Multi-field Categorical Data目前深度学习主要用在连续的数据集上，比如视觉、语音识别和自然语言处理上。但是用户点击率预测这种课题是一个离散的数据Multi-field Categorical Data，会有多种不同的字段，比如:[Weekday=Wednesday, Gender=Male, City=Shanghai, …]，我们想要识别这些特征之间的关系。传统的做法是应用One-Hot Binary的编码方式去处理这类数据，比如Weekday有7个取值，我们就将其编译为7维的二进制向量，其中只有Wednesday是1，其它都是0，因为它只有一个特征值；Gender只有两维，其中一维是1；如果有一万个城市的话，那City就有一万维，只有Shanghai取值为1，其它是0。 最终会得到一个高维稀疏向量。这个数据集不能直接用神经网络训练，因为如果直接用One-Hot Binary进行编码的话，那输入特征至少有一百万，第一层至少需要500个节点，那么第一层我们就需要训练5亿个参数，那就需要20亿或者50亿的数据集，而获得如此大的数据集比较困难。 FM, FNN以及PNN模型作者将FM跟Neural Network结合在一起，改进了之前的NN模型。FM (Factorization Machine)被认为是最有效的embedding model： 第一部分仍然是Logistic Regression，第二部分是特征之间跟目标变量之间的关系（通过两两向量之间的点积，点积大于0，表示这两个特征的组合跟目标值是正相关的）。这种算法在推荐系统领域应用比较广泛。作者用FM算法对底层输入field的one-hot binary编码进行embedding，把稀疏的二进制特征向量映射到dense real层，之后再把dense real层作为输入变量进行建模，这样就避免了高维二进制输入数据的计算复杂度。模型图如下： FNN跟一般的NN算法的区别是：大部分神经网络模型对向量之间的处理采用的是加法操作，相当于逻辑“或”；而FM则是通过向量之间的乘法来衡量两者之间的关系，这就相当于逻辑“且”。显然“且”比“或”更能严格区分目标变量。如下图所示。在第二层对向量的乘积处理中（比如上图蓝色节点直接为两个向量乘积，其连接边上没有参数需要学习），每一个field都只会被映射到一个low-dimensional vector，且field和field之间没有相互影响。 乘法关系的建模，可以采用内积或者外积，如下图。外积得到的矩阵中，只有对角线有值，即内积的结果，所以内积操作可以看做是外积操作的一种特殊情况。 PNN的神经网络图如下所示。进行embedding之后的输入数据有两种处理方法，一种是对该层的任意两个feature进行内积或者外积处理得到上图的蓝色节点，另外一种是把这些feature直接和1相乘复制到上一层的Z中，然后Z和P接在一起作为神经网络的输入层。 对特征进行内积或者外积处理会产生一个复杂度的问题：weight矩阵比较庞大。解决方法：由于weight矩阵是个对称矩阵，可以用factorization来处理这个对称阵，把它转换成矩阵乘矩阵的转置，这样就会减少需要训练的参数，如下图所示。 Metrics评估模型主要看以下几个指标： Area under ROC curve (AUC) Log loss Root mean squared error (RMSE) Relative Information Gain (RIG) Experiments实验比较了dropout、hidden layer的层数、迭代次数、不同层级节点分布形态以及不同的Activation Function的效果。 Summary of PNN 深度学习在Multi-field的数据集上也有显著的应用效果； 通过外积和内积找到特征之间的相关关系； 在广告点击率的预测中，PNN效果优于其他模型。 Wide and Deep Learning这个模型结合了离散LR以及DNN的方法，category feature进行embedding输入到DNN，其它特征通过LR学习。LR是feature interaction，比较细粒度；而DNN则强调generalization，可以结合这两者的优势。把position bias特征放在最后一个隐层很有意义，可以避免一些不适合结合的features。 DNN CTR Prediction通过embedding的方式把离散特征转化为Dense Feature， 输入层同时可以增加其他Dense 特征， 比如CTR统计特征、similarity 特征、pv、click等等。每一个隐层都可以增加新的特征输入， 尤其是最后一个隐层。","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"},{"name":"Ad","slug":"Ad","permalink":"http://cuiyungao.github.io/tags/Ad/"}]},{"title":"Tweet2Vec:Learning Tweet Embeddings Using Character-Level CNN-LSTM Encoder-Decoder","slug":"tweet2vec","date":"2016-08-03T12:41:08.000Z","updated":"2016-08-03T14:15:38.000Z","comments":true,"path":"2016/08/03/tweet2vec/","link":"","permalink":"http://cuiyungao.github.io/2016/08/03/tweet2vec/","excerpt":"SIGIR’16的文章，来自MIT Media Lab。Vosoughi, Soroush, Prashanth Vijayaraghavan, and Deb Roy. “Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM Encoder-Decoder.”","text":"SIGIR’16的文章，来自MIT Media Lab。Vosoughi, Soroush, Prashanth Vijayaraghavan, and Deb Roy. “Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM Encoder-Decoder.” Major Idea作者利用tweet用户的hashtag作为groundtruth，基于Character-level对每一条的embeddings进行学习，对hashtag进行推测，并与已有的word2vec跟Glove进行对比，发现Tweet2Vec的准确率最高。 Techniques基于Character-level，以及作者曾经提出Bi-directional gated recurrent unit (Bi-GRU)’14来学习tweet的表示形式。 Character-level CNN Tweet Model作者采用temporal convolutional和temporal max-pooling operations，即计算一维的输入和输出之间的convolution和pooling函数。给定一个离散的输入函数$f(x)\\in [1,l]\\mapsto \\mathbb{R}$，一个离散的kernel函数$k(x)\\in [1,m]\\mapsto \\mathbb{R}$，stride $s$，$k(x)$和$f(x)$之间的卷积$g(y)\\in [1,(l-m+1)/s]\\mathbb{R}$, 对$f(x)$的pooling操作$h(y)\\in [1, (l-m+1)/s]\\mapsto \\mathbb{R}$：$$g(y) = \\sum_{x=1}^m k(x)\\cdot f(y\\cdot s - x + c) \\\\h(y) = max_{x=1}^m f(y\\cdot s - x + c)$$其中，$c=m-s+1$是一个补偿常量。tweet的character set包含英语字母，数字，特殊字符和不明确的字符，总共统计了70个字符。每个字符被encode成一个one-hot vector $x_i\\in {0,1}^{70}$，单个tweet的最大长度是150，所以每个tweet被表示成了一个binary matrix $x_{1…150}\\in {0,1}^{150\\times70}$。表示成matix的tweet输入到一个包含4层一维卷基层的deep model。每个卷基层操作采用filter $w\\in \\mathbb{R}^l$来获取n-gram的character feature。一般来说，对于一个tweet $s$，在层$h$的一个特征$c_i$由下面式子生成：$$c_i^{(h)} (s) = g(w^{(h)}\\cdot \\hat{c}_i^{(h-1)} + b^{(h)})$$其中，$\\hat{c}_i^{(0)}=x_{i…i+l-1}$, $b^{(h)}\\in \\mathbb{R}$是h层的bias，$g$是一个rectified linear unit。整体框架如下图所示： Long-Short Term Memory (LSTM)给定一个输入序列$X=(x_1, x_2, …, x_N)$，LSTM计算hidden vector sequence $h=(h_1, h_2, …, h_N)$和output vector sequence $Y=(y_1, y_2, …, y_N)$。每一个时间步骤，一个模块的输出是由一组gates (前一个hidden state $h_{t1}$组成的函数)和当前时间步骤的输入控制的。forget gate $f_t$，input gate $i_t$和output gate $o_t$。这些gates集中决定了当前memory cess $c_t$的过渡和当前的hidden state $h_t$。LSTM的过渡函数定义如下：$$i_t = \\sigma(W_i\\cdot [h_{t-1}, x_t]+b_i) \\\\f_t = \\sigma(W_f\\cdot [h_{t-1}, x_t]+b_f) \\\\l_t = tanh(W_l\\cdot [h_{t-1}, x_t]+b_l) \\\\o_t = \\sigma(W_o\\cdot [h_{t-1}, x_t]+b_o) \\\\c_t = f_t\\odot c_{t-1} + i_t\\odot l_t \\\\h_t = o_t\\odot tanh(c_t)$$其中，$\\odot$表示component-wise multiplicaiton。$f_t$控制过去的memory cell要舍弃的信息，而$i_t$控制新的信息储存在current memory cell的程度，$o_t$是基于memory cell $c_t$的输出。1LSTM是学习`long-term denpendencies`，所以在卷基层之后用LSTM学习获取特征的序列中存在的依赖。 在seq-to-seq中，LSTM定义了output上的分布，之后用softmax来序列预测tokens。$$P(Y|X)=\\prod_{t\\in [1,N]} \\frac{exp(g(h_{t-1}, y_t))}{\\sum_{y^{\\prime} exp(g(h_{t1}, y_t^{\\prime}))}}$$其中，g是activation function。 Combined ModelFigure 1呈现了整个encoder-decoder过程。输入是由matrix呈现的tweet，字符由one-hot vector表示。Encoder部分，在Character-level CNN的较高层卷积之后不经过pooling，直接作为LSTM的输入，encoder过程可以表示为：$$H^{conv} = CharCNN(T) \\\\h_t = LSTM(g_t, h_{t-1}))$$其中，g=H^{conv}是一个特征矩阵，每一行代表LSTM的一个time step，$h_t$是$t$时刻的hidden representation。LSTM作用于$H^{conv}$的每一层，生成下一个序列的embedding。最终输出结果$enc_N$用来表示整条tweet。 Decoder部分，用两层的LSTM作用于encoded representation。每一时间步骤中，字符的预测是：$$P(C_t|\\cdot) = softmax(T_t, h_{t-1})$$其中，$C_t$指的是时间$t$的字符，$T_t$表示时刻$t$的one-hot vector。最终的结果是一个decoded tweet matrix $T^{dec}$，与实际的tweet进行比较，并学习模型的参数。 Experiments实验用于两个分类任务： tweet semantic relatedness和tweet sentiment classification。3 million tweets。感觉数据量也挺小。准确率在0.6~0.7左右。 SummaryCharacter level的deep learning的优势:占用内存少，不需要存储所有word的表示，只需要存储已有的character；不依赖于语言，只需要字符；而且不需要NLP的预处理，比如word segmentation。劣势是：运行速度慢，在文章的实验中，word2vec的速度是tweet2vec的6~7倍，原因是从word变成character输入意味着GRU的输入量变大。Character level可以用在NLP的很多方面，比如named entity recognition, POS tagging, text classification, and language modeling。 文章highlight了tweet2vec的优势，对word segmentation error, spelling mistakes, special characters, interpret emojis, in-vocab tokens非常有效。 Published paper.","categories":[{"name":"papers","slug":"papers","permalink":"http://cuiyungao.github.io/categories/papers/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"}]},{"title":"Impactful ML Topics at ICML'16","slug":"memory-network","date":"2016-07-04T07:32:54.000Z","updated":"2016-08-03T03:44:02.000Z","comments":true,"path":"2016/07/04/memory-network/","link":"","permalink":"http://cuiyungao.github.io/2016/07/04/memory-network/","excerpt":"Summary from a talk on ICML’16.","text":"Summary from a talk on ICML’16. Deep Residual NetworksResidual networks可以在深度增加的同时提高准确率，可以产生许多问题的representations。适当的weight initialization和batch normalization使得网络可以。 Memory Networks for Language UnderstandingMemory Networks是结合large memory跟可读可写的learning component的一类模型。结合了Reasoning with attention over memory (RAM)。大部分ML有limited memory，针对”low level” tasks，比如object detection。 得到word Embedding的过程是一个encoding的过程。","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"}]},{"title":"Find Researchers Here","slug":"researcher","date":"2016-07-04T07:27:10.000Z","updated":"2016-07-04T07:32:39.000Z","comments":true,"path":"2016/07/04/researcher/","link":"","permalink":"http://cuiyungao.github.io/2016/07/04/researcher/","excerpt":"With researchers in some fields related to NLP and deep learning contained.","text":"With researchers in some fields related to NLP and deep learning contained. Natural Language Processing Jason WestonLink. Research Scientist at Facebook.","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"}]},{"title":"Predicting Amazon Ratings Using Neural Networks","slug":"predictingamazon","date":"2016-06-30T09:41:17.000Z","updated":"2016-06-30T09:51:31.000Z","comments":true,"path":"2016/06/30/predictingamazon/","link":"","permalink":"http://cuiyungao.github.io/2016/06/30/predictingamazon/","excerpt":"The paper compares 3 different methods, bag-of-words model, word2vec (using RandomForests) and word2vec (using K Nearest Neighbors).","text":"The paper compares 3 different methods, bag-of-words model, word2vec (using RandomForests) and word2vec (using K Nearest Neighbors). Word vectors经常被用作features应用于许多NLP领域，有两种模型CBOW和Skip-Gram。Doc2vec是word2vec的改进版，区别在于model architecture。在doc2vec中，algorithms是distributed memory (dm) and distributed bag of word (dbow)，但是其分类效果并不如word2vec。 结果显示word2vec的准确率是最高的，而且运行时间最短。 Original paper can be download here.","categories":[{"name":"papers","slug":"papers","permalink":"http://cuiyungao.github.io/categories/papers/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"},{"name":"Rating Prediction","slug":"Rating-Prediction","permalink":"http://cuiyungao.github.io/tags/Rating-Prediction/"}]},{"title":"User Modeling with Neural Network for Review Rating Prediction","slug":"usermodeling","date":"2016-06-30T03:38:13.000Z","updated":"2016-06-30T09:41:46.000Z","comments":true,"path":"2016/06/30/usermodeling/","link":"","permalink":"http://cuiyungao.github.io/2016/06/30/usermodeling/","excerpt":"[IJCAI’15] User Modeling with Neural Network for Review Rating Prediction. Duyu Tang, Bing Qin, Ting LiuProceeding of The 24th International Joint Conference on Artificial Intelligence.","text":"[IJCAI’15] User Modeling with Neural Network for Review Rating Prediction. Duyu Tang, Bing Qin, Ting LiuProceeding of The 24th International Joint Conference on Artificial Intelligence. 作者Duyu Tang的分析集中于sentiment analysis，一年发了很多NLP的顶会文章（EMNLP, ACL, IJCAI, etc.）。 本文通过考虑user information来改善review rating prediction。启发源于lexical composition model，将compositional modifier作为一个matrix, 用matrix-vector mulplication作为compositon function。本文延伸lexical semantic compositio models，建立了user-word composition vector model (UWCVM)，并将结果作为feature用supervised learning framework来进行review rating prediction。 Problem DefinitionReview rating prediction可以看做是a classificaiton/regression problem，由Pang and Lee 2005最开始做，采用metric labeling framework。Rating的performance很大程度上依赖于feature representation的选择。 Problem: Given a review $r_{k_j}$ comprised of $n$ words $\\{w_1,w_2,…,w_n\\}$ written by user $u_k$ as input, review rating prediction aims at infering the numeric rating (1~4 or 1~5 stars) of $r_{k_j}$. The problem can be regarded as a multi-class classification problem by inferring a discrete rating score. MethodologyThe method includes two composition models, the user-word composition vector model (DWCVM) and the document composition vector model (DCVM). The former model将原始的word vectors加入user information，the latter model则将modified word vectors转化成review representation，并用其作为feature来进行rating prediction。Training的过程采用Pang and Lee提出的supervised metric labeling的方法。整个过程如下图表示： User-Word Composition Vector Model整个model将original word vectors融入user information。融入的方法有两种：additive和multiplicative。给定两个向量$v_1$和$v_2$，Additive的结合方式认为输出向量$p$是a linear function of Cartesian product of $v_1$ and $v_2$，如下： $$p = \\mathbf{A}\\times v_1 + \\mathbf{B}\\times v_2$$其中$\\mathbf{A}$和$\\mathbf{B}$是matrices parameters，用来encode $v_1$, $v_2$对$p$的贡献。Multiplicative的结合方式认为输出向量$p$是a linear function of the tensor product of $v_1$ and $v_2$，如下： $$p = \\mathbf{T}\\times v_1 \\times v_2 = \\mathbf{U}_1 \\times v_2$$其中$\\mathbf{T}$是一个rank为3的tensor，将$v_1,v_2$的tensor product映射到$p$上。$\\mathbf{T}$和$\\v_1$的Partial product可以被看做生成新的矩阵$\\mathbf{U}_1$，这个矩阵可以modify原始的word vectors。这两种结合方式可以用下图表示： 文章选用multiplicative composition，因为这种结合方式符合最初的用user information来改善word vectors的想法。后面的实验也验证了multiplicative composition的结合方式准确率要高于additive composition。 为了减少parameter size，user representation被用low-rank plus diagonal approximation来表示：$\\mathbf{U}_k=\\mathbf{U}_{k1} \\times \\mathbf{U}_{k2} + diag(u^{\\prime})$，其中$\\mathbf{U}_{k1}\\in \\mathbb{R}^{d\\times r}$, $\\mathbf{U}_{k2}\\in \\mathbb{R}^{r\\times d}$, $u^{\\prime}\\in \\mathbb{R}^d$。$u^{\\prime}$是每个user共享的background representation，以应对某些在test set中有而在training set中没有的users，即Out-Of-Vocabulary situation。最终modified word vectors $p_i$： $$p_i = tanh(\\mathbf{e}_{ik}) = tanh(\\mathbf{U}_k\\times \\mathbf{e}_i)\\; = tanh((\\mathbf{U}_{k1}\\times \\mathbf{U}_{k2} + diag(u^{\\prime})) \\times \\mathbf{e}_i)$$ Document Composition Vector Model文中采用一个简单而有效的方法paper, recursivley uses biTanh function来生成document representation: $$biTanh(p) = \\sum_{i=1}^n tanh(p_{i-1} + p_i)$$Each sentence将user-modified word vectors作为输入，得到sentence vectors；然后再将sentence vectors输入到biTanh得到最终的document vector $vec(doc)$。biTanh的Recursive use可以看做是two pairs of bag-of-word convolutional neural network，其中window size是2，parameters通过addition和tanh来定义。 Rating Prediction with Metric LabelingReview representation被用来进行review rating prediction，方法是metric labeling framework。 Model Training获取back-propagation中对于whole set of parameters的derivative of the loss, 然后用stochastic gradient descent with mini-batch来更新这些parameters。文中用dropout来避免neural network being over-fitting。 Note: Cases that rating does not match with review texts is not considered. [Zhang et al., SIGIR’14] Paper Experiment实验数据集有两个：Yelp13和RT05。实验结果说明: Semantic composition可以提高预测的准确度，并且结合metric labeling可以改善结果，因为它基于”similar items, similar labels”的idea。 一个用户有越多的reviews，则这个用户的rating可以被很好地估计。 SSPE [Tang et al., 2014a] paper更适用于short review.","categories":[{"name":"papers","slug":"papers","permalink":"http://cuiyungao.github.io/categories/papers/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"},{"name":"Rating Prediction","slug":"Rating-Prediction","permalink":"http://cuiyungao.github.io/tags/Rating-Prediction/"},{"name":"Neural Network","slug":"Neural-Network","permalink":"http://cuiyungao.github.io/tags/Neural-Network/"}]},{"title":"UFLDL Tutorial","slug":"UFLDL","date":"2016-06-29T14:39:40.000Z","updated":"2016-06-29T14:43:23.000Z","comments":true,"path":"2016/06/29/UFLDL/","link":"","permalink":"http://cuiyungao.github.io/2016/06/29/UFLDL/","excerpt":"The page records the knowledge learnt from UFLDL tutorial provided by Stanford. The resource can be found here.","text":"The page records the knowledge learnt from UFLDL tutorial provided by Stanford. The resource can be found here. The tutorial主要介绍unsupervised feature learning和deep learning，以及如何实现相关的算法。由于算法实现主要是matlab，所以主要记录里面的知识要点。","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"}]},{"title":"Good-Turing Estimation 大数据处理平滑算法","slug":"goodturing","date":"2016-06-29T09:22:55.000Z","updated":"2016-08-03T04:34:19.000Z","comments":true,"path":"2016/06/29/goodturing/","link":"","permalink":"http://cuiyungao.github.io/2016/06/29/goodturing/","excerpt":"参数平滑算法","text":"参数平滑算法 参数平滑算法，是在训练数据不足时，采用某种方式对统计结果和概率估计进行必要的调整和修补，以降低由于数据稀疏现象带来的统计误差。 在实际应用中，数据稀疏会产生大量空值，影响后续处理的性能和效果，所以需要平滑算法。 Good-Turing估计经常用在数据平滑方面。基本思想是：将统计参数按出现次数聚类，然后用出现次数加1的类来估计当前类。假定，一个元素在文本$W$中出现$r$次的概率是$\\theta(r)$，$N_r$表示元素在$W$中正好出现$r$次的个数，即$N_r=|{x_j: \\sharp(x_j)=r}|$，满足： $$N = \\sum_{r} rN_r.$$ Good-Turing估计$\\theta(r)$为：$\\hat{\\theta}(r) = \\frac{1}{N} (r+1) \\frac{N_{r+1}}{N_r}$. 具体的分析过程可以看这里。","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"Data Mining","slug":"Data-Mining","permalink":"http://cuiyungao.github.io/tags/Data-Mining/"}]},{"title":"Collaborative Filtering (CF)","slug":"cf","date":"2016-06-29T09:22:55.000Z","updated":"2016-07-19T02:31:24.000Z","comments":true,"path":"2016/06/29/cf/","link":"","permalink":"http://cuiyungao.github.io/2016/06/29/cf/","excerpt":"This webpage contains information and implementation related to collaborative filtering.","text":"This webpage contains information and implementation related to collaborative filtering. 对于recommender system，最常用的两个方法是Content-Based和Collaborative Filtering (CF)。CF基于users对items的态度来推荐items，即&quot;wisdom of the crows&quot;。相反，CB关注items的属性，用items之间的相似度来进行推荐。 CF分为Memory-based CF和Model-based CF。 Implementation StepsWe use scikit-learn library将数据分成testing set和training set。For example, 12345import pandas as pdfrom sklearn import cross_validation as cvheader = [&apos;user_id&apos;,&apos;item_id&apos;,&apos;rating&apos;,&apos;timestamp&apos;]data = pd.read_csv(&apos;ml-100k/u.data&apos;, sep=&apos;\\t&apos;, names=header)train_data, test_data = cv.train_test_split(data, test_size=0.25) 基于cosine similarity，我们得到user_similarity和item_similarity，对于user-based CF,我们用下面的公式： $$\\hat{x}_{k,m} = \\bar{x}_k + \\frac{\\sum_{u_a} sim_u(u_k, u_a)(x_{a,m}-\\bar{x}_{u_a})}{\\sum_{u_a} \\|sim_u(u_k, u_a)\\|}$$ Users $k$ and $a$可以被当做是weights。Normalize确保评分在1~5之间。由于有相似taste的两个users可能在评分上一个偏高，一个偏低，所以前面需要加上user的平均rating作为bias。对于item-based CF,公式如下： $$\\hat{x}_{k,m} = \\frac{\\sum_{i_b} sim_i(i_m,i_b)(x_{k,b})}{\\sum_{i_b} \\|sim_i(i_m,i_b)\\|}$$ EvaluationMetric采用Root Mean Squared Error (RMSE),公式如下： $$RMSE = \\sqrt{\\frac{1}{N}\\sum (x_i-\\hat{x}_i)^2}$$ 实现采用sklearn提供的mean_square_error (MSE), RMSE是MSE的平方根。 1234567from sklearn.metrics import mean_squared_errorfrom math import sqrtdef rmse(prediction, ground_truth): # find nonzeros places in groundtruth and extract from the prediction accordingly prediction = prediction[ground_truth.nonzero()].flatten() ground_truth = ground_truth[ground_truth.nonzero()].flatten() return sqrt(mean_squared_error(prediction, ground_truth)) 12print &apos;User-based CF RMSE: &apos; + str(rmse(user_prediction, test_data_matrix))print &apos;Item-based CF RMSE: &apos; + str(rmse(item_prediction, test_data_matrix)) Drawback: Memory-based CF doesn’t scale to real-world scenarios and cannot solve cold-start problem well. Model-based CF methods are scalabel and can deal with higher sparsity level than memory-based, but also suffers from cold-start problem. Original Link. Also implemented locally. RecosystemRecommender system using parallel matrix factorization Link.. The author’s intro. Fast Recommendation for Activity StreamFast Recommendations for Activity Streams Using Vowpal Wabbit Link.","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"Recommend System","slug":"Recommend-System","permalink":"http://cuiyungao.github.io/tags/Recommend-System/"}]},{"title":"Memorization and Exploration in Recurrent Neural Language Models","slug":"memorization","date":"2016-06-28T14:27:33.000Z","updated":"2016-06-28T14:38:07.000Z","comments":true,"path":"2016/06/28/memorization/","link":"","permalink":"http://cuiyungao.github.io/2016/06/28/memorization/","excerpt":"Deep Learning | Los Angeles Meetup","text":"Deep Learning | Los Angeles Meetup The speaker explains recurrent memory networks.Link LSTM LSTM is a powerful sequential model. It has complicated design of input gate, forget gate, update gate and cell. LSTM achieves SOTA in many NLP tasks: sentence completion, sentiment analysis, parsing…. Recurrent Memory Networks amplify the power of RNN offer a way to interpret and discover dependencies in data outperform TreeLSTM models that explicitly use syntactic information in Sentence Completion Challenge First AttemptCombine linearly source context vector, target context (Memory block) and target hidden state LSTM Did not see any gain in BLEU Potentially bias toward target LM Gating combination might be essential for the model Better MemorizationBetter memorization leads to better translation LSTM does not offer any representation advantage compared to vanilla RNN Gradient in LSTM just flows nicer","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"}]},{"title":"Data Crawling","slug":"crawler","date":"2016-06-28T13:26:03.000Z","updated":"2016-08-03T04:42:01.000Z","comments":true,"path":"2016/06/28/crawler/","link":"","permalink":"http://cuiyungao.github.io/2016/06/28/crawler/","excerpt":"This webpage contains information about data crawling, such as some useful tools and summaries.","text":"This webpage contains information about data crawling, such as some useful tools and summaries. Data scraper for Facebook PagesLink. Data scraper for Facebook Pages, and also code accompanying the blog post How to Scrape Data From Facebook Page Posts for Statistical Analysis. An example is shown below. Web Crawler System in PythonLink. Write script in Python Powerful WebUI with script editor, task monitor, project manager and result viewer MySQL, MongoDB, Redis, SQLite, Elasticsearch; PostgreSQL with SQLAlchemy as database backend RabbitMQ, Beanstalk, Redis and Kombu as message queue Task priority, retry, periodical, recrawl by age, etc… Distributed architecture, Crawl Javascript pages, Python 2&amp;3, etc…","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"Data Mining","slug":"Data-Mining","permalink":"http://cuiyungao.github.io/tags/Data-Mining/"},{"name":"Crawler","slug":"Crawler","permalink":"http://cuiyungao.github.io/tags/Crawler/"}]},{"title":"Generative Topic Embedding:a Continuous Representation of Documents","slug":"acl16","date":"2016-06-28T12:30:48.000Z","updated":"2016-08-03T03:40:38.000Z","comments":true,"path":"2016/06/28/acl16/","link":"","permalink":"http://cuiyungao.github.io/2016/06/28/acl16/","excerpt":"[ACL’16] From 清华Zhu Jun‘s Group","text":"[ACL’16] From 清华Zhu Jun‘s Group Summary from 洪亮劼 Problem DefinitionGiven the hyperparameters $\\alpha$, $\\gamma$, $\\mu$, the learning objective is to find the embeddings $\\mathbf{V}$, the topics $\\mathbf{T}$, and the word-topic and document-topic distributions $p(\\mathbf{Z}_i, \\phi_i|d_i, \\mathbf{A}, \\mathbf{V}, \\mathbf{T})$. Here, $\\mathbf{Z},\\mathbf{T},\\phi$ denote the collection of the document-specific $\\{\\mathbf{Z}_i\\}_{i=1}^M$, $\\{\\mathbf{T}_i\\}_{i=1}^M$, $\\{\\phi_i\\}_{i=1}^M$. $\\mathbf{V}$, $\\mathbf{A}$, and $\\mathbf{T}_i$ stand for the embeddings, the bigram residuals, and the topics, respectively. As a whole, a generative word embedding model PSDVec (Li et al., 2015, ref), by incorporating topics into it. The new model is named TopicVec. TopicVec Model模型结合了topic modeling和word embeddings。In TopicVec, an embedding link function对一个topic中的word distribution进行建模，而不是LDA中的categorical distribution。这个link function的优势是semantic relatedness在embedding space当中被encoded。其它的过程跟LDA很相似，同样用Dirichlet priors来regularize topic distributions，用variational inference algorithm来进行optimization。这个过程可以得到跟words在同样embedding space的topic embeddings。Topic embeddings的目的是来估计underlying semantic centroids。 The whole process: For the $k$-th topic, draw a tpic embedding uniformly from a hyperball of radius $\\gamma$, i.e., $t_k\\sim Unif(B_{\\gamma})$; For each document $d_i$:(a) Draw the mixing proportions $\\phi_i$ from the Dirichlet prior Dir($\\alpha$);(b) For the $j$-th word: $\\;$ i. Draw topic assignment $z_{ij}$ from the categorical distribution Cat($\\phi_i$); $\\;$ ii. Draw word $w_{ij}$ from $\\mathbf{S}$ according to $P(w_{ij}|w_{i,j-c}:w_{i,j-1},z_{ij},d_i)$. Fig1. Graphical representation of TopicVec The notations are listed in the following table: Word Embeddings &amp; Topic ModelingWord embedding通过一个小的context window里面的local word collocation patterns,将words映射到一个低维连续的embedding space。而topic modeling通过同一文档里面的global word collocation patterns，将documents映射到一个低维的topic space。这两个可以互补，本文由此得到TopicVec Model。其中，topics由embedding vectors来表示，并在documents之间共享。每个word的probability由local context和topic来决定。Topic embedding由variational inference method产生，同时得到每个document的topic mixing probability。结合topic embedding和topic mixing probability，可以在低维连续空间里面得到每个document的representation。 Euclidean distance不是衡量两个embeddings之间相似度的最优方法，已有的方法大部分采用exponentiated cosine similarity作为link function，所以cosine similarity可能是较好的估计语义相似度的方法。 Inspirations 从此方法得到的word embeddings，然后再用DRNN等方法来进行classification？ 可以用于DTM上么？ 本文的code可以在这里下载,Nguyen et al. (2015)的方法与之类似，将word embeddings作为latent features，但是实现速度慢，对large corpus不可行，code可以在这里找到。 The original paper can be found here.","categories":[{"name":"papers","slug":"papers","permalink":"http://cuiyungao.github.io/categories/papers/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"},{"name":"Topic Modeling","slug":"Topic-Modeling","permalink":"http://cuiyungao.github.io/tags/Topic-Modeling/"}]},{"title":"Study Material for Deep Learning and NLP","slug":"material","date":"2016-06-28T12:21:47.000Z","updated":"2016-06-28T12:46:54.000Z","comments":true,"path":"2016/06/28/material/","link":"","permalink":"http://cuiyungao.github.io/2016/06/28/material/","excerpt":"The website summaries the material for understanding deep learning and NLP.","text":"The website summaries the material for understanding deep learning and NLP. How to Start Learning Deep LearningLink [Book] First Contact with TensorFlowLink The book introduces how to use TensorFlow to implement the basic functions of neural network and also parallelism.","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"},{"name":"material","slug":"material","permalink":"http://cuiyungao.github.io/tags/material/"}]},{"title":"LDA Understanding","slug":"lda","date":"2016-06-28T09:17:25.000Z","updated":"2016-07-20T06:24:45.000Z","comments":true,"path":"2016/06/28/lda/","link":"","permalink":"http://cuiyungao.github.io/2016/06/28/lda/","excerpt":"LDA introduction and summary.","text":"LDA introduction and summary. Topic Model Reading ListLink.详细解释：","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"},{"name":"LDA","slug":"LDA","permalink":"http://cuiyungao.github.io/tags/LDA/"}]},{"title":"Useful Source Code for NLP","slug":"code","date":"2016-06-28T09:17:25.000Z","updated":"2016-07-29T03:27:18.000Z","comments":true,"path":"2016/06/28/code/","link":"","permalink":"http://cuiyungao.github.io/2016/06/28/code/","excerpt":"This page contains useful code related to NLP.","text":"This page contains useful code related to NLP. TensorFlow v0.9 now available with improved mobile support.Link TensorFlow now supports mobile invocation. Tensorflow and theano CNN code for insurance QA(question Answer matching).Link. Theano和tensorflow的网络结构都是一致的: word embedings + CNN + max pooling + cosine similarity.目前再insuranceQA的test1数据集上，top-1准确率可以达到62%左右，跟论文上是一致的。这里只提供了CNN的代码，后面测试了LSTM和LSTM+CNN的方法，LSTM+CNN的方法比单纯使用CNN或LSTM效果还要更好一些，在test1上的准确率可以再提示5%-6%. LSTM language model with CNN over characters in TensorFlow.Link.Tensorflow implementation of Character-Aware Neural Language Models paper. Topic Augmented Neural Response Generation with a Joint Attention MechanismLink. Use attention model. Text input with relevant emoji sorted with deep learninglink. After inputing a sentence, the related emojis are shown in the top. I’ve tried the project, and it’s amazing. It uses API from Dango. The webpage recommends deep learning material. I need to read them in detail, which maybe helpful for review analysis. Information Extraction with Reinforcement LearningLink. It’s based on Torch and python. Visualization Toolbox for Long Short Term Memory networks (LSTMs)Link. Visual Analysis for State Changes in RNNs. More information about LSTMVis, an introduction video, and the link to the live demo can be found here. Attention Sum ReaderLink. This is a Theano/Blocks implementation of the Attention Sum Reader model as presented in “Text Comprehension with the Attention Sum Reader Network” available at here. Wider &amp; deep learning: better together with TensorFlowLink. Query的过程就是找出类似的词的过程，这个可以用phrase extraction？是否可以用在rating prediction上面。 Minimal Character-Level Language Model需看~！Link. Minimal character-level language model with a Vanilla Recurrent Neural Network, in Python/numpy. LSTM-ParserLink. Based on C++ language. Transition-based joint syntactic dependency parser and semantic role labeler using stack LSTM RNN architecture. Paper Greedy, joint syntactic-semantic parsing with stack LSTMs can be downloaded here. Pre-trained Word Embedding in KerasLink. Based on Keras library. In this tutorial, we will walk you through the process of solving a text classification problem using pre-trained word embeddings and a convolutional neural network. Building Machine Learning Estimator in TensorFlowLink. The purpose of this post is to help you better understand the underlying principles of estimators in TensorFlow Learn and point out some tips and hints if you ever want to build your own estimator that’s suitable for your particular application. TensorLayer: Deep learning and Reinforcement learning libraryLink. It was designed to provide a higher-level API to TensorFlow in order to speed-up experimentations. Neural Relation ExtractionLink. Neural relation extraction aims to extract relations from plain text with neural models, which has been the state-of-the-art methods for relation extraction. In this project, we provide our implementations of CNN [Zeng et al., 2014] and PCNN [Zeng et al.,2015] and their extended version with sentence-level attention scheme [Lin et al., 2016] Make a Chatting RobotLink. 自己动手做聊天机器人教程. Door to Machine LearningLink. 机器学习精简入门教程. Sequence Classification with LSTM RNN with KerasLink. Python based.实验结果证明加了CNN的效果更好，用dropout避免过拟合，但是实验结果不是那么理想，原因是深度只有3，当层数增加的时候可以实现比不用dropout更好的效果。 Neural Conversation ModelsLink. TensorFlow based.支持simple seq2seq models和attention based seq2seq models. 深度学习主机环境配置Link.","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"},{"name":"code","slug":"code","permalink":"http://cuiyungao.github.io/tags/code/"}]},{"title":"Deep Recursive Neural Networks for Compositionality in Language","slug":"drnn","date":"2016-06-28T07:04:35.000Z","updated":"2016-06-28T09:26:20.000Z","comments":true,"path":"2016/06/28/drnn/","link":"","permalink":"http://cuiyungao.github.io/2016/06/28/drnn/","excerpt":"Irsoy, Ozan, and Claire Cardie. “Deep recursive neural networks for compositionality in language.” Advances in Neural Information Processing Systems. 2014.","text":"Irsoy, Ozan, and Claire Cardie. “Deep recursive neural networks for compositionality in language.” Advances in Neural Information Processing Systems. 2014. Recursive Neural NetworkRecursive neural networks (RNNs) comprise a class of architecture that can operate on structured input. The same set of weights is recursively applied within a structural setting. Given a positional directed acyclic graph, it visits the nodes in topological order, and recursively applies transformations to generate further representations from previously computed representations of children. A recurrent neural network is simply a recursive neural network with a particular structure (Figure 1c). Problem: Even though RNNs are deep in structure, they lack the capacity for hierarchical representation that exists in conventional deep feed-forward networks and recurrent neural networks. Recurrent v.s. RecursiveRecurrent neural networks are deep in time, while recursive neural networks are deep in structure (due to the repeated application of recursive connections). Recently, the notions of depth in time - the result of recurrent connections, and depth in space - the result of stacking multiple layers on top of one another, are distinguished for recurrent neural network. Deep recurrent neural networks were proposed for composing these concepts. They are created by stacking multiple recurrent layers on top of each other. This allows the extra notion of depth to be incorporated into temporal processing. Inspired by deep recurrent neural networks, deep recursive neural networks are proposed in this paper. Deep Recursive Neural NetworksAn important benefit of depth is the hierarchy among hidden representations: every hidden layer conceptually lies in a different representation space and potentially is a more abstract representation of the input than the previous layer. The DRNN is constructed by stacking multiple layers of individual recursive nets: where $i$ means the multiple stacked layers, $W_L^{(i)}$, $W_R^{(i)}$, and $b^{(i)}$ are the weight matrices that connect the left and right children to the parent, and a bias vector, respectively. $V^{(i)}$ is the weight matrix that connects the $(i-1)$th hidden layer to the $i$th hidden layer. For the untying shown in Figure 1b, every node is represented in the same space above the first, regardless of their leafness. Figure 2 shows the weights that are untied or shared. For prediction, we connect the output layer to only the final hidden layer. If we connect the output layer to all hidden layers, multiple hidden layers can have synergistic effects on the output and make it more difficult to qualitatively analyze each layer. ResultsData: Stanford Sentiment Treebank (SST) link Result 1Comparing with multiplicative RNN and the more recent Paragraph Vectors, DRNNs outperform their shallow counterparts of the same size. Deep RNN outperforms the baselines, achieving state-of-the-art performance on the task. Reason: The authors attribute an important contribution of the improvement to dropouts. Result 2For searching nearest neighbor phrases, different layers capture different aspects. One-nor distance mearsure is used. Analysis: The first layer is dominated by one of the words that is composed. The seconde layer takes syntactic similarity more into account. The third layer captures the sentiment. Inspiration: Can we apply this into emoji detection?Paper can be download here.Code is written in C++, can be found here.Introduction webpage is here.","categories":[{"name":"papers","slug":"papers","permalink":"http://cuiyungao.github.io/categories/papers/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"}]},{"title":"How to read:character level deep learning","slug":"howtoread","date":"2016-06-27T08:26:48.000Z","updated":"2016-06-28T02:37:07.000Z","comments":true,"path":"2016/06/27/howtoread/","link":"","permalink":"http://cuiyungao.github.io/2016/06/27/howtoread/","excerpt":"A character level model for sentiment classification is demonstrated with TensorFlow.","text":"A character level model for sentiment classification is demonstrated with TensorFlow. Some NLP applications are impressive, such as Gmail’s auto-reply and FB’s deep-text. The models are built with framework Keras, with TensorFlow as back-end. KerasThe core data structure of Keras is a model, a way to organize layers. The main type of the model is the sequential model, a linear stack of layers. Character-Level ModelThe typical problem of sentiment analysis is, given a text $x_i$ (e.g., a movie review), we need to figure out whether the review is positive(1) or negative(0), denoted as $y_i$. A network $f(x_i)$ is created to predict the lable of the review. Typically, the text is split into a sequence of words, and then learn fixed length embedding of the sequence, which later is used for classification. In a recurrent model, each word is encoded as a vector (word embeddingsChristopher Olah) and also his explanation about LSTMs. The neural network tries to learn the specific sequences of letters from words separated by spaces or other punctuation points. The visualization of some internal processes of char-rnn models can be found in this paper. Building a Sentiment Model Dataset: labelled lebeledTrainData.tsv. Split the text into sentences. This bounds the maximum length of a sequence. Encode each sentence from characters to a fixed length encoding. Use a bi-directional LSTM to read sentence by sentence and create a complete document encoding. Full Model Architecture The model uses two bi-directional LSTM. It starts from reading characters and forming concepts of “words”, then uses a bi-directional LSTM to read “words” as a sequence and account for their position. Then, a second bi-directional LSTM is used for each sentence for the final document encoding. Code Advantage of Character-Level ModellingIt enables us to deal with common miss-spellings, different permutation of words (think run, runs, running). Texts that contain emojis, signaling chars, hashtags, and all the funky annotations that are being used in social media are very interesting directions. Take HomeSome things might improve the generalisation and reduce overfitting: Different hidden layer sizes. Smaller layers will reduce the ability of the model to overfit to the training set. Larger dropout rates. $l2/l1$ regularization. A deeper and/or wider architecture of cnn encoder. Different doc encoder, maybe include an attention model. link","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"},{"name":"character level","slug":"character-level","permalink":"http://cuiyungao.github.io/tags/character-level/"}]},{"title":"Courses for NLP and Deep Learning","slug":"nlpcourses","date":"2016-06-27T08:01:41.000Z","updated":"2016-06-27T08:06:55.000Z","comments":true,"path":"2016/06/27/nlpcourses/","link":"","permalink":"http://cuiyungao.github.io/2016/06/27/nlpcourses/","excerpt":"The website contains the courses related to NLP and deep learning.","text":"The website contains the courses related to NLP and deep learning. 1. Deep learning for text mining from scratchThis website contains links for optimization and statistics, and also very useful tutorials for NLP, such as UFLDL tutorial.","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"},{"name":"Courses","slug":"Courses","permalink":"http://cuiyungao.github.io/tags/Courses/"}]},{"title":"Dive into TensorFlow","slug":"diveTensorflow","date":"2016-06-27T03:37:45.000Z","updated":"2016-08-12T02:23:57.000Z","comments":true,"path":"2016/06/27/diveTensorflow/","link":"","permalink":"http://cuiyungao.github.io/2016/06/27/diveTensorflow/","excerpt":"Get started with TensorFlow.","text":"Get started with TensorFlow. TensorFlowTensorFlow is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. TensorFlow programs use a tensor data structure to represent all data - only tensors are passed between operations in the computation graph. USAGEA scalar, vector, or matrix is a ternsor. A tensor has a rank, a shape, and a static type. Tensor’s dimension can be described by rank, shape, and dimension number. 12345scalar = tf.constant(100)vector = tf.constant([1,2,3,4])matrix = tf.constant([[1,2],[3,4]])## get shapescalar.get_shape() Virables must be initialized by running an init Op after haveing launched the graph. 123456789101112var = tf.Variable(10)ten = tf.constant(10)new_var = tf.mul(var, ten)upate = tf.assign(var, new_var)init_op = tf.initialize_all_variables()with tf.Session() as sess: sess.run(init_op) print(sess.run(var)) for _ in range(5): sess.run(update) print(sess.run(var)) Variables are in-memory buffers containing tensors. During model training, variables can be used to hold and update parameters. 12weight = tf.Variable(tf.random_normal([4096,500], stddev=0.25))weight_update = tf.Variable(weight.initialized_value()) To get the results, we need to execute the graph with a run() call on the Session object. Feed temporarily replaces the output of an update with a tensor value. The most common “feed” operations is created by tf.placeholder(). 12345plh1 = tf.placeholder(tf.float32)plh2 = tf.placeholder(tf.float32)result = tf.mul(plh1, plh2)with tf.Session as sess: print(sess.run([result], feed_dict=&#123;plh1:[100.], plh2:[200.]&#125;)) Link How Good is Your Fit?Through the video provided by TensorFlow, here summaries the ways to prevent or reduce overfitting: Split up the data into 3 sets: the training set, test set, and cross validation set. Along with the parameters averaging, the model is not too dependent on any particular subset of the overall data set. For neural networks particularly, regularization is a common way. There are a few different types, such as L1 and L2, but each follows the same general principle - the model is penalized for having weights and biases that are too large. Another method is Max Norm constraints. It directly adds a size limit to the weights or biases. A completely different approach is Dropout. It randomly switches off ceratin neurons in the network, preventing the model from becoming too dependent on a set of neurons and its associated weights and biases. Building Blocks and Algorithms基于TensorFlow - Not just for deep learning。TensorFlow提供了很多支持数值计算和机器学习的building blocks，比如tf.contrib模块。 TF.Learn EstimatorsTF.Learn是一个high-level的模块，提供了很多机器学习的算法，比如k-means，random forests, SVM, Gaussian Mixture Model Clustering和Linear/logistic regression。 Statistical Distributionstf.contrib.distributions提供了很多的分布，比如Bernoulli, Beta, Chi2, Dirichlet, Gamma, Uniform等。 Layer Componentstf.contrib.layers里提供了很多生成layer操作和相关权重以及bias variables的函数，比如batch normalization, convolution layer, dropout layer等。当然还有one-hot encoding。tf.contrib.layers.optimizers提供了很多优化算法，比如Adagrad, SGC, Momentum等，经常用于数值分析上的优化问题，包括优化参数空间以找到一个较好的模型。tf.contrib.layers.reguarizers提供了许多的Regularizers，比如L1和L2的正则器，常用于通过惩罚模型中大量的特征，以减少overfitting的风险。许多算法需要计算gradients，在参数空间中寻找最佳参数前通常需要初始化模型参数，tf.contrib.layers.initilizers提供了类似Xavier的初始化方法，用于保证所有层gradients的scale大致保持一致。tf.contrib.layers.feature_column将连续的或者分类的特征用bucketing/binning, crossing/composition和embedding的方法来转化。tf.contrib.layers.embedding讲高维的分类特征转变成低维的dense的实数向量，通常被称为an embedding vector。这些低维的紧密的矩阵后面被连接起来，形成一个连续的特征，然后输入到模型中。 Loss Functions and Metricstf.contrib.losses提供了很多loss functions，比如sigmoid, softmax cross entropy, log-loss, hinge loss, sum of squares, sum of pairwise squares等。衡量标准在tf.contrib.metrics，比如precision, recall, accuracy, auc, MSE以及它们演变出来的一些版本。 Reference[1] Annotated notes and summaries of the TensorFlow white paper","categories":[{"name":"daily","slug":"daily","permalink":"http://cuiyungao.github.io/categories/daily/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://cuiyungao.github.io/tags/TensorFlow/"}]},{"title":"SPINN","slug":"SPINN","date":"2016-06-26T03:58:46.000Z","updated":"2016-06-27T02:59:15.000Z","comments":true,"path":"2016/06/26/SPINN/","link":"","permalink":"http://cuiyungao.github.io/2016/06/26/SPINN/","excerpt":"Hybrid tree-sequence neural networks with SPINN, published at Stanford","text":"Hybrid tree-sequence neural networks with SPINN, published at Stanford - Online Reading Summay - SPINNStack-augmented Parser-Interpreter Neural Network. This is a hybrid tree-sequence architecture, 融合了recursive 和recurrent neural networks，比当中的任何一个要好。 GOALoutput compact, sufficient representations of natural language. IDEACompute representations bottom-up, starting at the leaves and moving to nonterminals. This allows linguistic structure to guide computation. TECHShift-Reduce Parsing, a method for building parse structures from sequence inputs in linear time. It works by exploiting an auxiliary stack structure, which stores partially-parsed subtrees, and a buffer, which stores input tokens which have yet to be parsed. This can generate the constituency tree. For a sentence with $n$ tokens, we can produce its parse with a shift-reduce parser in exactly $2n-1$ transitions. Shift Phase:Pulls the next word embeddings from the buffer and pushes it onto the stack; Recude Phase:Combines top two elements of the stack $\\vec{c_1},\\vec{c_2}$ into a single element $\\vec{p}$ via the standard recursive neural network feedforward: $$\\vec{p} = \\sigma(W[\\vec{c_1}, \\vec{c_2}]).$$ Now we have a shift-reduce parser, deep-learning style. The feedforward speed is up to 25x improvement over the recursive neura network, but 2~5 times slower than a recurrent neural network. Recursive neural networks have often been dissed as too slow and “not batchable”, and this development proves both points wrong. Hybrid Tree-Sequence NetworksVisualization of the post-order tree traversal performed by a shift-reduce parser. Why not have a recurrent neural network follow along this path of arrows? Tracking Memory: At any given timestep $t$, a new tracking value $\\vec{m_t}$ is computed by $\\vec{m_t} = Track(\\vec{m_t-1}, \\vec{c_1}, \\vec{c_2}, \\vec{b_1})$. This tracking memory is then passed noto the recursive composition function, via $\\vec{p} = \\sigma(W[\\vec{c_1};\\vec{c_2};\\vec{m_t}])$. A recurrent neural network has just been interwovened into a recursive neural network. ResultA representation $f(x)$ for an input sentence $x$ is built by a new way. It shows a high-accuracy result on the Stanford Natural Language Inference dataset. Published paper: A fast unified model for parsing and sentence understanding.Code.","categories":[{"name":"papers","slug":"papers","permalink":"http://cuiyungao.github.io/categories/papers/"}],"tags":[{"name":"RNN","slug":"RNN","permalink":"http://cuiyungao.github.io/tags/RNN/"},{"name":"NLP","slug":"NLP","permalink":"http://cuiyungao.github.io/tags/NLP/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://cuiyungao.github.io/tags/Deep-Learning/"}]}]}