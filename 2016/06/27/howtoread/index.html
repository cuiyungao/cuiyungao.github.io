<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.5.0/styles/default.min.css">
<script src="http://code.jquery.com/jquery-2.2.4.min.js"  integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44="   crossorigin="anonymous"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.5.0/highlight.min.js"></script>
<script src="/js/SimpleCore.js" type="text/javascript"></script>
<!--[if lte IE 9]><meta http-equiv="refresh" content="0;url=/warn.html"><![endif]-->

<title>How to read:character level deep learning</title>
<meta name="author" content="Cuiyun Gao">

<meta name="keywords" content="undefined">

<meta name="description " content="null">

<link rel="icon" href="/images/favicon.png">

<link rel="stylesheet" href="/css/style.css">

  </head>
  <body>
    <aside id="sidebar">
  <nav id="tags">
    <a href="http://shuoit.net" id="avatar" style="background-image:url(/images/favicon.png)"></a>
    <ul id="tags__ul">
      <li id="pl__all" class="tags__li tags-btn active">所有文章</li>
      
      <li id="daily" class="tags__li tags-btn">daily</li>
      
      <li id="papers" class="tags__li tags-btn">papers</li>
      
    </ul>
    <div id="tags__bottom">
      <a href="http://github.com/tangkunyin" rel="nofollow" id="icon-github" target="_blank" class="tags-btn fontello"></a>
      <a href="http://shang.qq.com/wpa/qunwpa?idkey=6dc2741479cd031cc533fe5080081df44d0f9d95bc5083e8e1244c92f4aae218" rel="nofollow" id="icon-qq" target="_blank" class="tags-btn fontello"></a>
    </div>
  </nav> <!-- end #tags -->
  <div id="posts-list">
    <form action="#" id="search-form" target="_blank">
      <a href="/" id="mobile-avatar" style="background-image:url(/images/favicon.png)"></a>
      <input id="search-input" type="text" placeholder="戳一下不会怀孕" />
    </form>
    <nav id="pl__container">
      
            
                <a class="daily pl__all" href="/2016/06/29/UFLDL/" title="UFLDL Tutorial">
                  <span class="pl__circle"></span><span class="pl__title">UFLDL Tutorial</span><span class="pl__date">2016-06-29</span>
                </a>
            
                <a class="daily pl__all" href="/2016/06/29/cf/" title="Collaborative Filtering (CF)">
                  <span class="pl__circle"></span><span class="pl__title">Collaborative Filtering (CF)</span><span class="pl__date">2016-06-29</span>
                </a>
            
                <a class="daily pl__all" href="/2016/06/28/code/" title="Useful Source Code for NLP">
                  <span class="pl__circle"></span><span class="pl__title">Useful Source Code for NLP</span><span class="pl__date">2016-06-28</span>
                </a>
            
                <a class="daily pl__all" href="/2016/06/28/crawler/" title="Data Crawling">
                  <span class="pl__circle"></span><span class="pl__title">Data Crawling</span><span class="pl__date">2016-06-28</span>
                </a>
            
                <a class="daily pl__all" href="/2016/06/27/diveTensorflow/" title="Dive into TensorFlow">
                  <span class="pl__circle"></span><span class="pl__title">Dive into TensorFlow</span><span class="pl__date">2016-06-27</span>
                </a>
            
                <a class="daily pl__all" href="/2016/06/29/goodturing/" title="Good-Turing Estimation 大数据处理平滑算法">
                  <span class="pl__circle"></span><span class="pl__title">Good-Turing Estimation 大数据处理平滑算法</span><span class="pl__date">2016-06-29</span>
                </a>
            
                <a class="daily pl__all" href="/2016/06/27/howtoread/" title="How to read:character level deep learning">
                  <span class="pl__circle"></span><span class="pl__title">How to read:character level deep learning</span><span class="pl__date">2016-06-27</span>
                </a>
            
                <a class="daily pl__all" href="/2016/06/28/lda/" title="LDA Understanding">
                  <span class="pl__circle"></span><span class="pl__title">LDA Understanding</span><span class="pl__date">2016-06-28</span>
                </a>
            
                <a class="daily pl__all" href="/2016/06/28/material/" title="Study Material for Deep Learning and NLP">
                  <span class="pl__circle"></span><span class="pl__title">Study Material for Deep Learning and NLP</span><span class="pl__date">2016-06-28</span>
                </a>
            
                <a class="daily pl__all" href="/2016/07/04/memory-network/" title="Three Impactful ML Topics at ICML&#39;16">
                  <span class="pl__circle"></span><span class="pl__title">Three Impactful ML Topics at ICML&#39;16</span><span class="pl__date">2016-07-04</span>
                </a>
            
                <a class="daily pl__all" href="/2016/06/28/memorization/" title="Memorization and Exploration in Recurrent Neural Language Models">
                  <span class="pl__circle"></span><span class="pl__title">Memorization and Exploration in Recurrent Neural Language Models</span><span class="pl__date">2016-06-28</span>
                </a>
            
                <a class="daily pl__all" href="/2016/07/04/researcher/" title="Find Researchers Here">
                  <span class="pl__circle"></span><span class="pl__title">Find Researchers Here</span><span class="pl__date">2016-07-04</span>
                </a>
            
                <a class="daily pl__all" href="/2016/06/27/nlpcourses/" title="Courses for NLP and Deep Learning">
                  <span class="pl__circle"></span><span class="pl__title">Courses for NLP and Deep Learning</span><span class="pl__date">2016-06-27</span>
                </a>
            
      
            
                <a class="papers pl__all" href="/2016/06/28/acl16/" title="Generative Topic Embedding:a Continuous Representation of Documents">
                  <span class="pl__circle"></span><span class="pl__title">Generative Topic Embedding:a Continuous Representation of Documents</span><span class="pl__date">2016-06-28</span>
                </a>
            
                <a class="papers pl__all" href="/2016/06/26/SPINN/" title="SPINN">
                  <span class="pl__circle"></span><span class="pl__title">SPINN</span><span class="pl__date">2016-06-26</span>
                </a>
            
                <a class="papers pl__all" href="/2016/06/28/drnn/" title="Deep Recursive Neural Networks for Compositionality in Language">
                  <span class="pl__circle"></span><span class="pl__title">Deep Recursive Neural Networks for Compositionality in Language</span><span class="pl__date">2016-06-28</span>
                </a>
            
                <a class="papers pl__all" href="/2016/06/30/usermodeling/" title="User Modeling with Neural Network for Review Rating Prediction">
                  <span class="pl__circle"></span><span class="pl__title">User Modeling with Neural Network for Review Rating Prediction</span><span class="pl__date">2016-06-30</span>
                </a>
            
                <a class="papers pl__all" href="/2016/06/30/predictingamazon/" title="Predicting Amazon Ratings Using Neural Networks">
                  <span class="pl__circle"></span><span class="pl__title">Predicting Amazon Ratings Using Neural Networks</span><span class="pl__date">2016-06-30</span>
                </a>
            
      
    </nav>
  </div> <!-- end #posts-list -->
</aside> <!-- end #sidebar -->

    <div id="post">
      <div id="pjax">
        <article id="post__content">
  <h1 id="post__title" data-identifier="2016-06-27">How to read:character level deep learning</h1>
  <p>A character level model for sentiment classification is demonstrated with TensorFlow.</p>
<a id="more"></a>
<p>Some NLP applications are impressive, such as Gmail’s <a href="http://arxiv.org/abs/1606.04870" target="_blank" rel="external">auto-reply</a> and FB’s <a href="https://code.facebook.com/posts/181565595577955" target="_blank" rel="external">deep-text</a>. The models are built with framework <a href="http://keras.io/" target="_blank" rel="external">Keras</a>, with TensorFlow as back-end.</p>
<h3 id="Keras"><a href="#Keras" class="headerlink" title="Keras"></a><center><strong>Keras</strong></center></h3><p>The core data structure of Keras is a <code>model</code>, a way to organize layers. The main type of the model is the <code>sequential model</code>, a linear stack of layers.</p>
<h3 id="Character-Level-Model"><a href="#Character-Level-Model" class="headerlink" title="Character-Level Model"></a><center><strong>Character-Level Model</strong></center></h3><p>The typical problem of sentiment analysis is, given a text $x_i$ (<em>e.g.</em>, a movie review), we need to figure out whether the review is positive(1) or negative(0), denoted as $y_i$. A network $f(x_i)$ is created to predict the lable of the review. Typically, the text is split into <code>a sequence of words</code>, and then learn <code>fixed length embedding</code> of the sequence, which later is used for classification.</p>
<p>In a <code>recurrent model</code>, each word is encoded as <code>a vector</code> (<span style="color:red">word embeddings</span><a href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/" target="_blank" rel="external">Christopher Olah</a>) and also his explanation about <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">LSTMs</a>.</p>
<p>The neural network tries to learn the specific sequences of letters from words <code>separated by spaces</code> or <code>other punctuation points</code>. The visualization of some internal processes of <code>char-rnn models</code> can be found in this <a href="http://arxiv.org/abs/1506.02078" target="_blank" rel="external">paper</a>.</p>
<h3 id="Building-a-Sentiment-Model"><a href="#Building-a-Sentiment-Model" class="headerlink" title="Building a Sentiment Model"></a><center><strong>Building a Sentiment Model</strong></center></h3><ol>
<li><p>Dataset: <a href="https://www.kaggle.com/c/word2vec-nlp-tutorial/data" target="_blank" rel="external">labelled lebeledTrainData.tsv</a>.</p>
</li>
<li><p>Split the text into sentences. This <code>bounds the maximum length of a sequence</code>.</p>
</li>
<li><p>Encode each sentence from characters to a fixed length encoding.</p>
</li>
<li><p>Use a bi-directional LSTM to read sentence by sentence and create a complete document encoding.</p>
</li>
</ol>
<center><span style="color:white">Full Model Architecture</span></center><br><center><img src="/img/daily/fullmodel.jpg" style="width:500px;"></center>

<p>The model uses <span style="color:red">two bi-directional LSTM</span>. It starts from reading characters and forming concepts of “words”, then uses <code>a bi-directional LSTM</code> to read “words” as <code>a sequence</code> and account for their <code>position</code>. Then, a <code>second</code> bi-directional LSTM is used for each sentence for the final document encoding.</p>
<p><a href="https://github.com/offbit/char-models" target="_blank" rel="external">Code</a></p>
<h3 id="Advantage-of-Character-Level-Modelling"><a href="#Advantage-of-Character-Level-Modelling" class="headerlink" title="Advantage of Character-Level Modelling"></a><center><strong>Advantage of Character-Level Modelling</strong></center></h3><p>It enables us to deal with common miss-spellings, different permutation of words (think run, runs, running). Texts that contain <code>emojis</code>, <code>signaling chars</code>, <code>hashtags</code>, and all the <code>funky annotations</code> that are being used in social media are very interesting directions.</p>
<h3 id="Take-Home"><a href="#Take-Home" class="headerlink" title="Take Home"></a><center><strong>Take Home</strong></center></h3><p>Some things might improve the <code>generalisation</code> and <code>reduce overfitting</code>:</p>
<ol>
<li><p>Different hidden layer sizes. Smaller layers will reduce the ability of the model to overfit to the training set.</p>
</li>
<li><p>Larger dropout rates.</p>
</li>
<li><p>$l2/l1$ regularization.</p>
</li>
<li><p>A deeper and/or wider architecture of cnn encoder.</p>
</li>
<li><p>Different doc encoder, maybe include <code>an attention model</code>.</p>
</li>
</ol>
<p><a href="https://offbit.github.io/how-to-read/" target="_blank" rel="external">link</a></p>

</article>
<button id="js-fullscreen"><span id="icon-arrow" class="fontello"></span></button>

  <div id="post__action">
      <a id="icon-heart" class="fontello" href="javascript:donate();"></a>
  </div>
  <script src="/layer/layer.js" type="text/javascript"></script>
  <script src="/layer/extend/layer.ext.js" type="text/javascript"></script>


<!-- 多说评论框 start -->
<div class="ds-thread" data-thread-key="undefined" data-title="How to read:character level deep learning" data-url="http://shuoit.net/2016/06/27/howtoread/"></div>
<!-- 多说评论框 end -->
<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"shuoit"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0]
		 || document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
<!-- 多说公共JS代码 end -->

        <p id="copyright">由<a href="http://hexo.io" target="_blank">Hexo</a>创建&nbsp;&nbsp;|&nbsp;&nbsp;主题<a href="https://github.com/tangkunyin/hexo-theme-simple" target="_blank">Simple</a>&nbsp;&nbsp;|&nbsp;&nbsp;感谢<a href="https://coding.net/register?key=bcace447-fa41-4579-9783-86fdf0f723e2" target="_blank">Coding</a>提供强力驱动</p>
<!-- 百度统计 -->
<script>
  var _hmt = _hmt || [];
  (function() {
    var hm = document.createElement("script");
    hm.src = "//hm.baidu.com/hm.js?a01595b492af7e4a793230e3d49ae638";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
  })();
</script>

      </div>
      <div id="post__toc-trigger">
        <div id="post__toc">
          <span id="post__toc-title">目录</span>
          <ul id="post__toc-ul"></ul>
        </div>
      </div>
    </div>
  </body>
</html>
