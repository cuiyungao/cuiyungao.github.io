<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.5.0/styles/default.min.css">
<script src="http://code.jquery.com/jquery-2.2.4.min.js"  integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44="   crossorigin="anonymous"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.5.0/highlight.min.js"></script>
<script src="/js/SimpleCore.js" type="text/javascript"></script>
<!--[if lte IE 9]><meta http-equiv="refresh" content="0;url=/warn.html"><![endif]-->

<title>SPINN</title>
<meta name="author" content="Cuiyun Gao">

<meta name="keywords" content="undefined">

<meta name="description " content="null">

<link rel="icon" href="/images/favicon.png">

<link rel="stylesheet" href="/css/style.css">

  </head>
  <body>
    <aside id="sidebar">
  <nav id="tags">
    <a href="http://shuoit.net" id="avatar" style="background-image:url(/images/favicon.png)"></a>
    <ul id="tags__ul">
      <li id="pl__all" class="tags__li tags-btn active">所有文章</li>
      
      <li id="daily" class="tags__li tags-btn">daily</li>
      
      <li id="papers" class="tags__li tags-btn">papers</li>
      
    </ul>
    <div id="tags__bottom">
      <a href="http://github.com/tangkunyin" rel="nofollow" id="icon-github" target="_blank" class="tags-btn fontello"></a>
      <a href="http://shang.qq.com/wpa/qunwpa?idkey=6dc2741479cd031cc533fe5080081df44d0f9d95bc5083e8e1244c92f4aae218" rel="nofollow" id="icon-qq" target="_blank" class="tags-btn fontello"></a>
    </div>
  </nav> <!-- end #tags -->
  <div id="posts-list">
    <form action="#" id="search-form" target="_blank">
      <a href="/" id="mobile-avatar" style="background-image:url(/images/favicon.png)"></a>
      <input id="search-input" type="text" placeholder="戳一下不会怀孕" />
    </form>
    <nav id="pl__container">
      
            
                <a class="daily pl__all" href="/2016/06/29/UFLDL/" title="UFLDL Tutorial">
                  <span class="pl__circle"></span><span class="pl__title">UFLDL Tutorial</span><span class="pl__date">2016-06-29</span>
                </a>
            
                <a class="daily pl__all" href="/2016/06/29/cf/" title="Collaborative Filtering (CF)">
                  <span class="pl__circle"></span><span class="pl__title">Collaborative Filtering (CF)</span><span class="pl__date">2016-06-29</span>
                </a>
            
                <a class="daily pl__all" href="/2016/06/28/code/" title="Useful Source Code for NLP">
                  <span class="pl__circle"></span><span class="pl__title">Useful Source Code for NLP</span><span class="pl__date">2016-06-28</span>
                </a>
            
                <a class="daily pl__all" href="/2016/06/28/crawler/" title="Data Crawling">
                  <span class="pl__circle"></span><span class="pl__title">Data Crawling</span><span class="pl__date">2016-06-28</span>
                </a>
            
                <a class="daily pl__all" href="/2016/06/27/diveTensorflow/" title="Dive into TensorFlow">
                  <span class="pl__circle"></span><span class="pl__title">Dive into TensorFlow</span><span class="pl__date">2016-06-27</span>
                </a>
            
                <a class="daily pl__all" href="/2016/06/29/goodturing/" title="Good-Turing Estimation 大数据处理平滑算法">
                  <span class="pl__circle"></span><span class="pl__title">Good-Turing Estimation 大数据处理平滑算法</span><span class="pl__date">2016-06-29</span>
                </a>
            
                <a class="daily pl__all" href="/2016/06/27/howtoread/" title="How to read:character level deep learning">
                  <span class="pl__circle"></span><span class="pl__title">How to read:character level deep learning</span><span class="pl__date">2016-06-27</span>
                </a>
            
                <a class="daily pl__all" href="/2016/06/28/lda/" title="LDA Understanding">
                  <span class="pl__circle"></span><span class="pl__title">LDA Understanding</span><span class="pl__date">2016-06-28</span>
                </a>
            
                <a class="daily pl__all" href="/2016/06/28/material/" title="Study Material for Deep Learning and NLP">
                  <span class="pl__circle"></span><span class="pl__title">Study Material for Deep Learning and NLP</span><span class="pl__date">2016-06-28</span>
                </a>
            
                <a class="daily pl__all" href="/2016/07/04/memory-network/" title="Three Impactful ML Topics at ICML&#39;16">
                  <span class="pl__circle"></span><span class="pl__title">Three Impactful ML Topics at ICML&#39;16</span><span class="pl__date">2016-07-04</span>
                </a>
            
                <a class="daily pl__all" href="/2016/06/28/memorization/" title="Memorization and Exploration in Recurrent Neural Language Models">
                  <span class="pl__circle"></span><span class="pl__title">Memorization and Exploration in Recurrent Neural Language Models</span><span class="pl__date">2016-06-28</span>
                </a>
            
                <a class="daily pl__all" href="/2016/07/04/researcher/" title="Find Researchers Here">
                  <span class="pl__circle"></span><span class="pl__title">Find Researchers Here</span><span class="pl__date">2016-07-04</span>
                </a>
            
                <a class="daily pl__all" href="/2016/06/27/nlpcourses/" title="Courses for NLP and Deep Learning">
                  <span class="pl__circle"></span><span class="pl__title">Courses for NLP and Deep Learning</span><span class="pl__date">2016-06-27</span>
                </a>
            
      
            
                <a class="papers pl__all" href="/2016/06/28/acl16/" title="Generative Topic Embedding:a Continuous Representation of Documents">
                  <span class="pl__circle"></span><span class="pl__title">Generative Topic Embedding:a Continuous Representation of Documents</span><span class="pl__date">2016-06-28</span>
                </a>
            
                <a class="papers pl__all" href="/2016/06/26/SPINN/" title="SPINN">
                  <span class="pl__circle"></span><span class="pl__title">SPINN</span><span class="pl__date">2016-06-26</span>
                </a>
            
                <a class="papers pl__all" href="/2016/06/28/drnn/" title="Deep Recursive Neural Networks for Compositionality in Language">
                  <span class="pl__circle"></span><span class="pl__title">Deep Recursive Neural Networks for Compositionality in Language</span><span class="pl__date">2016-06-28</span>
                </a>
            
                <a class="papers pl__all" href="/2016/06/30/usermodeling/" title="User Modeling with Neural Network for Review Rating Prediction">
                  <span class="pl__circle"></span><span class="pl__title">User Modeling with Neural Network for Review Rating Prediction</span><span class="pl__date">2016-06-30</span>
                </a>
            
                <a class="papers pl__all" href="/2016/06/30/predictingamazon/" title="Predicting Amazon Ratings Using Neural Networks">
                  <span class="pl__circle"></span><span class="pl__title">Predicting Amazon Ratings Using Neural Networks</span><span class="pl__date">2016-06-30</span>
                </a>
            
      
    </nav>
  </div> <!-- end #posts-list -->
</aside> <!-- end #sidebar -->

    <div id="post">
      <div id="pjax">
        <article id="post__content">
  <h1 id="post__title" data-identifier="2016-06-26">SPINN</h1>
  <p>Hybrid tree-sequence neural networks with SPINN, published at Stanford</p>
<a id="more"></a>
<h2 id="Online-Reading-Summay"><a href="#Online-Reading-Summay" class="headerlink" title=" - Online Reading Summay - "></a><center> - Online Reading Summay - </center></h2><h3 id="SPINN"><a href="#SPINN" class="headerlink" title="SPINN"></a><center><strong>SPINN</strong></center></h3><p><a href="http://www.foldl.me/2016/spinn-hybrid-tree-sequence-models/" target="_blank" rel="external">Stack-augmented Parser-Interpreter Neural Network</a>. This is a hybrid tree-sequence architecture, 融合了recursive 和recurrent neural networks，比当中的任何一个要好。</p>
<h3 id="GOAL"><a href="#GOAL" class="headerlink" title="GOAL"></a><center><strong>GOAL</strong></center></h3><p>output compact, sufficient representations of natural language. </p>
<h3 id="IDEA"><a href="#IDEA" class="headerlink" title="IDEA"></a><center><strong>IDEA</strong></center></h3><p>Compute representations bottom-up, starting at the leaves and moving to nonterminals. This allows <span style="color:red">linguistic structure</span> to guide computation.</p>
<h3 id="TECH"><a href="#TECH" class="headerlink" title="TECH"></a><center><strong>TECH</strong></center></h3><p><em>Shift-Reduce Parsing</em>, a method for building parse structures from sequence inputs in linear time. It works by exploiting an auxiliary <span style="color:white">stack</span> structure, which stores partially-parsed subtrees, and a <span style="color:white">buffer</span>, which stores input tokens which have yet to be parsed. This can generate the constituency tree.</p>
<p>For a sentence with <code>$n$ tokens</code>, we can produce its parse with a <code>shift-reduce parser</code> in exactly $2n-1$ transitions.</p>
<h4 id="Shift-Phase"><a href="#Shift-Phase" class="headerlink" title="Shift Phase:"></a>Shift Phase:</h4><p>Pulls the <code>next word embeddings</code> from the buffer and pushes it onto the stack;</p>
<h4 id="Recude-Phase"><a href="#Recude-Phase" class="headerlink" title="Recude Phase:"></a>Recude Phase:</h4><p>Combines top two elements of the stack $\vec{c_1},\vec{c_2}$ into a single element $\vec{p}$ via the standard <code>recursive neural network feedforward</code>: </p>
<p>$$<br>\vec{p} = \sigma(W[\vec{c_1}, \vec{c_2}]).<br>$$</p>
<p>Now we have a <code>shift-reduce parser</code>, deep-learning style. The feedforward speed is <code>up to 25x improvement</code> over the recursive neura network, but <code>2~5 times slower</code> than a recurrent neural network.</p>
<p>Recursive neural networks have often been dissed as <strong>too slow</strong> and “not batchable”, and this development proves both points wrong.</p>
<h3 id="Hybrid-Tree-Sequence-Networks"><a href="#Hybrid-Tree-Sequence-Networks" class="headerlink" title="Hybrid Tree-Sequence Networks"></a><center><strong>Hybrid Tree-Sequence Networks</strong></center></h3><center><span style="color:white">Visualization of the post-order tree traversal performed by a shift-reduce parser.</span></center><br><center><img src="/img/papers/tree-shift-reduce-with-trace.gif" alt=""></center>

<p>Why not have a <code>recurrent</code> neural network follow along this path of arrows?</p>
<p><strong>Tracking Memory</strong>: At any given timestep $t$, a new tracking value $\vec{m_t}$ is computed by $\vec{m_t} = Track(\vec{m_t-1}, \vec{c_1}, \vec{c_2}, \vec{b_1})$.</p>
<p>This tracking memory is then passed noto the <code>recursive composition function</code>, via $\vec{p} = \sigma(W[\vec{c_1};\vec{c_2};\vec{m_t}])$.</p>
<p><span style="color:red">A recurrent neural network has just been interwovened into a recursive neural network.</span></p>
<h3 id="Result"><a href="#Result" class="headerlink" title="Result"></a><center>Result</center></h3><p>A representation $f(x)$ for an input sentence $x$ is built by a new way. It shows a high-accuracy result on the <a href="http://nlp.stanford.edu/projects/snli/" target="_blank" rel="external">Stanford Natural Language Inference dataset</a>.</p>
<p><a href="http://www.foldl.me/uploads/papers/acl2016-spinn.pdf" title="Published paper" target="_blank" rel="external">Published paper</a>: A fast unified model for parsing and sentence understanding.<br><a href="https://github.com/stanfordnlp/spinn" target="_blank" rel="external">Code</a>.</p>

</article>
<button id="js-fullscreen"><span id="icon-arrow" class="fontello"></span></button>

  <div id="post__action">
      <a id="icon-heart" class="fontello" href="javascript:donate();"></a>
  </div>
  <script src="/layer/layer.js" type="text/javascript"></script>
  <script src="/layer/extend/layer.ext.js" type="text/javascript"></script>


<!-- 多说评论框 start -->
<div class="ds-thread" data-thread-key="undefined" data-title="SPINN" data-url="http://shuoit.net/2016/06/26/SPINN/"></div>
<!-- 多说评论框 end -->
<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"shuoit"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0]
		 || document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
<!-- 多说公共JS代码 end -->

        <p id="copyright">由<a href="http://hexo.io" target="_blank">Hexo</a>创建&nbsp;&nbsp;|&nbsp;&nbsp;主题<a href="https://github.com/tangkunyin/hexo-theme-simple" target="_blank">Simple</a>&nbsp;&nbsp;|&nbsp;&nbsp;感谢<a href="https://coding.net/register?key=bcace447-fa41-4579-9783-86fdf0f723e2" target="_blank">Coding</a>提供强力驱动</p>
<!-- 百度统计 -->
<script>
  var _hmt = _hmt || [];
  (function() {
    var hm = document.createElement("script");
    hm.src = "//hm.baidu.com/hm.js?a01595b492af7e4a793230e3d49ae638";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
  })();
</script>

      </div>
      <div id="post__toc-trigger">
        <div id="post__toc">
          <span id="post__toc-title">目录</span>
          <ul id="post__toc-ul"></ul>
        </div>
      </div>
    </div>
  </body>
</html>
