<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.5.0/styles/default.min.css">
<script src="http://code.jquery.com/jquery-2.2.4.min.js"  integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44="   crossorigin="anonymous"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.5.0/highlight.min.js"></script>
<script src="/js/SimpleCore.js" type="text/javascript"></script>
<!--[if lte IE 9]><meta http-equiv="refresh" content="0;url=/warn.html"><![endif]-->

<title>Deep Recursive Neural Networks for Compositionality in Language</title>
<meta name="author" content="Cuiyun Gao">

<meta name="keywords" content="undefined">

<meta name="description " content="null">

<link rel="icon" href="/images/favicon.png">

<link rel="stylesheet" href="/css/style.css">

  </head>
  <body>
    <aside id="sidebar">
  <nav id="tags">
    <a href="http://shuoit.net" id="avatar" style="background-image:url(/images/favicon.png)"></a>
    <ul id="tags__ul">
      <li id="pl__all" class="tags__li tags-btn active">所有文章</li>
      
      <li id="daily" class="tags__li tags-btn">daily</li>
      
      <li id="papers" class="tags__li tags-btn">papers</li>
      
    </ul>
    <div id="tags__bottom">
      <a href="http://github.com/tangkunyin" rel="nofollow" id="icon-github" target="_blank" class="tags-btn fontello"></a>
      <a href="http://shang.qq.com/wpa/qunwpa?idkey=6dc2741479cd031cc533fe5080081df44d0f9d95bc5083e8e1244c92f4aae218" rel="nofollow" id="icon-qq" target="_blank" class="tags-btn fontello"></a>
    </div>
  </nav> <!-- end #tags -->
  <div id="posts-list">
    <form action="#" id="search-form" target="_blank">
      <a href="/" id="mobile-avatar" style="background-image:url(/images/favicon.png)"></a>
      <input id="search-input" type="text" placeholder="戳一下不会怀孕" />
    </form>
    <nav id="pl__container">
      
            
                <a class="daily pl__all" href="/2016/06/29/UFLDL/" title="UFLDL Tutorial">
                  <span class="pl__circle"></span><span class="pl__title">UFLDL Tutorial</span><span class="pl__date">2016-06-29</span>
                </a>
            
                <a class="daily pl__all" href="/2016/06/29/cf/" title="Collaborative Filtering (CF)">
                  <span class="pl__circle"></span><span class="pl__title">Collaborative Filtering (CF)</span><span class="pl__date">2016-06-29</span>
                </a>
            
                <a class="daily pl__all" href="/2016/06/28/code/" title="Useful Source Code for NLP">
                  <span class="pl__circle"></span><span class="pl__title">Useful Source Code for NLP</span><span class="pl__date">2016-06-28</span>
                </a>
            
                <a class="daily pl__all" href="/2016/06/28/crawler/" title="Data Crawling">
                  <span class="pl__circle"></span><span class="pl__title">Data Crawling</span><span class="pl__date">2016-06-28</span>
                </a>
            
                <a class="daily pl__all" href="/2016/06/27/diveTensorflow/" title="Dive into TensorFlow">
                  <span class="pl__circle"></span><span class="pl__title">Dive into TensorFlow</span><span class="pl__date">2016-06-27</span>
                </a>
            
                <a class="daily pl__all" href="/2016/06/29/goodturing/" title="Good-Turing Estimation 大数据处理平滑算法">
                  <span class="pl__circle"></span><span class="pl__title">Good-Turing Estimation 大数据处理平滑算法</span><span class="pl__date">2016-06-29</span>
                </a>
            
                <a class="daily pl__all" href="/2016/06/27/howtoread/" title="How to read:character level deep learning">
                  <span class="pl__circle"></span><span class="pl__title">How to read:character level deep learning</span><span class="pl__date">2016-06-27</span>
                </a>
            
                <a class="daily pl__all" href="/2016/06/28/lda/" title="LDA Understanding">
                  <span class="pl__circle"></span><span class="pl__title">LDA Understanding</span><span class="pl__date">2016-06-28</span>
                </a>
            
                <a class="daily pl__all" href="/2016/06/28/material/" title="Study Material for Deep Learning and NLP">
                  <span class="pl__circle"></span><span class="pl__title">Study Material for Deep Learning and NLP</span><span class="pl__date">2016-06-28</span>
                </a>
            
                <a class="daily pl__all" href="/2016/07/04/memory-network/" title="Three Impactful ML Topics at ICML&#39;16">
                  <span class="pl__circle"></span><span class="pl__title">Three Impactful ML Topics at ICML&#39;16</span><span class="pl__date">2016-07-04</span>
                </a>
            
                <a class="daily pl__all" href="/2016/06/28/memorization/" title="Memorization and Exploration in Recurrent Neural Language Models">
                  <span class="pl__circle"></span><span class="pl__title">Memorization and Exploration in Recurrent Neural Language Models</span><span class="pl__date">2016-06-28</span>
                </a>
            
                <a class="daily pl__all" href="/2016/07/04/researcher/" title="Find Researchers Here">
                  <span class="pl__circle"></span><span class="pl__title">Find Researchers Here</span><span class="pl__date">2016-07-04</span>
                </a>
            
                <a class="daily pl__all" href="/2016/06/27/nlpcourses/" title="Courses for NLP and Deep Learning">
                  <span class="pl__circle"></span><span class="pl__title">Courses for NLP and Deep Learning</span><span class="pl__date">2016-06-27</span>
                </a>
            
      
            
                <a class="papers pl__all" href="/2016/06/28/acl16/" title="Generative Topic Embedding:a Continuous Representation of Documents">
                  <span class="pl__circle"></span><span class="pl__title">Generative Topic Embedding:a Continuous Representation of Documents</span><span class="pl__date">2016-06-28</span>
                </a>
            
                <a class="papers pl__all" href="/2016/06/26/SPINN/" title="SPINN">
                  <span class="pl__circle"></span><span class="pl__title">SPINN</span><span class="pl__date">2016-06-26</span>
                </a>
            
                <a class="papers pl__all" href="/2016/06/28/drnn/" title="Deep Recursive Neural Networks for Compositionality in Language">
                  <span class="pl__circle"></span><span class="pl__title">Deep Recursive Neural Networks for Compositionality in Language</span><span class="pl__date">2016-06-28</span>
                </a>
            
                <a class="papers pl__all" href="/2016/06/30/usermodeling/" title="User Modeling with Neural Network for Review Rating Prediction">
                  <span class="pl__circle"></span><span class="pl__title">User Modeling with Neural Network for Review Rating Prediction</span><span class="pl__date">2016-06-30</span>
                </a>
            
                <a class="papers pl__all" href="/2016/06/30/predictingamazon/" title="Predicting Amazon Ratings Using Neural Networks">
                  <span class="pl__circle"></span><span class="pl__title">Predicting Amazon Ratings Using Neural Networks</span><span class="pl__date">2016-06-30</span>
                </a>
            
      
    </nav>
  </div> <!-- end #posts-list -->
</aside> <!-- end #sidebar -->

    <div id="post">
      <div id="pjax">
        <article id="post__content">
  <h1 id="post__title" data-identifier="2016-06-28">Deep Recursive Neural Networks for Compositionality in Language</h1>
  <p>Irsoy, Ozan, and Claire Cardie. “Deep recursive neural networks for compositionality in language.” Advances in Neural Information Processing Systems. 2014.</p>
<a id="more"></a>
<h3 id="Recursive-Neural-Network"><a href="#Recursive-Neural-Network" class="headerlink" title="Recursive Neural Network"></a><center><strong>Recursive Neural Network</strong></center></h3><p>Recursive neural networks (RNNs) comprise a class of architecture that can operate on <code>structured input</code>. <span style="color:white">The same set of weights is recursively applied within a structural setting.</span> Given a positional directed acyclic graph, it visits the nodes in topological order, and recursively applies transformations to generate further representations from previously computed representations of children.</p>
<p>A <code>recurrent neural network</code> is simply a <code>recursive</code> neural network with a particular structure (Figure 1c).<br><br></p>
<center><img src="/img/papers/drnn.png" width="60%"></center>

<p><span style="color:red">Problem:</span> Even though RNNs are deep in structure, they lack the capacity for hierarchical representation that exists in conventional deep feed-forward networks and recurrent neural networks.</p>
<h3 id="Recurrent-v-s-Recursive"><a href="#Recurrent-v-s-Recursive" class="headerlink" title="Recurrent v.s. Recursive"></a><center><strong>Recurrent v.s. Recursive</strong></center></h3><p>Recurrent neural networks are deep in time, while recursive neural networks are deep in structure (due to the repeated application of recursive connections). Recently, the notions of depth in time - the result of recurrent connections, and depth in space - the result of stacking multiple layers on top of one another, are distinguished for recurrent neural network. Deep recurrent neural networks were proposed for composing these concepts. They are created by stacking multiple recurrent layers on top of each other. This allows the extra notion of depth to be incorporated into temporal processing. </p>
<p>Inspired by <code>deep recurrent</code> neural networks, <code>deep recursive</code> neural networks are proposed in this paper.</p>
<h3 id="Deep-Recursive-Neural-Networks"><a href="#Deep-Recursive-Neural-Networks" class="headerlink" title="Deep Recursive Neural Networks"></a><center><strong>Deep Recursive Neural Networks</strong></center></h3><p>An important benefit of depth is the hierarchy among <code>hidden representations</code>: every hidden layer conceptually lies in a different representation space and potentially is a more abstract representation of the input than the previous layer.</p>
<p>The DRNN is constructed by <code>stacking</code> multiple layers of individual <code>recursive</code> nets:<br><br></p>
<center><img src="/img/papers/drnnformula.png" width="50%"></center>

<p>where $i$ means the multiple stacked layers, $W_L^{(i)}$, $W_R^{(i)}$, and $b^{(i)}$ are the weight matrices that connect the left and right children to the parent, and a bias vector, respectively. $V^{(i)}$ is the weight matrix that connects the $(i-1)$th hidden layer to the $i$th hidden layer. For the untying shown in Figure 1b, every node is represented in the same space above the first, regardless of their <code>leafness</code>. Figure 2 shows the weights that are untied or shared.<br><br></p>
<center><img src="/img/papers/drnnfig2.png" width="60%"></center>

<p>For <code>prediction</code>, we connect the output layer to <span style="color: red">only</span> the <code>final hidden layer</code>.<br><br></p>
<center><img src="/img/papers/drnnformula2.png" width="20%"></center>

<p>If we connect the output layer to all hidden layers, multiple hidden layers can have <code>synergistic effects</code> on the output and make it more difficult to qualitatively analyze each layer.</p>
<h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a><center><strong>Results</strong></center></h3><p><span style="color:white">Data:</span> Stanford Sentiment Treebank (SST) <a href="http://nlp.stanford.edu/pubs/SocherEtAl_EMNLP2013.pdf" target="_blank" rel="external">link</a></p>
<h4 id="Result-1"><a href="#Result-1" class="headerlink" title="Result 1"></a>Result 1</h4><p>Comparing with multiplicative RNN and the more recent Paragraph Vectors, DRNNs outperform their shallow counterparts of the same size. Deep RNN outperforms the baselines, achieving <code>state-of-the-art</code> performance on the task.<br><br></p>
<center><img src="/img/papers/drnnresult1.png" width="60%"></center>

<p><span style="color: red">Reason:</span> The authors attribute an important contribution of the improvement to <code>dropouts</code>.</p>
<h4 id="Result-2"><a href="#Result-2" class="headerlink" title="Result 2"></a>Result 2</h4><p>For searching <code>nearest neighbor phrases</code>, different layers capture different aspects. <code>One-nor</code> distance mearsure is used.<br><br></p>
<center><img src="/img/papers/drnnresult2.png" width="60%"></center>

<p><span style="color: red">Analysis:</span> The first layer is dominated by one of the words that is composed. The seconde layer takes syntactic similarity more into account. The third layer captures the sentiment.</p>
<p><span style="color: red">Inspiration:</span> Can we apply this into emoji detection?<br><br><br>Paper can be download <a href="https://www.cs.cornell.edu/~oirsoy/files/nips14drsv.pdf" target="_blank" rel="external">here</a>.<br>Code is written in C++, can be found <a href="https://github.com/oir/deep-recursive" target="_blank" rel="external">here</a>.<br>Introduction webpage is <a href="http://www.cs.cornell.edu/~oirsoy/drsv.htm" target="_blank" rel="external">here</a>.</p>

</article>
<button id="js-fullscreen"><span id="icon-arrow" class="fontello"></span></button>

  <div id="post__action">
      <a id="icon-heart" class="fontello" href="javascript:donate();"></a>
  </div>
  <script src="/layer/layer.js" type="text/javascript"></script>
  <script src="/layer/extend/layer.ext.js" type="text/javascript"></script>


<!-- 多说评论框 start -->
<div class="ds-thread" data-thread-key="undefined" data-title="Deep Recursive Neural Networks for Compositionality in Language" data-url="http://shuoit.net/2016/06/28/drnn/"></div>
<!-- 多说评论框 end -->
<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"shuoit"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0]
		 || document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
<!-- 多说公共JS代码 end -->

        <p id="copyright">由<a href="http://hexo.io" target="_blank">Hexo</a>创建&nbsp;&nbsp;|&nbsp;&nbsp;主题<a href="https://github.com/tangkunyin/hexo-theme-simple" target="_blank">Simple</a>&nbsp;&nbsp;|&nbsp;&nbsp;感谢<a href="https://coding.net/register?key=bcace447-fa41-4579-9783-86fdf0f723e2" target="_blank">Coding</a>提供强力驱动</p>
<!-- 百度统计 -->
<script>
  var _hmt = _hmt || [];
  (function() {
    var hm = document.createElement("script");
    hm.src = "//hm.baidu.com/hm.js?a01595b492af7e4a793230e3d49ae638";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
  })();
</script>

      </div>
      <div id="post__toc-trigger">
        <div id="post__toc">
          <span id="post__toc-title">目录</span>
          <ul id="post__toc-ul"></ul>
        </div>
      </div>
    </div>
  </body>
</html>
