<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>fastText与Word2Vec之间的比较 | Cuiyun Gao&#39;s Daily Digest | Work Hard, and Play Harder. 如果有一天：你不再寻找爱情，只是去爱；你不再渴望成功，只是去做；你不再追求成长，只是去修行；一切才真正开始~！</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <meta name="theme-color" content="#3F51B5">
  
  
  <meta name="keywords" content="NLP,Deep Learning,Word Embedding,fastText">
  <meta name="description" content="本篇文章总结了试验fastText跟Word2Vec在embedding上的performance，源于这篇文章。">
<meta property="og:type" content="article">
<meta property="og:title" content="fastText与Word2Vec之间的比较">
<meta property="og:url" content="http://cuiyungao.github.io/2016/08/09/fastvsword/index.html">
<meta property="og:site_name" content="Cuiyun Gao's Daily Digest">
<meta property="og:description" content="本篇文章总结了试验fastText跟Word2Vec在embedding上的performance，源于这篇文章。">
<meta property="og:updated_time" content="2016-08-09T02:08:42.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="fastText与Word2Vec之间的比较">
<meta name="twitter:description" content="本篇文章总结了试验fastText跟Word2Vec在embedding上的performance，源于这篇文章。">
  
    <link rel="alternative" href="/atom.xml" title="Cuiyun Gao&#39;s Daily Digest" type="application/atom+xml">
  
  <meta name="summary" content="&lt;p&gt;本篇文章总结了试验fastText跟Word2Vec在embedding上的performance，源于&lt;a href=&quot;http://nbviewer.jupyter.org/github/jayantj/gensim/blob/683720515165a332baed8a2a46b6711cefd2d739/docs/notebooks/Word2Vec_FastText_Comparison.ipynb&quot;&gt;这篇文章&lt;/a&gt;。&lt;/p&gt;">
  <link rel="shortcut icon" href="/img/favicon.png">
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="loading" class="active"></div>

  <nav id="menu" class="hide" >
   <div class="inner flex-row-vertical">
  <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
      <i class="icon icon-lg icon-close"></i>
  </a>
  <div class="brand-wrap">
    <div class="brand">
      <a href="/" class="avatar"><img src="/img/head.jpg"></a>
      <hgroup class="introduce">
        <h5 class="nickname">Cuiyun Gao</h5>
        <a href="mailto:undefined" title="cygao@cse.cuhk.edu.hk" class="mail">cygao@cse.cuhk.edu.hk</a>
      </hgroup>
    </div>
  </div>
  <ul class="nav flex-col">
    
        <li class="waves-block waves-effect">
          <a href="/"  >
            <i class="icon icon-lg icon-Home"></i>
            My Home
          </a>
        </li>
    
        <li class="waves-block waves-effect">
          <a href="/categories/papers"  >
            <i class="icon icon-lg icon-Reading"></i>
            Paper Reading
          </a>
        </li>
    
        <li class="waves-block waves-effect">
          <a href="/categories/daily"  >
            <i class="icon icon-lg icon-Daily"></i>
            Daily Digest
          </a>
        </li>
    
        <li class="waves-block waves-effect">
          <a href="/"  >
            <i class="icon icon-lg icon-Works"></i>
            My Works
          </a>
        </li>
    
        <li class="waves-block waves-effect">
          <a href="/"  >
            <i class="icon icon-lg icon-About"></i>
            About Me
          </a>
        </li>
    
  </ul>

  <footer class="footer">
  <p><a rel="license" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0;vertical-align:middle;" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAFAAAAAPCAMAAABEF7i9AAAAllBMVEUAAAD///+rsapERER3d3eIiIjMzMzu7u4iIiKUmZO6v7rKzsoODg4RERFVVVUNDQ0NDg0PEA8zMzNLTEtbXltmZmZydnF9gn2AgICPkI+ZmZmqqqq7u7vFxsXIzMgNDQwZGRkgICAhISEkJSMnKCcuMC4xMzE5Ozk7PTtBQkFCQkJDQ0Nna2eGhoaHh4ezuLLGysbd3d1wVGpAAAAA4UlEQVR42q2T1xqCMAyFk7QsBQeKA9x7j/d/OSm22CpX0nzcpA1/T05aAOuBVkMAScQFHLnEwoCo2f1TnQIGoVMewjZEjVFN4GH1Ue1Cn2jWqwfsOOj6wDwGvotsl/c8lv7KIq1eLOsT0HMFHMIE/RZyHnlphryT9zyV+8WH5e8yQw3wnQvgAFxPTKUVi555SHR/lOfLMgVTeDlSfN+TaoUsiTyeIm+bCkHvCA2FUKG48LDtYBZBknsYP/G8NTw0gaaHyuQf4H5pecrB/FYCT2sL9zAfy1Xyjou6L8X2W7YcLyBZCRtnq/zfAAAAAElFTkSuQmCC" /></a></p>
  <p>Cuiyun Gao&#39;s Daily Digest &copy; 2017</p>
  <p>Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme
  <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a></p>
  <a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-2x icon-rss-square"></i></a>
</footer>

</div>

  </nav>
  <main id="main">
    <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">fastText与Word2Vec之间的比较</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input " autocomplete="off" placeholder="Search">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
    </div>
</header>
<header class="content-header">
  <div class="container">
    <h1 class="author">fastText与Word2Vec之间的比较</h1>
    <h5 class="subtitle">
        
            <time datetime="2016-08-09T01:29:03.000Z" itemprop="datePublished" class="page-time">
  2016-08-09
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/daily/">daily</a></li></ul>

        
    </h5>
  </div>
</header>

    <div class="container body-wrap">
      <article id="post-fastvsword" 
  class="article article-type-post" itemprop="blogPost">
    <div class="post-meat flex-row">
        
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Word-Embedding/">Word Embedding</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/fastText/">fastText</a></li></ul>

    </div>
    <div class="post-body">
        <aside class="post-widget" id="post-widget">

            

            
            <nav class="post-toc-wrap" id="post-toc">
            <ol class="post-toc"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Dataset"><span class="post-toc-text">Dataset</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#text8-Corpus-Download"><span class="post-toc-text">text8 Corpus Download</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#brown-Corpus-Download"><span class="post-toc-text">brown Corpus Download</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Model-Training"><span class="post-toc-text">Model Training</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#fastText-Training"><span class="post-toc-text">fastText Training</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Word2Vec-Training"><span class="post-toc-text">Word2Vec Training</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Comparison"><span class="post-toc-text">Comparison</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Based-on-Brown-Corpus"><span class="post-toc-text">Based on Brown Corpus</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Based-on-text8-Corpus"><span class="post-toc-text">Based on text8 Corpus</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#实验中用到的Hyperparameters"><span class="post-toc-text">实验中用到的Hyperparameters</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Reference"><span class="post-toc-text">Reference</span></a></li></ol>
            </nav>
            
        </aside>

        <div class="post-main">

            <div class="post-content" id="post-content" itemprop="postContent">
            <p>本篇文章总结了试验fastText跟Word2Vec在embedding上的performance，源于<a href="http://nbviewer.jupyter.org/github/jayantj/gensim/blob/683720515165a332baed8a2a46b6711cefd2d739/docs/notebooks/Word2Vec_FastText_Comparison.ipynb" target="_blank" rel="external">这篇文章</a>。</p>
<a id="more"></a>
<h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a><center><strong>Dataset</strong></center></h3><p>训练embedding的数据集有两个，一个是<code>text8</code> corpus，一个是nltk自带的<code>brown</code> corpus。Groundtruth是<code>questtions-words</code>文本，可以从<a href="https://raw.githubusercontent.com/arfon/word2vec/master/questions-words.txt" target="_blank" rel="external">这里</a>下载。</p>
<h4 id="text8-Corpus-Download"><a href="#text8-Corpus-Download" class="headerlink" title="text8 Corpus Download"></a>text8 Corpus Download</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">wget http://mattmahoney.net/dc/text8.zip</div></pre></td></tr></table></figure>
<h4 id="brown-Corpus-Download"><a href="#brown-Corpus-Download" class="headerlink" title="brown Corpus Download"></a>brown Corpus Download</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">import nltk</div><div class="line"># 从当中选择brown corpus进行下载</div><div class="line">nltk.download()</div><div class="line"></div><div class="line"># Generate brown corpus text file</div><div class="line">with open(&apos;brown_corp.txt&apos;, &apos;w+&apos;) as f:</div><div class="line">    for word in nltk.corpus.brown.words():</div><div class="line">        f.write(&apos;&#123;word&#125; &apos;.format(word=word))</div></pre></td></tr></table></figure>
<h3 id="Model-Training"><a href="#Model-Training" class="headerlink" title="Model Training"></a><center><strong>Model Training</strong></center></h3><p>用fastText和Word2Vec分别对上述两个数据集进行训练，得到word embeddings。</p>
<h4 id="fastText-Training"><a href="#fastText-Training" class="headerlink" title="fastText Training"></a>fastText Training</h4><p>下载<a href="https://github.com/facebookresearch/fastText" target="_blank" rel="external">fastText</a>源码，对上述两个数据集进行训练。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">./fasttext skipgram -input brown_corp.txt -output brown_ft</div><div class="line">./fasttext skipgram -input text8 -output text8_ft</div></pre></td></tr></table></figure></p>
<h4 id="Word2Vec-Training"><a href="#Word2Vec-Training" class="headerlink" title="Word2Vec Training"></a>Word2Vec Training</h4><p>Word2Vec的训练基于gensim，采用<code>logging</code>来对过程进行输出。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">from nltk.corpus import brown</div><div class="line">from gensim.models import Word2Vec</div><div class="line">from gensim.models.word2vec import Text8Corpus</div><div class="line">import logging</div><div class="line"></div><div class="line">logging.basicConfig(format=&apos;%(asctime)s : %(levelname)s : %(message)s&apos;)</div><div class="line">logging.root.setLevel(level=logging.INFO)</div><div class="line"></div><div class="line">MODELS_DIR = &apos;models/&apos;</div><div class="line"></div><div class="line">brown_gs = Word2Vec(brown.sents())</div><div class="line">brown_gs.save_word2vec_format(MODELS_DIR + &apos;brown_gs.vec&apos;)</div><div class="line"></div><div class="line">text8_gs = Word2Vec(Text8Corpus(&apos;text8&apos;))</div><div class="line">text8_gs.save_word2vec_format(MODELS_DIR + &apos;text8_gs.vec&apos;)</div></pre></td></tr></table></figure></p>
<h3 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a><center><strong>Comparison</strong></center></h3><p>用<code>questions-words.txt</code>提供的数据作为Groundtruth，从<code>semantic</code>和<code>syntactic</code>两方面来对两种embedding的方法进行比较。</p>
<h4 id="Based-on-Brown-Corpus"><a href="#Based-on-Brown-Corpus" class="headerlink" title="Based on Brown Corpus"></a>Based on Brown Corpus</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">from gensim.models import Word2Vec</div><div class="line"></div><div class="line">def print_accuracy(model, questions_file):</div><div class="line">    print(&apos;Evaluating...\n&apos;)</div><div class="line">    acc = model.accuracy(questions_file)</div><div class="line">    for section in acc:</div><div class="line">        correct = len(section[&apos;correct&apos;])</div><div class="line">        total = len(section[&apos;correct&apos;]) + len(section[&apos;incorrect&apos;])</div><div class="line">        total = total if total else 1</div><div class="line">        accuracy = 100*float(correct)/total</div><div class="line">        print(&apos;&#123;:d&#125;/&#123;:d&#125;, &#123;:.2f&#125;%, Section: &#123;:s&#125;&apos;.format(correct, total, accuracy, section[&apos;section&apos;]))</div><div class="line">    sem_correct = sum((len(acc[i][&apos;correct&apos;]) for i in range(5)))</div><div class="line">    sem_total = sum((len(acc[i][&apos;correct&apos;]) + len(acc[i][&apos;incorrect&apos;])) for i in range(5))</div><div class="line">    print(&apos;\nSemantic: &#123;:d&#125;/&#123;:d&#125;, Accuracy: &#123;:.2f&#125;%&apos;.format(sem_correct, sem_total, 100*float(sem_correct)/sem_total))</div><div class="line"></div><div class="line">    syn_correct = sum((len(acc[i][&apos;correct&apos;]) for i in range(5, len(acc)-1)))</div><div class="line">    syn_total = sum((len(acc[i][&apos;correct&apos;]) + len(acc[i][&apos;incorrect&apos;])) for i in range(5,len(acc)-1))</div><div class="line">    print(&apos;Syntactic: &#123;:d&#125;/&#123;:d&#125;, Accuracy: &#123;:.2f&#125;%\n&apos;.format(syn_correct, syn_total, 100*float(syn_correct)/syn_total))</div><div class="line"></div><div class="line">MODELS_DIR = &apos;models/&apos;</div><div class="line"></div><div class="line">word_analogies_file = &apos;questions-words.txt&apos;</div><div class="line">print(&apos;\nLoading FastText embeddings&apos;)</div><div class="line">ft_model = Word2Vec.load_word2vec_format(MODELS_DIR + &apos;brown_ft.vec&apos;)</div><div class="line">print(&apos;Accuracy for FastText:&apos;)</div><div class="line">print_accuracy(ft_model, word_analogies_file)</div><div class="line"></div><div class="line">print(&apos;\nLoading Gensim embeddings&apos;)</div><div class="line">gs_model = Word2Vec.load_word2vec_format(MODELS_DIR + &apos;brown_gs.vec&apos;)</div><div class="line">print(&apos;Accuracy for word2vec:&apos;)</div><div class="line">print_accuracy(gs_model, word_analogies_file)</div></pre></td></tr></table></figure>
<p>结果如下：</p>
<blockquote><br>Loading FastText embeddings<br>Accuracy for FastText:<br>Evaluating…<br><br>0/1, 0.00%, Section: capital-common-countries<br>0/1, 0.00%, Section: capital-world<br>0/1, 0.00%, Section: currency<br>0/1, 0.00%, Section: city-in-state<br>36/182, 19.78%, Section: family<br>498/702, 70.94%, Section: gram1-adjective-to-adverb<br>110/132, 83.33%, Section: gram2-opposite<br>675/1056, 63.92%, Section: gram3-comparative<br>140/210, 66.67%, Section: gram4-superlative<br>426/650, 65.54%, Section: gram5-present-participle<br>0/1, 0.00%, Section: gram6-nationality-adjective<br>153/1260, 12.14%, Section: gram7-past-tense<br>318/552, 57.61%, Section: gram8-plural<br>245/342, 71.64%, Section: gram9-plural-verbs<br>2601/5086, 51.14%, Section: total<br><br>Semantic: 36/182, Accuracy: <mark>19.78%</mark><br>Syntactic: 2565/4904, Accuracy: <mark>52.30%</mark><br><br><br>Loading Gensim embeddings<br>Accuracy for word2vec:<br>Evaluating…<br><br>0/1, 0.00%, Section: capital-common-countries<br>0/1, 0.00%, Section: capital-world<br>0/1, 0.00%, Section: currency<br>0/1, 0.00%, Section: city-in-state<br>54/182, 29.67%, Section: family<br>8/702, 1.14%, Section: gram1-adjective-to-adverb<br>0/132, 0.00%, Section: gram2-opposite<br>72/1056, 6.82%, Section: gram3-comparative<br>0/210, 0.00%, Section: gram4-superlative<br>14/650, 2.15%, Section: gram5-present-participle<br>0/1, 0.00%, Section: gram6-nationality-adjective<br>28/1260, 2.22%, Section: gram7-past-tense<br>4/552, 0.72%, Section: gram8-plural<br>8/342, 2.34%, Section: gram9-plural-verbs<br>188/5086, 3.70%, Section: total<br><br>Semantic: 54/182, Accuracy: <mark>29.67%</mark><br>Syntactic: 134/4904, Accuracy: <mark>2.73%</mark><br></blockquote>

<p>从运行结果可以看到，fastText的semantic accuracy比Word2Vec要稍微差一点儿，但是Syntactic accuracy的效果明显优于Word2Vec。这是因为<a href="https://arxiv.org/abs/1607.04606" target="_blank" rel="external">1</a>中提到，fastText中word embeddings是由他们的n-gram embeddings来表示，所以形态上相似的词的embeddings也会比较类似。比如：<br>$$<br>embedding(amazing)-embedding(amazingly) = embedding(calm)-embedding(calmly).<br>$$</p>
<h4 id="Based-on-text8-Corpus"><a href="#Based-on-text8-Corpus" class="headerlink" title="Based on text8 Corpus"></a>Based on text8 Corpus</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">print(&apos;Loading FastText embeddings&apos;)</div><div class="line">ft_model = Word2Vec.load_word2vec_format(MODELS_DIR + &apos;text8_ft.vec&apos;)</div><div class="line">print(&apos;Accuracy for FastText:&apos;)</div><div class="line">print_accuracy(ft_model, word_analogies_file)</div><div class="line"></div><div class="line">print(&apos;Loading Gensim embeddings&apos;)</div><div class="line">gs_model = Word2Vec.load_word2vec_format(MODELS_DIR + &apos;text8_gs.vec&apos;)</div><div class="line">print(&apos;Accuracy for word2vec:&apos;)</div><div class="line">print_accuracy(gs_model, word_analogies_file)</div></pre></td></tr></table></figure>
<p>结果如下：</p>
<blockquote><br>Loading FastText embeddings<br>Accuracy for FastText:<br>Evaluating…<br><br>322/506, 63.64%, Section: capital-common-countries<br>609/1452, 41.94%, Section: capital-world<br>36/268, 13.43%, Section: currency<br>286/1520, 18.82%, Section: city-in-state<br>134/306, 43.79%, Section: family<br>556/756, 73.54%, Section: gram1-adjective-to-adverb<br>186/306, 60.78%, Section: gram2-opposite<br>838/1260, 66.51%, Section: gram3-comparative<br>270/506, 53.36%, Section: gram4-superlative<br>556/992, 56.05%, Section: gram5-present-participle<br>1293/1371, 94.31%, Section: gram6-nationality-adjective<br>490/1332, 36.79%, Section: gram7-past-tense<br>888/992, 89.52%, Section: gram8-plural<br>365/650, 56.15%, Section: gram9-plural-verbs<br>6829/12217, 55.90%, Section: total<br><br>Semantic: 1387/4052, Accuracy: <mark>34.23%</mark><br>Syntactic: 5442/8165, Accuracy: <mark>66.65%</mark><br><br>Loading Gensim embeddings<br>Accuracy for word2vec:<br>Evaluating…<br><br>153/506, 30.24%, Section: capital-common-countries<br>248/1452, 17.08%, Section: capital-world<br>27/268, 10.07%, Section: currency<br>172/1571, 10.95%, Section: city-in-state<br>218/306, 71.24%, Section: family<br>88/756, 11.64%, Section: gram1-adjective-to-adverb<br>45/306, 14.71%, Section: gram2-opposite<br>716/1260, 56.83%, Section: gram3-comparative<br>179/506, 35.38%, Section: gram4-superlative<br>325/992, 32.76%, Section: gram5-present-participle<br>702/1371, 51.20%, Section: gram6-nationality-adjective<br>343/1332, 25.75%, Section: gram7-past-tense<br>401/992, 40.42%, Section: gram8-plural<br>219/650, 33.69%, Section: gram9-plural-verbs<br>3836/12268, 31.27%, Section: total<br><br>Semantic: 818/4103, Accuracy: <mark>19.94%</mark><br>Syntactic: 3018/8165, Accuracy: <mark>36.96%</mark><br></blockquote>

<p>实验结果可以看出，用在较大的数据集上，fastText的优势表现得更加明显，当然word2vec的Syntactic accuracy提高得也比较明显。所以总的来看，<mark>fastText比word2vec在word embedding上更好，特别是对于syntactic information。</mark></p>
<h4 id="实验中用到的Hyperparameters"><a href="#实验中用到的Hyperparameters" class="headerlink" title="实验中用到的Hyperparameters"></a>实验中用到的Hyperparameters</h4><p>Gensim word2vec和fastText用了相似的参数，dim_size = 100, window_size = 5, num_epochs = 5。但是它们的模型完全不同，尽管有很多相似性。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a><center><strong>Reference</strong></center></h3><p>[1]<a href="https://arxiv.org/abs/1607.04606" target="_blank" rel="external">Enriching Word Vectors with Subword Information</a><br>[2]<a href="https://arxiv.org/pdf/1301.3781v3.pdf" target="_blank" rel="external">Efficient Estimation of Word Representations in Vector Space</a></p>

            <blockquote>
                <p>
                Permalink：
                <a href="http://cuiyungao.github.io/2016/08/09/fastvsword/" target="_blank" rel="external">http://cuiyungao.github.io/2016/08/09/fastvsword/</a>
                </p>
                <footer><cite><a href="http://cuiyungao.github.io">@Cuiyun Gao's Daily Digest</a></cite></footer>
            </blockquote>
            </div>
            
<nav class="post-nav">
  
    <div class="waves-block waves-effect prev fl">
      <a href="/2016/08/11/svmbaidu/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">SVM模型小结</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next fr">
      <a href="/2016/08/04/adclick/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">用户在线广告点击行为的预测</h4>
      </a>
    </div>
  
</nav>


            
            
<div class="duoshuo">
	<!-- 多说评论框 start -->
	<div class="ds-thread" data-thread-key="fastvsword" data-title="fastText与Word2Vec之间的比较" data-url="http://cuiyungao.github.io/2016/08/09/fastvsword/index.html"></div>
	<!-- 多说评论框 end -->
	<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
	<script type="text/javascript">
	var duoshuoQuery = {short_name:"cuiyungao"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		 || document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
	<!-- 多说公共JS代码 end -->
</div>





        </div>
    </div>
</article>
    </div>
  </main>
<div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>


<script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>

<script src="/js/main.js"></script>



<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<script type="text/template" id="search-tpl">
<li class="item">
    <a href="/{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</script>

<script src="/js/search.js"></script>



<! -- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->








</body>
</html>
