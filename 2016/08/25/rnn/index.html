<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Understanding RNN | Cuiyun Gao&#39;s Daily Digest | Work Hard, and Play Harder. 如果有一天：你不再寻找爱情，只是去爱；你不再渴望成功，只是去做；你不再追求成长，只是去修行；一切才真正开始~！</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <meta name="theme-color" content="#3F51B5">
  
  
  <meta name="keywords" content="RNN,Deep Learning">
  <meta name="description" content="这篇文章主要总结RNN，主要出处。">
<meta property="og:type" content="article">
<meta property="og:title" content="Understanding RNN">
<meta property="og:url" content="http://cuiyungao.github.io/2016/08/25/rnn/index.html">
<meta property="og:site_name" content="Cuiyun Gao's Daily Digest">
<meta property="og:description" content="这篇文章主要总结RNN，主要出处。">
<meta property="og:image" content="http://cuiyungao.github.io/img/daily/rnn1.png">
<meta property="og:updated_time" content="2016-08-26T07:00:36.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Understanding RNN">
<meta name="twitter:description" content="这篇文章主要总结RNN，主要出处。">
<meta name="twitter:image" content="http://cuiyungao.github.io/img/daily/rnn1.png">
  
    <link rel="alternative" href="/atom.xml" title="Cuiyun Gao&#39;s Daily Digest" type="application/atom+xml">
  
  <meta name="summary" content="&lt;p&gt;这篇文章主要总结RNN，主要&lt;a href=&quot;http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/&quot;&gt;出处&lt;/a&gt;。&lt;/p&gt;">
  <link rel="shortcut icon" href="/img/favicon.png">
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="loading" class="active"></div>

  <nav id="menu" class="hide" >
   <div class="inner flex-row-vertical">
  <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
      <i class="icon icon-lg icon-close"></i>
  </a>
  <div class="brand-wrap">
    <div class="brand">
      <a href="/" class="avatar"><img src="/img/head.jpg"></a>
      <hgroup class="introduce">
        <h5 class="nickname">Cuiyun Gao</h5>
        <a href="mailto:undefined" title="cygao@cse.cuhk.edu.hk" class="mail">cygao@cse.cuhk.edu.hk</a>
      </hgroup>
    </div>
  </div>
  <ul class="nav flex-col">
    
        <li class="waves-block waves-effect">
          <a href="/"  >
            <i class="icon icon-lg icon-My Home"></i>
            My Home
          </a>
        </li>
    
        <li class="waves-block waves-effect">
          <a href="/"  >
            <i class="icon icon-lg icon-Paper Reading"></i>
            Paper Reading
          </a>
        </li>
    
        <li class="waves-block waves-effect">
          <a href="/"  >
            <i class="icon icon-lg icon-Daily Digest"></i>
            Daily Digest
          </a>
        </li>
    
        <li class="waves-block waves-effect">
          <a href="/"  >
            <i class="icon icon-lg icon-My Works"></i>
            My Works
          </a>
        </li>
    
        <li class="waves-block waves-effect">
          <a href="/"  >
            <i class="icon icon-lg icon-About Me"></i>
            About Me
          </a>
        </li>
    
  </ul>

  <footer class="footer">
  <p><a rel="license" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0;vertical-align:middle;" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAFAAAAAPCAMAAABEF7i9AAAAllBMVEUAAAD///+rsapERER3d3eIiIjMzMzu7u4iIiKUmZO6v7rKzsoODg4RERFVVVUNDQ0NDg0PEA8zMzNLTEtbXltmZmZydnF9gn2AgICPkI+ZmZmqqqq7u7vFxsXIzMgNDQwZGRkgICAhISEkJSMnKCcuMC4xMzE5Ozk7PTtBQkFCQkJDQ0Nna2eGhoaHh4ezuLLGysbd3d1wVGpAAAAA4UlEQVR42q2T1xqCMAyFk7QsBQeKA9x7j/d/OSm22CpX0nzcpA1/T05aAOuBVkMAScQFHLnEwoCo2f1TnQIGoVMewjZEjVFN4GH1Ue1Cn2jWqwfsOOj6wDwGvotsl/c8lv7KIq1eLOsT0HMFHMIE/RZyHnlphryT9zyV+8WH5e8yQw3wnQvgAFxPTKUVi555SHR/lOfLMgVTeDlSfN+TaoUsiTyeIm+bCkHvCA2FUKG48LDtYBZBknsYP/G8NTw0gaaHyuQf4H5pecrB/FYCT2sL9zAfy1Xyjou6L8X2W7YcLyBZCRtnq/zfAAAAAElFTkSuQmCC" /></a></p>
  <p>Cuiyun Gao&#39;s Daily Digest &copy; 2016</p>
  <p>Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme
  <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a></p>
  <a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-2x icon-rss-square"></i></a>
</footer>

</div>

  </nav>
  <main id="main">
    <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">Understanding RNN</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input " autocomplete="off" placeholder="Search">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
    </div>
</header>
<header class="content-header">
  <div class="container">
    <h1 class="author">Understanding RNN</h1>
    <h5 class="subtitle">
        
            <time datetime="2016-08-25T02:49:14.000Z" itemprop="datePublished" class="page-time">
  2016-08-25
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/daily/">daily</a></li></ul>

        
    </h5>
  </div>
</header>

    <div class="container body-wrap">
      <article id="post-rnn" 
  class="article article-type-post" itemprop="blogPost">
    <div class="post-meat flex-row">
        
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RNN/">RNN</a></li></ul>

    </div>
    <div class="post-body">
        <aside class="post-widget" id="post-widget">

            

            
            <nav class="post-toc-wrap" id="post-toc">
            <ol class="post-toc"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Reference"><span class="post-toc-text">Reference</span></a></li></ol>
            </nav>
            
        </aside>

        <div class="post-main">

            <div class="post-content" id="post-content" itemprop="postContent">
            <p>这篇文章主要总结RNN，主要<a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" target="_blank" rel="external">出处</a>。</p>
<a id="more"></a>
<p>RNN在NLP的应用主要是两方面，一方面是可以判断两个句子的相似度；另外一方面则是可以生成新的文本。RNN的主要想法是<mark>使用sequential information</mark>。在传统的神经网络中，输入被认为是相互独立的，而RNN可以学习历史的信息，对sequence中的每一个element都进行相同的任务。另外一个想法是<mark>RNN有历史计算的<code>memory</code></mark>。这个<code>memory</code>就是网络的hidden state，所以<mark>hidden state是RNN的关键</mark>。较大的hidden layer size可以学到更复杂的patterns，但是会增加计算力。需要注意一下几点：</p>
<ol>
<li>Hidden state就是网络的memory，它获取历史信息，但是不能获取很长步骤之前的信息；</li>
<li>RNN在所有步骤之间共享参数，而传统的神经网络是每一层学习不同的参数。这能大大地减少需要学习的参数；</li>
<li>每一步不一定要有输出，这是由任务决定的；同理，每一步也不一定要有输入。</li>
</ol>
<p>因为参数在所有步骤之间共享，每一步的gradient不仅仅依赖于当前步的计算，还有之前很多步的计算，即<code>Backpropagation Through Time</code> (BPTT)。理论上，RNN绝对能处理<code>long-term dependencies</code>的情况，但是在实际应用中，RNN并不能有效地学习历史信息 (因为存在vanishing/exploding gradient problems)。LSTM是用来解决这种问题。所有的recurrent neural networks都有神经网络的重复module链的形式。在传统的RNN当中，重复的module有着非常简单的结构，比如单个的tanh层。LSTM结构也呈链状，但是在每一个单独的网络中，它有四层，而不是简单的一层。如图所示。LSTM的关键是cell state，即在图表中的上层，LSTM能够移除或者增加cell state。门(gate)的作用是选择性地让信息通过。有很多完善LSTM的方法，比如增加<code>peephole connections</code>，使用coupled forget gate和input gate来决定要forget和add的信息，以及GRU (Gated Recurrent Unit,将cell state和hidden state结合到<code>update gate</code>中)。具体的过程可以参见[1]。</p>
<center><img src="/img/daily/rnn1.png" width="100%"></center>

<p>RNN的延伸还有BRNN (Bidirectional RNN)和Deep (Bidriectioanl) RNN。前者的主要想法是时间$t$的输出不仅依赖于当前element，还有future element有关系；它的输出建立在双向的hidden state的基础之上。后者是每一个time step有很多层，这种情况通常有更高的学习能力，但通常也需要大量的训练数据。详细解释可以参考[2]。对于backpropagation，可以参考[3,4]。</p>
<p>Vanilla Neural Network的主要限制是API受限，只接受固定大小的输入和固定大小的输出，以及固定数量的计算步骤。引用原文[5]的话，<mark>If training vanilla neural nets is optimization over functions, training recurrent nets is optimization over programs.</mark></p>
<p>总结了RNN的衍生算法，比如DRAW, GAN, Pixel RNN等，可以看[6]。详细的GRU与LSTM之间的区别，可以见[8]。</p>
<p>关于初始化过程，RNN当中一般不能单纯地初始化为0，这样会导致对称计算。我们必须<mark>随机地初始化</mark>，适当地初始化会对训练结果又影响。最好的初始化依赖于<code>activation function</code>，并且<code>recommend</code>的方法是将权重随机地初始化在$[-\frac{1}{\sqrt{n}}, \frac{1}{\sqrt{n}}]$这个范围内，其中$n$是前面层的输入链接数。为了降低计算的复杂度，可以用<code>hierarchical softmax</code>或者增加<code>projection layers</code>来避免大的矩阵相乘。</p>
<p>关于<mark>vanishing/exploding gradients</mark>，主要考虑到求导部分，比如$\frac{\partial E_3}{\partial W} = \sum_{k=0}^3\frac{\partial E_3}{\partial y_3}\frac{\partial \hat{y}_3}{\partial s_3}(\prod_{j=k+1}^3\frac{\partial s_j}{\partial s_{j-1})\frac{\partial s_k}{\partial W}$。可以看到向量对向量进行求导，结果是一个矩阵，即<code>Jacobian matrix</code>。当中间连乘的值趋近于0或者较大的时候，就会出现<code>vanishing gradients</code>或者<code>exploding gradients</code>的情况。对于exploding的情况，可以简单地通过定义一个threshold来解决，而且比较容易发现，当gradients为NaN时，程序会崩溃，所以vanishing的情况更值得关注。有很多解决vanishing的方法，比如将activation function换成<code>ReLU</code>，而不是<code>tanh/sigmoid</code>，更常用的解决方法是用LSTM或者GRU (LSTM的简化版本)，这两种方法都是用来解决vanishing gradients和long-range dependencies的学习问题。具体分析可见[7]。关于parameter update，可以参见[9]，后面关于使用这些gradients的summary非常有用。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a><center><strong>Reference</strong></center></h3><p>[1] <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">Understanding LSTM Networks</a><br>[2] <a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" target="_blank" rel="external">Recurrent Neural Networks Tutorial</a><br>[3] <a href="http://colah.github.io/posts/2015-08-Backprop/" target="_blank" rel="external">Calculus on Computational Graphs: Backpropagation</a><br>[4] <a href="http://cs231n.github.io/optimization-2/" target="_blank" rel="external">CS231n Convolutional Neural Networks for Visual Recognition</a><br>[5] <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="external">The Unreasonable Effectiveness of Recurrent Neural Networks</a><br>[6] <a href="https://github.com/tensorflow/magenta/tree/master/magenta/reviews" target="_blank" rel="external">Reviews of research papers</a><br>[7] <a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/" target="_blank" rel="external">Backpropagation through time and vanishing gradients</a><br>[8] <a href="http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/" target="_blank" rel="external">Implementing a GRU/LSTM RNN with python and theano</a><br>[9] <a href="http://cs231n.github.io/neural-networks-3/#update" target="_blank" rel="external">Parameter updates</a></p>

            <blockquote>
                <p>
                Permalink：
                <a href="http://cuiyungao.github.io/2016/08/25/rnn/" target="_blank" rel="external">http://cuiyungao.github.io/2016/08/25/rnn/</a>
                </p>
                <footer><cite><a href="http://cuiyungao.github.io">@Cuiyun Gao's Daily Digest</a></cite></footer>
            </blockquote>
            </div>
            
<nav class="post-nav">
  
    <div class="waves-block waves-effect prev fl">
      <a href="/2016/09/01/machine/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">Machine Comprehension Using Match-LSTM and Answer Pointer</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next fr">
      <a href="/2016/08/24/fm/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">MF, FM, 与NMF</h4>
      </a>
    </div>
  
</nav>


            
            
<div class="duoshuo">
	<!-- 多说评论框 start -->
	<div class="ds-thread" data-thread-key="rnn" data-title="Understanding RNN" data-url="http://cuiyungao.github.io/2016/08/25/rnn/index.html"></div>
	<!-- 多说评论框 end -->
	<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
	<script type="text/javascript">
	var duoshuoQuery = {short_name:"cuiyungao"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		 || document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
	<!-- 多说公共JS代码 end -->
</div>





        </div>
    </div>
</article>
    </div>
  </main>
<div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>


<script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>

<script src="/js/main.js"></script>



<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<script type="text/template" id="search-tpl">
<li class="item">
    <a href="/{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</script>

<script src="/js/search.js"></script>



<! -- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->








</body>
</html>
