<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Tweet2Vec:Learning Tweet Embeddings Using Character-Level CNN-LSTM Encoder-Decoder | Cuiyun Gao&#39;s Daily Digest | Work Hard, and Play Harder.</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <meta name="theme-color" content="#3F51B5">
  
  
  <meta name="keywords" content="Deep Learning,NLP">
  <meta name="description" content="SIGIR’16的文章，来自MIT Media Lab。Vosoughi, Soroush, Prashanth Vijayaraghavan, and Deb Roy. “Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM Encoder-Decoder.”">
<meta property="og:type" content="article">
<meta property="og:title" content="Tweet2Vec:Learning Tweet Embeddings Using Character-Level CNN-LSTM Encoder-Decoder">
<meta property="og:url" content="http://cuiyungao.github.io/2016/08/03/tweet2vec/index.html">
<meta property="og:site_name" content="Cuiyun Gao's Daily Digest">
<meta property="og:description" content="SIGIR’16的文章，来自MIT Media Lab。Vosoughi, Soroush, Prashanth Vijayaraghavan, and Deb Roy. “Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM Encoder-Decoder.”">
<meta property="og:image" content="http://cuiyungao.github.io/img/papers/tweet2vec.png">
<meta property="og:updated_time" content="2016-08-03T14:08:02.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Tweet2Vec:Learning Tweet Embeddings Using Character-Level CNN-LSTM Encoder-Decoder">
<meta name="twitter:description" content="SIGIR’16的文章，来自MIT Media Lab。Vosoughi, Soroush, Prashanth Vijayaraghavan, and Deb Roy. “Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM Encoder-Decoder.”">
<meta name="twitter:image" content="http://cuiyungao.github.io/img/papers/tweet2vec.png">
  
    <link rel="alternative" href="/atom.xml" title="Cuiyun Gao&#39;s Daily Digest" type="application/atom+xml">
  
  <meta name="summary" content="&lt;p&gt;SIGIR’16的文章，来自MIT Media Lab。Vosoughi, Soroush, Prashanth Vijayaraghavan, and Deb Roy. “Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM Encoder-Decoder.”&lt;/p&gt;">
  <link rel="shortcut icon" href="/img/favicon.png">
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="loading" class="active"></div>

  <nav id="menu" class="hide" >
   <div class="inner flex-row-vertical">
  <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
      <i class="icon icon-lg icon-close"></i>
  </a>
  <div class="brand-wrap">
    <div class="brand">
      <a href="/" class="avatar"><img src="/img/head.jpg"></a>
      <hgroup class="introduce">
        <h5 class="nickname">Cuiyun Gao</h5>
        <a href="mailto:undefined" title="cygao@cse.cuhk.edu.hk" class="mail">cygao@cse.cuhk.edu.hk</a>
      </hgroup>
    </div>
  </div>
  <ul class="nav flex-col">
    
        <li class="waves-block waves-effect">
          <a href="/"  >
            <i class="icon icon-lg icon-My Home"></i>
            My Home
          </a>
        </li>
    
        <li class="waves-block waves-effect">
          <a href="/"  >
            <i class="icon icon-lg icon-Paper Reading"></i>
            Paper Reading
          </a>
        </li>
    
        <li class="waves-block waves-effect">
          <a href="/"  >
            <i class="icon icon-lg icon-Daily Digest"></i>
            Daily Digest
          </a>
        </li>
    
        <li class="waves-block waves-effect">
          <a href="/"  >
            <i class="icon icon-lg icon-My Works"></i>
            My Works
          </a>
        </li>
    
        <li class="waves-block waves-effect">
          <a href="/"  >
            <i class="icon icon-lg icon-About Me"></i>
            About Me
          </a>
        </li>
    
  </ul>

  <footer class="footer">
  <p><a rel="license" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0;vertical-align:middle;" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAFAAAAAPCAMAAABEF7i9AAAAllBMVEUAAAD///+rsapERER3d3eIiIjMzMzu7u4iIiKUmZO6v7rKzsoODg4RERFVVVUNDQ0NDg0PEA8zMzNLTEtbXltmZmZydnF9gn2AgICPkI+ZmZmqqqq7u7vFxsXIzMgNDQwZGRkgICAhISEkJSMnKCcuMC4xMzE5Ozk7PTtBQkFCQkJDQ0Nna2eGhoaHh4ezuLLGysbd3d1wVGpAAAAA4UlEQVR42q2T1xqCMAyFk7QsBQeKA9x7j/d/OSm22CpX0nzcpA1/T05aAOuBVkMAScQFHLnEwoCo2f1TnQIGoVMewjZEjVFN4GH1Ue1Cn2jWqwfsOOj6wDwGvotsl/c8lv7KIq1eLOsT0HMFHMIE/RZyHnlphryT9zyV+8WH5e8yQw3wnQvgAFxPTKUVi555SHR/lOfLMgVTeDlSfN+TaoUsiTyeIm+bCkHvCA2FUKG48LDtYBZBknsYP/G8NTw0gaaHyuQf4H5pecrB/FYCT2sL9zAfy1Xyjou6L8X2W7YcLyBZCRtnq/zfAAAAAElFTkSuQmCC" /></a></p>
  <p>Cuiyun Gao&#39;s Daily Digest &copy; 2016</p>
  <p>Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme
  <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a></p>
  <a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-2x icon-rss-square"></i></a>
</footer>

</div>

  </nav>
  <main id="main">
    <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">Tweet2Vec:Learning Tweet Embeddings Using Character-Level CNN-LSTM Encoder-Decoder</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input " autocomplete="off" placeholder="Search">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
    </div>
</header>
<header class="content-header">
  <div class="container">
    <h1 class="author">Tweet2Vec:Learning Tweet Embeddings Using Character-Level CNN-LSTM Encoder-Decoder</h1>
    <h5 class="subtitle">
        
            <time datetime="2016-08-03T12:41:08.000Z" itemprop="datePublished" class="page-time">
  2016-08-03
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/papers/">papers</a></li></ul>

        
    </h5>
  </div>
</header>

    <div class="container body-wrap">
      <article id="post-tweet2vec" 
  class="article article-type-post" itemprop="blogPost">
    <div class="post-meat flex-row">
        
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li></ul>

    </div>
    <div class="post-body">
        <aside class="post-widget" id="post-widget">

            

            
            <nav class="post-toc-wrap" id="post-toc">
            <ol class="post-toc"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Major-Idea"><span class="post-toc-text">Major Idea</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Techniques"><span class="post-toc-text">Techniques</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Character-level-CNN-Tweet-Model"><span class="post-toc-text">Character-level CNN Tweet Model</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Long-Short-Term-Memory-LSTM"><span class="post-toc-text">Long-Short Term Memory (LSTM)</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Combined-Model"><span class="post-toc-text">Combined Model</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Experiments"><span class="post-toc-text">Experiments</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Summary"><span class="post-toc-text">Summary</span></a></li></ol>
            </nav>
            
        </aside>

        <div class="post-main">

            <div class="post-content" id="post-content" itemprop="postContent">
            <p>SIGIR’16的文章，来自MIT Media Lab。Vosoughi, Soroush, Prashanth Vijayaraghavan, and Deb Roy. “Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM Encoder-Decoder.”</p>
<a id="more"></a>
<h3 id="Major-Idea"><a href="#Major-Idea" class="headerlink" title="Major Idea"></a><center><strong>Major Idea</strong></center></h3><p>作者利用tweet用户的hashtag作为groundtruth，基于Character-level对每一条的embeddings进行学习，对hashtag进行推测，并与已有的word2vec跟Glove进行对比，发现Tweet2Vec的准确率最高。</p>
<h3 id="Techniques"><a href="#Techniques" class="headerlink" title="Techniques"></a><center><strong>Techniques</strong></center></h3><p>基于Character-level，以及作者曾经提出Bi-directional gated recurrent unit (Bi-GRU)’14来学习tweet的表示形式。</p>
<h4 id="Character-level-CNN-Tweet-Model"><a href="#Character-level-CNN-Tweet-Model" class="headerlink" title="Character-level CNN Tweet Model"></a>Character-level CNN Tweet Model</h4><p>作者采用temporal convolutional和temporal max-pooling operations，即计算一维的输入和输出之间的convolution和pooling函数。给定一个离散的输入函数$f(x)\in [1,l]\mapsto \mathbb{R}$，一个离散的kernel函数$k(x)\in [1,m]\mapsto \mathbb{R}$，stride $s$，$k(x)$和$f(x)$之间的卷积$g(y)\in [1,(l-m+1)/s]\mathbb{R}$, 对$f(x)$的pooling操作$h(y)\in [1, (l-m+1)/s]\mapsto \mathbb{R}$：<br>$$<br>g(y) = \sum_{x=1}^m k(x)\cdot f(y\cdot s - x + c) \<br>h(y) = max_{x=1}^m f(y\cdot s - x + c)<br>$$<br>其中，$c=m-s+1$是一个补偿常量。tweet的character set包含英语字母，数字，特殊字符和不明确的字符，总共统计了70个字符。每个字符被encode成一个one-hot vector $x_i\in {0,1}^{70}$，单个tweet的最大长度是150，所以每个tweet被表示成了一个binary matrix $x_{1…150}\in {0,1}^{150\times70}$。表示成matix的tweet输入到一个包含4层一维卷基层的deep model。每个卷基层操作采用filter $w\in \mathbb{R}^l$来获取n-gram的character feature。一般来说，对于一个tweet $s$，在层$h$的一个特征$c_i$由下面式子生成：<br>$$<br>c_i^{(h)} (s) = g(w^{(h)}\cdot \hat{c}_i^{(h-1)} + b^{(h)})<br>$$<br>其中，$\hat{c}_i^{(0)}=x_{i…i+l-1}$, $b^{(h)}\in \mathbb{R}$是h层的bias，$g$是一个rectified linear unit。整体框架如下图所示：</p>
<center><img src="/img/papers/tweet2vec.png"></center>

<h4 id="Long-Short-Term-Memory-LSTM"><a href="#Long-Short-Term-Memory-LSTM" class="headerlink" title="Long-Short Term Memory (LSTM)"></a>Long-Short Term Memory (LSTM)</h4><p>给定一个输入序列$X=(x_1, x_2, …, x_N)$，LSTM计算hidden vector sequence $h=(h_1, h_2, …, h_N)$和output vector sequence $Y=(y_1, y_2, …, y_N)$。每一个时间步骤，一个模块的输出是由一组gates (前一个hidden state $h_{t1}$组成的函数)和当前时间步骤的输入控制的。forget gate $f_t$，input gate $i_t$和output gate $o_t$。这些gates集中决定了当前memory cess $c_t$的过渡和当前的hidden state $h_t$。LSTM的过渡函数定义如下：<br>$$<br>i_t = \sigma(W_i\cdot [h_{t-1}, x_t]+b_i) \<br>f_t = \sigma(W_f\cdot [h_{t-1}, x_t]+b_f) \<br>l_t = tanh(W_l\cdot [h_{t-1}, x_t]+b_l) \<br>o_t = \sigma(W_o\cdot [h_{t-1}, x_t]+b_o) \<br>c_t = f_t\odot c_{t-1} + i_t\odot l_t \<br>h_t = o_t\odot tanh(c_t)<br>$$<br>其中，$\odot$表示component-wise multiplicaiton。$f_t$控制过去的memory cell要舍弃的信息，而$i_t$控制新的信息储存在current memory cell的程度，$o_t$是基于memory cell $c_t$的输出。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">LSTM是学习`long-term denpendencies`，所以在卷基层之后用LSTM学习获取特征的序列中存在的依赖。</div></pre></td></tr></table></figure></p>
<p>在seq-to-seq中，LSTM定义了output上的分布，之后用softmax来序列预测tokens。<br>$$<br>P(Y|X)=\prod_{t\in [1,N]} \frac{exp(g(h_{t-1}, y_t))}{\sum_{y^{\prime} exp(g(h_{t1}, y_t^{\prime}))}}<br>$$<br>其中，g是activation function。</p>
<h4 id="Combined-Model"><a href="#Combined-Model" class="headerlink" title="Combined Model"></a>Combined Model</h4><p>Figure 1呈现了整个encoder-decoder过程。输入是由matrix呈现的tweet，字符由one-hot vector表示。<br><mark>Encoder部分</mark>，在Character-level CNN的较高层卷积之后不经过pooling，直接作为LSTM的输入，encoder过程可以表示为：<br>$$<br>H^{conv} = CharCNN(T) \<br>h_t = LSTM(g_t, h_{t-1}))<br>$$<br>其中，g=H^{conv}是一个特征矩阵，每一行代表LSTM的一个time step，$h_t$是$t$时刻的hidden representation。LSTM作用于$H^{conv}$的每一层，生成下一个序列的embedding。最终输出结果$enc_N$用来表示整条tweet。</p>
<p><mark>Decoder部分</mark>，用两层的LSTM作用于encoded representation。每一时间步骤中，字符的预测是：<br>$$<br>P(C_t|\cdot) = softmax(T_t, h_{t-1})<br>$$<br>其中，$C_t$指的是时间$t$的字符，$T_t$表示时刻$t$的one-hot vector。最终的结果是一个decoded tweet matrix $T^{dec}$，与实际的tweet进行比较，并学习模型的参数。</p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>实验用于两个分类任务： tweet semantic relatedness和tweet sentiment classification。3 million tweets。感觉数据量也挺小。准确率在0.6~0.7左右。</p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p><mark>Character level的deep learning的优势:</mark>占用内存少，不需要存储所有word的表示，只需要存储已有的character；不依赖于语言，只需要字符；而且不需要NLP的预处理，比如word segmentation。<br><mark>劣势是：</mark>运行速度慢，在文章的实验中，word2vec的速度是tweet2vec的6~7倍，原因是从word变成character输入意味着GRU的输入量变大。<br>Character level可以用在NLP的很多方面，比如named entity recognition, POS tagging, text classification, and language modeling。</p>
<blockquote><br>文章highlight了tweet2vec的优势，对word segmentation error, spelling mistakes, special characters, interpret emojis, in-vocab tokens非常有效。<br></blockquote>

<p><a href="http://soroush.mit.edu/publications/tweet2vec_vvr.pdf" target="_blank" rel="external">Published paper</a>.</p>

            <blockquote>
                <p>
                Permalink：
                <a href="http://cuiyungao.github.io/2016/08/03/tweet2vec/" target="_blank" rel="external">http://cuiyungao.github.io/2016/08/03/tweet2vec/</a>
                </p>
                <footer><cite><a href="http://cuiyungao.github.io">@Cuiyun Gao's Daily Digest</a></cite></footer>
            </blockquote>
            </div>
            
<nav class="post-nav">
  

  
    <div class="waves-block waves-effect next fr">
      <a href="/2016/07/04/memory-network/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">Impactful ML Topics at ICML&#39;16</h4>
      </a>
    </div>
  
</nav>


            
            
<div class="duoshuo">
	<!-- 多说评论框 start -->
	<div class="ds-thread" data-thread-key="tweet2vec" data-title="Tweet2Vec:Learning Tweet Embeddings Using Character-Level CNN-LSTM Encoder-Decoder" data-url="http://cuiyungao.github.io/2016/08/03/tweet2vec/index.html"></div>
	<!-- 多说评论框 end -->
	<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
	<script type="text/javascript">
	var duoshuoQuery = {short_name:"cuiyungao"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		 || document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
	<!-- 多说公共JS代码 end -->
</div>





        </div>
    </div>
</article>
    </div>
  </main>
<div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>


<script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>

<script src="/js/main.js"></script>



<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<script type="text/template" id="search-tpl">
<li class="item">
    <a href="/{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</script>

<script src="/js/search.js"></script>



<! -- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->








</body>
</html>
