<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Cuiyun Gao&#39;s Daily Digest</title>
  <subtitle>Work Hard, and Play Harder. 如果有一天：你不再寻找爱情，只是去爱；你不再渴望成功，只是去做；你不再追求成长，只是去修行；一切才真正开始~！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://cuiyungao.github.io/"/>
  <updated>2016-09-02T07:01:50.000Z</updated>
  <id>http://cuiyungao.github.io/</id>
  
  <author>
    <name>Cuiyun Gao</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Leveraging Sentence-Level Information with Encoder LSTM for Semantic Slot Filling</title>
    <link href="http://cuiyungao.github.io/2016/09/02/leveraging/"/>
    <id>http://cuiyungao.github.io/2016/09/02/leveraging/</id>
    <published>2016-09-02T05:44:53.000Z</published>
    <updated>2016-09-02T07:01:50.000Z</updated>
    
    <content type="html"><![CDATA[<p>这是在Arxiv 23, Aug, 2016的文章，作者Gakuto Kurata (IBM research),和Bing Xiang, Bowen Zhou, Mo Yu (IBM Watson)。<br><a id="more"></a><br>文章将slot filling结合sentence-level的信息，提出了<code>encoder-labeler LSTM</code>，即分为encoder和labeler两部分。<code>Encoder</code>基于LSTM将输入的句子编码成固定长度的向量，这个向量被用来作为<code>labeler</code> hidden layer的初始状态。这样，就可以预测label sequence了。</p>
<h3 id="LSTM-for-Slot-Filling"><a href="#LSTM-for-Slot-Filling" class="headerlink" title="LSTM for Slot Filling"></a><center><strong>LSTM for Slot Filling</strong></center></h3><p>用于slot filling的传统LSTM方法，即<code>labeler LSTM(W)</code>，将每个word$x_t$表示成$V$维的one-hot向量，这个向量转换成embedding的形式$Ex_t$，其中$E\in \mathbb{R}^{d_e\times V}$为word embedding matrix。<code>Context Window</code>用来取$k$个前面和后面的词$Ex_{t-k}^{t+k}\in \mathbb{R}^{d_e(2k+1)}$作为LSTM的输入，而不是单单输入一个词。紧接着softmax预测输出label，BPTT用来更新迭代parameters。如下图(a)。</p>
<p>这是比较传统的方法，<mark>它不能考虑label之间的依赖关系</mark>。为了弥补这个问题，文章提出了<code>labeler LSTM(W+L)</code>,即前一个time step输出的label作为当前time step的hidden state，并与当前词相结合。在training的过程中，<mark>label用one-hot-vector的形式表示</mark>，用<code>left-to-right beam search</code>做evaluation。如下图(b)。</p>
<p><code>Encoder-labeler LSTM(W)</code>和<code>encoder-labeler LSTM(W+L)</code>如图(d)和(e)所示。训练部分跟传统的<code>labeler LSTM</code>类似，但是labeler LSTM当中的error通过BPTT的方法更新encoder LSTM。跟以往的方法不同，输入序列被使用了两次，但是embedding matrix都是相同的。</p>
<p>其它考虑sentence-level信息的方法还有bi-directional LSTM以及attention-based methods。前者只是在一个特定的词的周围建模，没有考虑整个句子的信息，文章中的方法用<code>encoder-labeler LSTM</code>来编码整个句子的信息，并基于编码的信息来预测槽位；而后者更适用于输入跟输出长度不对等的情况，而且对于slot filling，输入的词跟输出的词有着很强的关联性，文章用<code>context window</code>的方法来对应attention。</p>
<center><img src="/img/papers/leveraging1.png" width="100%"></center>

<h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a><center><strong>Experiment</strong></center></h3><p>实验分为两部分，一部分是基于已有的数据集ATIS corpus (training data是4,978个句子，evaluation是893个句子)，第二部分是融合了MIT restaurant corpus和MIT movie corpus。Evaluation采用F1-score，用<code>ADAM</code>来控制learning rate，设置<code>dropout</code>为0.5。实验中<mark>句子的结尾没有加终止符</mark>，因为输入跟输出的sequence长度是一样的。结果如下表所示，encoder-labeler LSTM(W+L)的效果不如encoder-labeler LSTM(W)的原因是中间的label预测错误导致。</p>
<center><img src="/img/papers/leveraging2.png" width="100%"></center>

<p>与其它方法的如下图，更说明了方法的有效性。</p>
<center><img src="/img/papers/leveraging3.png" width="100%"></center>

<p><mark>结合slot filling的training过程跟intent分类的结果可能会更完善已有的方法。</mark></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a><center><strong>Reference</strong></center></h3><p>[1] <a href="https://arxiv.org/pdf/1601.01530v4.pdf" target="_blank" rel="external">Leveraging Sentence-Level Information with Encoder LSTM for Semantic Slot Filling</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是在Arxiv 23, Aug, 2016的文章，作者Gakuto Kurata (IBM research),和Bing Xiang, Bowen Zhou, Mo Yu (IBM Watson)。&lt;br&gt;
    
    </summary>
    
      <category term="papers" scheme="http://cuiyungao.github.io/categories/papers/"/>
    
    
      <category term="LSTM" scheme="http://cuiyungao.github.io/tags/LSTM/"/>
    
      <category term="Slot Filling" scheme="http://cuiyungao.github.io/tags/Slot-Filling/"/>
    
      <category term="Seq2Seq" scheme="http://cuiyungao.github.io/tags/Seq2Seq/"/>
    
  </entry>
  
  <entry>
    <title>Machine Comprehension Using Match-LSTM and Answer Pointer</title>
    <link href="http://cuiyungao.github.io/2016/09/01/machine/"/>
    <id>http://cuiyungao.github.io/2016/09/01/machine/</id>
    <published>2016-09-01T09:48:16.000Z</published>
    <updated>2016-09-02T01:49:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>这是发在Arxiv 29, Aug, 2016的文章，作者Shuohang Wang和Jing Jiang (SMU).<br><a id="more"></a><br>文章的目的是机器理解(machine comprehension)，利用最新发布的数据集Stanford Question Answering Dataset (SQuAD，提供了大量的真实问题和人工创建的答案)。这个数据集的挑战有两个方面，首先答案不是取自于一个小的候选集，而是<mark>从原文来进行抽取</mark>；其次是答案的<mark>长度不同</mark>。本文基于两个LSTM的模型，<code>match-LSTM</code>和<code>Pointer Net</code>，前者为了提取上下文需要(textual entailment)，后者为了限制输出的tokens来自输入序列。</p>
<p>这个新的数据集的特殊之处在于<mark>许多问题是原文的解释</mark>。文章利用了这个特殊之处来进行机器理解。文章贡献在于提出了<code>new end-to-end neural network models for machine comprehension</code>。文章的一些模型跟思路可以用在以后的research中，所以这里做简单记录。</p>
<h3 id="Match-LSTM"><a href="#Match-LSTM" class="headerlink" title="Match-LSTM"></a><center><strong>Match-LSTM</strong></center></h3><p>Match-LSTM的主要作用是预测上下文需求，即给定两个句子，其中一句是前提(premise)，一句是假设(hypothesis)，预测前提是否是假设的必要。Match-LSTM有序列性地经过hypothesis的tokens。在hypothesis的每个位置，<code>attention mechanism</code>用来获取premise的weighted vector representation。本质上，match-LSTM将attention-weighted premise加到每一个hypothesis的token上，并用这个matching的结果预测。</p>
<h3 id="Pointer-Net"><a href="#Pointer-Net" class="headerlink" title="Pointer Net"></a><center><strong>Pointer Net</strong></center></h3><p>Pointer net主要适用于一类问题，即输出序列的token必须要来自于输入序列的token。</p>
<h3 id="Proposed-Model"><a href="#Proposed-Model" class="headerlink" title="Proposed Model"></a><center><strong>Proposed Model</strong></center></h3><p>文章提供了两种模型,<code>sequence model</code>和<code>boundary model</code>，如下图。两个模型都有三层网络，即<code>preprocessing layer</code>，<code>match-LSTM layer</code>和<code>answer pointer layer</code>；区别在于输出，前者输出整个答案的tokens，后者输出答案开始和结束的token，取中间的tokens作为答案。<mark>Boundary model巧妙地保证了输出的连贯性。</mark></p>
<center><img src="/img/papers/machine1.png" width="100%"></center>

<h4 id="LSTM-Preprocessing-Layer"><a href="#LSTM-Preprocessing-Layer" class="headerlink" title="LSTM Preprocessing Layer"></a><strong>LSTM Preprocessing Layer</strong></h4><p>这层的作用是将token的representation结合上下文信息，LSTM的hidden layer即为passage和question的表示形式。<br>$$<br>\mathbf{H}^p = \overrightarrow{LSTM}(\mathbf{P}), \mathbf{H}^q = \overrightarrow{LSTM}(\mathbf{Q}),<br>$$<br>其中，$\mathbf{H}^p\in \mathbb{R}^{l\times P}$, $\mathbf{H}^q\in \mathbb{R}^{l\times Q}$即表示形式，$\mathbf{h}_i^p$和$\mathbf{h}_i^q$分别表示passage和question中的第$i$个token。</p>
<h4 id="Match-LSTM-Layer"><a href="#Match-LSTM-Layer" class="headerlink" title="Match-LSTM Layer"></a><strong>Match-LSTM Layer</strong></h4><p>这层的作用是引入attention，基于<code>word-by-word attention mechanism</code>得到attention weight vector $\overrightarrow{\alpha}_i\in \mathbb{R}^Q$，<br>$$<br>\overrightarrow{\mathbf{G}}_i = tanh(\mathbf{W}^q\mathbf{h}_i^q+(\mathbf{W}^p\mathbf{h}_i^p+\mathbf{W}^r\overrightarrow{\mathbf{h}}_{i-1}^r+\mathbf{b}^p)\otimes\mathbf{e}_Q), \\<br>\overrightarrow{\alpha}_i = softmax(\mathbf{w}^T\overrightarrow{\mathbf{G}}_i + \mathbf{b}\otimes\mathbf{e}_Q),<br>$$<br>得到的$\overrightarrow{\alpha}_{i,j}$表示passage中第$i$个token和question中第$j$个token中间的匹配程度。Question的weighted version的表示形式为：<br>$$<br>\overrightarrow{\mathbf{z}}_i = \begin{bmatrix}<br>\mathbf{h}_i^p \\<br>\mathbf{H}^q\overrightarrow{\alpha}_i^T<br>\end{bmatrix}.<br>$$<br>得到的向量$\overrightarrow{\mathbf{z}}_i$输入到经典的LSTM模型中即为<code>match-LSTM</code>：<br>$$<br>\overrightarrow{\mathbf{h}}_i^r = \overrightarrow{LSTM}(\overrightarrow{\mathbf{z}}_i, \overrightarrow{\mathbf{h}}_{i-1}^r).<br>$$</p>
<h4 id="Answer-Pointer-Layer"><a href="#Answer-Pointer-Layer" class="headerlink" title="Answer Pointer Layer"></a><strong>Answer Pointer Layer</strong></h4><p>用得到的$\overrightarrow{\mathbf{H}}^r$作为输入，预测输出序列。前面提到的两种模型的不同就体现在这一步，<code>sequence model</code>是生成answer sequence直到句尾，即$p(\mathbf{a}|\mathbf{H}^r)=\prod_k p(a_k|a_1, a_2, …, a_{k-1}, \mathbf{H}^r)$；而<code>boundary model</code>仅仅是找到answer的开头跟结尾，即$p(\mathbf{a}|\mathbf{H}^r)=p(a_s|\mathbf{H}^r)p(a_e|a_s, \mathbf{H}^r)$。</p>
<h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a><center>Experiment</center></h3><p>结果如下表所示。hidden layer维度定义为150，发现boundary model结果最好。作者还对各种类型的问题和答案的长度进行了比较分析，发现问题答案越长的结果比较差，”when”类型的问题结果较好。<mark>文章还对现有的数据集进行了总结。</mark></p>
<center><img src="/img/papers/machine2.png" width="100%"></center>

<h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a><center><strong>Reference</strong></center></h4><p>[1] <a href="http://128.84.21.199/pdf/1608.07905v1.pdf" target="_blank" rel="external">Machine Comprehension Using Match-LSTM and Answer Pointer</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是发在Arxiv 29, Aug, 2016的文章，作者Shuohang Wang和Jing Jiang (SMU).&lt;br&gt;
    
    </summary>
    
      <category term="papers" scheme="http://cuiyungao.github.io/categories/papers/"/>
    
    
      <category term="LSTM" scheme="http://cuiyungao.github.io/tags/LSTM/"/>
    
      <category term="Machine Comprehension" scheme="http://cuiyungao.github.io/tags/Machine-Comprehension/"/>
    
  </entry>
  
  <entry>
    <title>Understanding RNN</title>
    <link href="http://cuiyungao.github.io/2016/08/25/rnn/"/>
    <id>http://cuiyungao.github.io/2016/08/25/rnn/</id>
    <published>2016-08-25T02:49:14.000Z</published>
    <updated>2016-08-26T07:00:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章主要总结RNN，主要<a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" target="_blank" rel="external">出处</a>。</p>
<a id="more"></a>
<p>RNN在NLP的应用主要是两方面，一方面是可以判断两个句子的相似度；另外一方面则是可以生成新的文本。RNN的主要想法是<mark>使用sequential information</mark>。在传统的神经网络中，输入被认为是相互独立的，而RNN可以学习历史的信息，对sequence中的每一个element都进行相同的任务。另外一个想法是<mark>RNN有历史计算的<code>memory</code></mark>。这个<code>memory</code>就是网络的hidden state，所以<mark>hidden state是RNN的关键</mark>。较大的hidden layer size可以学到更复杂的patterns，但是会增加计算力。需要注意一下几点：</p>
<ol>
<li>Hidden state就是网络的memory，它获取历史信息，但是不能获取很长步骤之前的信息；</li>
<li>RNN在所有步骤之间共享参数，而传统的神经网络是每一层学习不同的参数。这能大大地减少需要学习的参数；</li>
<li>每一步不一定要有输出，这是由任务决定的；同理，每一步也不一定要有输入。</li>
</ol>
<p>因为参数在所有步骤之间共享，每一步的gradient不仅仅依赖于当前步的计算，还有之前很多步的计算，即<code>Backpropagation Through Time</code> (BPTT)。理论上，RNN绝对能处理<code>long-term dependencies</code>的情况，但是在实际应用中，RNN并不能有效地学习历史信息 (因为存在vanishing/exploding gradient problems)。LSTM是用来解决这种问题。所有的recurrent neural networks都有神经网络的重复module链的形式。在传统的RNN当中，重复的module有着非常简单的结构，比如单个的tanh层。LSTM结构也呈链状，但是在每一个单独的网络中，它有四层，而不是简单的一层。如图所示。LSTM的关键是cell state，即在图表中的上层，LSTM能够移除或者增加cell state。门(gate)的作用是选择性地让信息通过。有很多完善LSTM的方法，比如增加<code>peephole connections</code>，使用coupled forget gate和input gate来决定要forget和add的信息，以及GRU (Gated Recurrent Unit,将cell state和hidden state结合到<code>update gate</code>中)。具体的过程可以参见[1]。</p>
<center><img src="/img/daily/rnn1.png" width="100%"></center>

<p>RNN的延伸还有BRNN (Bidirectional RNN)和Deep (Bidriectioanl) RNN。前者的主要想法是时间$t$的输出不仅依赖于当前element，还有future element有关系；它的输出建立在双向的hidden state的基础之上。后者是每一个time step有很多层，这种情况通常有更高的学习能力，但通常也需要大量的训练数据。详细解释可以参考[2]。对于backpropagation，可以参考[3,4]。</p>
<p>Vanilla Neural Network的主要限制是API受限，只接受固定大小的输入和固定大小的输出，以及固定数量的计算步骤。引用原文[5]的话，<mark>If training vanilla neural nets is optimization over functions, training recurrent nets is optimization over programs.</mark></p>
<p>总结了RNN的衍生算法，比如DRAW, GAN, Pixel RNN等，可以看[6]。详细的GRU与LSTM之间的区别，可以见[8]。</p>
<p>关于初始化过程，RNN当中一般不能单纯地初始化为0，这样会导致对称计算。我们必须<mark>随机地初始化</mark>，适当地初始化会对训练结果又影响。最好的初始化依赖于<code>activation function</code>，并且<code>recommend</code>的方法是将权重随机地初始化在$[-\frac{1}{\sqrt{n}}, \frac{1}{\sqrt{n}}]$这个范围内，其中$n$是前面层的输入链接数。为了降低计算的复杂度，可以用<code>hierarchical softmax</code>或者增加<code>projection layers</code>来避免大的矩阵相乘。</p>
<p>关于<mark>vanishing/exploding gradients</mark>，主要考虑到求导部分，比如$\frac{\partial E_3}{\partial W} = \sum_{k=0}^3\frac{\partial E_3}{\partial y_3}\frac{\partial \hat{y}_3}{\partial s_3}(\prod_{j=k+1}^3\frac{\partial s_j}{\partial s_{j-1})\frac{\partial s_k}{\partial W}$。可以看到向量对向量进行求导，结果是一个矩阵，即<code>Jacobian matrix</code>。当中间连乘的值趋近于0或者较大的时候，就会出现<code>vanishing gradients</code>或者<code>exploding gradients</code>的情况。对于exploding的情况，可以简单地通过定义一个threshold来解决，而且比较容易发现，当gradients为NaN时，程序会崩溃，所以vanishing的情况更值得关注。有很多解决vanishing的方法，比如将activation function换成<code>ReLU</code>，而不是<code>tanh/sigmoid</code>，更常用的解决方法是用LSTM或者GRU (LSTM的简化版本)，这两种方法都是用来解决vanishing gradients和long-range dependencies的学习问题。具体分析可见[7]。关于parameter update，可以参见[9]，后面关于使用这些gradients的summary非常有用。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a><center><strong>Reference</strong></center></h3><p>[1] <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">Understanding LSTM Networks</a><br>[2] <a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" target="_blank" rel="external">Recurrent Neural Networks Tutorial</a><br>[3] <a href="http://colah.github.io/posts/2015-08-Backprop/" target="_blank" rel="external">Calculus on Computational Graphs: Backpropagation</a><br>[4] <a href="http://cs231n.github.io/optimization-2/" target="_blank" rel="external">CS231n Convolutional Neural Networks for Visual Recognition</a><br>[5] <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="external">The Unreasonable Effectiveness of Recurrent Neural Networks</a><br>[6] <a href="https://github.com/tensorflow/magenta/tree/master/magenta/reviews" target="_blank" rel="external">Reviews of research papers</a><br>[7] <a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/" target="_blank" rel="external">Backpropagation through time and vanishing gradients</a><br>[8] <a href="http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/" target="_blank" rel="external">Implementing a GRU/LSTM RNN with python and theano</a><br>[9] <a href="http://cs231n.github.io/neural-networks-3/#update" target="_blank" rel="external">Parameter updates</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章主要总结RNN，主要&lt;a href=&quot;http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/&quot;&gt;出处&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="RNN" scheme="http://cuiyungao.github.io/tags/RNN/"/>
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>MF, FM, 与NMF</title>
    <link href="http://cuiyungao.github.io/2016/08/24/fm/"/>
    <id>http://cuiyungao.github.io/2016/08/24/fm/</id>
    <published>2016-08-24T08:53:46.000Z</published>
    <updated>2016-08-24T13:59:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>百度周讲，这次主要讲的是MF, FM, NMF。</p>
<a id="more"></a>
<p>假设存在一个集合$S={x_i, y_i}_N$，我们的目标是由$x_0$推测出$y_0$，即$x_0-&gt;y_0$。如果$y_0=\sum_i f(x_0, x_i, S)y_i$，即权重参数是由样本集本身决定的，那么我们称其为<code>non-parametric</code>的模型，比如KNN, one-shot都是<code>non-parametric</code>的模型。</p>
<p>在线性空间中看到$x_i^Tx_j$，我们可以通过<code>kernel</code>将其转换到非线性空间。</p>
<p>在解决问题的时候，我们需要考虑三个方面，即<code>模型</code>，<code>目标</code>，跟<code>算法</code>。</p>
<h3 id="MF"><a href="#MF" class="headerlink" title="MF"></a><center><strong>MF</strong></center></h3><p>MF (matrix factorization)，即矩阵分解。假设$X\in R^{m\times n}, U\in R^{d\times m}, V\in R^{d\times n}$，则有：</p>
<ul>
<li>模型： $U, V$</li>
<li>目标： ${min}_{U,V} ||X-U^TV||_F^2 + \lambda(||U||_F^2 + ||V||_F^2)$，后面为<code>正则项</code>，<mark>一定要有，不能扔掉！</mark></li>
<li>算法： 解决这个问题的算法有很多，比如GD, SDG等。</li>
</ul>
<p>对于目标函数，在$X$中会存在missing value，这时候我们将目标函数修改为：<br>$$<br>{min}_{U,V} \sum_{i,j \\ x_{ij}可见} ||x_{ij}-u_i^Tv_j||_2^2 + \lambda(||u_i||_2^2 + ||v_j||_2^2).<br>$$</p>
<p>我们从<mark>概率的角度</mark>来理解MF，有<code>Probabilistic MF</code> (PMF)或者叫<code>Bayes MF</code>,则有：<br>$$<br>p(u)=\prod_i p(u_i), p(u_i) = G(u_i|0, I_d) \\<br>p(v)=\prod_i p(v_i), p(v_i) = G(v_i|0, I_d) \\<br>p(x_{ij}|u,v) = G(x_{ij}|u_i^Tv_j, \xi^2),<br>$$<br>我们的目标函数是$max \prod_{ij \\ x_{ij}可见} p(x_{ij})$。这种不可积的情况，可用Bayes或者sampling的方法来解决。</p>
<h3 id="FM"><a href="#FM" class="headerlink" title="FM"></a><center><strong>FM</strong></center></h3><p>FM (Factorization Machine)，应用在很多领域，比如推荐系统等等。以二阶为例，<br>$$<br>\begin{align}<br>&amp; \sum_i\sum_{j&gt;i} v_i^Tv_jx_ix_j + \sum_i w_ix_i + b \\<br>= &amp; \frac{1}{2}\sum_i\sum_j v_i^Tv_jx_ix_j - \frac{1}{2}\sum_iv_i^Tv_ix_i^2+ \sum_i w_ix_i + b \\<br>= &amp; \frac{1}{2}(\sum_i v_ix_i)^T(\sum_i v_ix_i)- \frac{1}{2}\sum_iv_i^Tv_ix_i^2 + \sum_i w_ix_i+ b.<br>\end{align}<br>$$</p>
<h3 id="NMF"><a href="#NMF" class="headerlink" title="NMF"></a><center><strong>NMF</strong></center></h3><p>NMF (Non-negative matrix factorization)，目标函数为：<br>$$<br>L = {min}_{u\geq 0 \\ v\geq 0} ||X-U^TV||_F^2.<br>$$<br>解法有很多：</p>
<ol>
<li>正常解，后面truncate；</li>
<li>log barrier  增加$-\sum_{ij}log u_{ij}$等regularizers；</li>
<li>$U = \tilde{U}\odot\tilde{U}$，解$\tilde{U}$；</li>
<li>multiplicative rule.<br>$$<br>\frac{\partial L}{\partial U} = -V(X-U^TU)^T = VV^TU - VX<br>$$<br>我们得到$U = U-\eta(VV^TU-VX)$，由于$\eta$后面的两项均为正，则可以写成$U=U\odot\frac{VX}{VV^TU}$(哪一个是分子、哪一个是分母，通过考虑增减性来判断)。<mark>这样我们就把相减的形式转换成了相乘的形式。</mark></li>
</ol>
<p><mark>物理解释：</mark>考虑隐含变量，比如PLSI跟LSI等都会用到这些概念。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;百度周讲，这次主要讲的是MF, FM, NMF。&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="Machine Learning" scheme="http://cuiyungao.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Pixel Recurrent Neural Networks</title>
    <link href="http://cuiyungao.github.io/2016/08/22/pixelcnn/"/>
    <id>http://cuiyungao.github.io/2016/08/22/pixelcnn/</id>
    <published>2016-08-22T09:48:44.000Z</published>
    <updated>2016-08-22T09:53:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>ICML’16 best paper. Google DeepMind的作品。 van den Oord A, Kalchbrenner N, Kavukcuoglu K. Pixel Recurrent Neural Networks[J]. arXiv preprint arXiv:1601.06759, 2016.</p>
<a id="more"></a>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ICML’16 best paper. Google DeepMind的作品。 van den Oord A, Kalchbrenner N, Kavukcuoglu K. Pixel Recurrent Neural Networks[J]. arXiv preprint arXiv:1601.06759, 2016.&lt;/p&gt;
    
    </summary>
    
      <category term="papers" scheme="http://cuiyungao.github.io/categories/papers/"/>
    
    
      <category term="RNN" scheme="http://cuiyungao.github.io/tags/RNN/"/>
    
      <category term="CNN" scheme="http://cuiyungao.github.io/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>Why Should I Trust You? Explaining the Predictions of Any Classifier</title>
    <link href="http://cuiyungao.github.io/2016/08/22/classifier/"/>
    <id>http://cuiyungao.github.io/2016/08/22/classifier/</id>
    <published>2016-08-22T03:41:00.000Z</published>
    <updated>2016-08-22T06:37:48.000Z</updated>
    
    <content type="html"><![CDATA[<p>KDD’16。Ribeiro M T, Singh S, Guestrin C. “ Why Should I Trust You?”: Explaining the Predictions of Any Classifier[J]. arXiv preprint arXiv:1602.04938, 2016.</p>
<a id="more"></a>
<p>文章的主要贡献在于如何评判一个classifier的可靠性，提出了LIME算法。解决的问题是尽管有的分类器的准确率较高，但是分类基于的原因并不可信，所以这种分类器不应该被相信。文章的主要目的是，帮助人来决定是否要相信一个预测，在models之间选择，并改善不可信的分类器，识别其为什么不可信。</p>
<center><img src="/img/papers/classifier1.png" width="100%"></center>

<p><mark>据证明，为电影推荐或者其它一些自动化系统提供解释能够提高接受率。这个能够用在review analysis上面？</mark>Model和估计错误的原因通常有以下几种：</p>
<ol>
<li>Data leakage.无意的信号泄露到训练集和validation集中，但是真正使用的时候并不会使用这个信号。<mark>不太懂这个….</mark></li>
<li>Dataset shift.训练集跟测试集不同。</li>
<li>计算跟优化的标准不同。</li>
</ol>
<p>这种数据可靠性的explainers应该有哪些标准呢？一是<code>可解释性</code>，需要考虑到用户限制，有时候这些features并不一定被model所用，所以说explaination中的输入变量跟features可以不同；二是<code>local fidelity</code>，对一个实例的近邻的预测需要有意义。文章提出了<code>LIME</code> (Local Interpretable Model-agnostic Explanations)来对模型的可靠性进行解释。</p>
<h3 id="LIME"><a href="#LIME" class="headerlink" title="LIME"></a><center><strong>LIME</strong></center></h3><p>假设模型$g\in G$，$G$是一类potentially interpretable的模型，比如线性模型和决策树等。$\Omega(g)$为复杂度的衡量，$f(x)$表示$x$属于某个类的概率，$\pi_x(z)$定义$x$的近邻，$\mathcal{L}(f,g,\pi_x)$为在$\pi_x(z)$范围用$g$预测$f$的不可靠程度，由LIME生成的解释为：<br>$$<br>\xi(x) = {argmin}_{g\in G} \mathcal{L}(f,g,\pi_x) + \Omega(g)。<br>$$<br>算法过程如下：</p>
<center><img src="/img/papers/classifier2.png" width="70%"></center>

<h3 id="SP-LIME"><a href="#SP-LIME" class="headerlink" title="SP-LIME"></a><center><strong>SP-LIME</strong></center></h3><p>上面的LIME仅仅是针对单个实例，我们应该从整体上来评估一个模型，所以加进去了<code>pick step</code>来选择$B$个实例。如下图所示，特征$f_2$的得分较高。但是我们选择第2个和第5个文档来进行explaination预测。</p>
<center><img src="/img/papers/classifier3.png" width="70%"></center>

<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a><center><strong>Reference</strong></center></h3><p>[1] <a href="https://arxiv.org/pdf/1602.04938v3.pdf" target="_blank" rel="external">“Why Should I Trust You?” Explaining the Predictions of Any Classifier</a><br>[2] <a href="https://github.com/marcotcr/lime-experiments" target="_blank" rel="external">Code</a>.可以重复试验（含数据）。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;KDD’16。Ribeiro M T, Singh S, Guestrin C. “ Why Should I Trust You?”: Explaining the Predictions of Any Classifier[J]. arXiv preprint arXiv:1602.04938, 2016.&lt;/p&gt;
    
    </summary>
    
      <category term="papers" scheme="http://cuiyungao.github.io/categories/papers/"/>
    
    
      <category term="Machine Learning" scheme="http://cuiyungao.github.io/tags/Machine-Learning/"/>
    
      <category term="Classification" scheme="http://cuiyungao.github.io/tags/Classification/"/>
    
  </entry>
  
  <entry>
    <title>A Derivation of Backpropagation in Matrix Form</title>
    <link href="http://cuiyungao.github.io/2016/08/21/backpro/"/>
    <id>http://cuiyungao.github.io/2016/08/21/backpro/</id>
    <published>2016-08-21T09:53:32.000Z</published>
    <updated>2016-08-21T10:07:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>本篇罗列了back propagation的general functions。</p>
<a id="more"></a>
<p>在神经网络中，更新权重经常用<code>batch gradient descent</code>或者<code>stochastic gradient descent</code>。后者是对单个的数据点更新权重，即$E=\frac{1}{2}||z-t||_2^2$；前者是批量地处理数据，即$E=\frac{1}{2}\sum_{i\in Batch}||z_i-t_i||_2^2$，其中，$t$是ground truth。这里就举例stochastic的方法，batch的方法类似。</p>
<p>为了减少误差函数，我们应该按照loss function梯度的逆方向来改变权重，即$w=w-\alpha_w\frac{\partial E}{\partial w}$，其中，$\alpha_w$是learning rate。假设神经网络有$L$层，$x_0$是输入向量，$x_L$为输出向量，$t$为truth vector，weight matrices为$W_1, W_2, .., W_L$，activation functions为$f_1, f_2, …, f_L$。</p>
<p>Forward Pass:<br>$$<br>x_i = f_i(W_ix_{i-1}) \\<br>E= ||x_L - t||_2^2<br>$$</p>
<p>Backward Pass:<br>$$<br>\delta_L = (x_L-t)\circ f_L^{\prime}(W_Lx_{L-1}) \\<br>\delta_i = W_{t+1}^T\delta_{i+1}\circ f_i^{\prime}(W_ix_{i-1})<br>$$</p>
<p>Weight Update:<br>$$<br>\frac{\partial E}{\partial W_i} = \delta_ix_{i-1}^T \\<br>W_i = W_i - \alpha W_i \circ\frac{\partial E}{\partial W_i}<br>$$</p>
<p>用矩阵表示变量，可以方便代码实现，并利用GPU来加速。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a><center><strong>Reference</strong></center></h3><p>[1] <a href="https://sudeepraja.github.io/Neural/" target="_blank" rel="external">A Derivation of Backpropagation in Matrix Form</a>。原文有更详细的过程。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇罗列了back propagation的general functions。&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Softmax Approximations for Learning Word Embeddings and Language Modeling</title>
    <link href="http://cuiyungao.github.io/2016/08/19/softmax/"/>
    <id>http://cuiyungao.github.io/2016/08/19/softmax/</id>
    <published>2016-08-19T14:39:25.000Z</published>
    <updated>2016-08-21T09:20:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要介绍了softmax在word embedding方面的应用，资料来源于<a href="http://www.slideshare.net/SebastianRuder/softmax-approximations-for-learning-word-embeddings-and-language-modeling-sebastian-ruder" target="_blank" rel="external">这里</a>。</p>
<a id="more"></a>
<h3 id="Language-Modeling-Objective"><a href="#Language-Modeling-Objective" class="headerlink" title="Language Modeling Objective"></a><center><strong>Language Modeling Objective</strong></center></h3><p>语言模型的目标是最大化给定词$w_t$的前$n$个单词预测当前词的概率，即$p(w_t|w_{t-1},…,w_{t-n+1})$。对于n-gram models，即:<br>$$<br>p(w_t|w_{t-1},…,w_{t-n+1})=\frac{count(w_{t-n+1},…,w_{t-1},w_t)}{count(w_{t-n+1},..,w_{t-1})},<br>$$<br>对于神经网络，即：<br>$$<br>p(w_t|w_{t-1},…,w_{t-n+1})=\frac{exp(h^Tv_w)}{\sum_{w_i\in V}exp(h^Tv_{w_i})}.<br>$$</p>
<p>Maximum entropy models最小化同一个概率分布，即$P_h(y|x)=\frac{exp(h\cdot f(x,y))}{\sum_{y^{\prime}\in y}exp(h\cdot f(x,y^{\prime}))}$,其中$h$是一个<code>weight vector</code>，$f(x,y)$是一个feature vector。在神经网络中经常用于：</p>
<ul>
<li>Multi-class分类；</li>
<li>“Soft” selection, 比如attention, memory retrievals等。</li>
</ul>
<p>分母经常被叫做<code>partition function</code>,即$Z=\sum_{w_i\in V}exp(h^Tv_{w_i})$。<code>Softmax-based approaches</code>保持了softmax层的完整性，使得算法更高效；而<code>sampling-based approaches</code>优化估计softmax的不同的loss函数。</p>
<h3 id="Softmax-based-Approaches"><a href="#Softmax-based-Approaches" class="headerlink" title="Softmax-based Approaches"></a><center><strong>Softmax-based Approaches</strong></center></h3><p><code>Hierarchical softmax</code>可以被看做是一个二叉树，最多估计$log_2|V|$个节点，而不是所有的$|V|$个节点，如图Fig.1所示。结构比较重要，可以快速地遍历点，经常使用的结构是<code>Huffman tree</code>，对于frequent words路径短，如图Fig.2所示。</p>
<center><img src="/img/daily/softmax1.png" width="80%"></center><br><center>Fig.1 Hierarchical Softmax</center><br><center><img src="/img/daily/softmax2.png" width="80%"></center><br><center>Fig.2 Hierarchical Softmax [Mnih and Hinton, 2008]</center>

<p><code>Differentiated Softmax</code>的出发点是我们对于经常出现的词获取的知识较多，但是对于很少出现的词信息较少，这就导致前者可以学习较多的参数，而后者较少，最终导致每个输出单词的embedding的大小不同。频繁出现的词的embedding较大，而很少出现的词的embedding较小。框架图如图Fig.3所示。</p>
<center><img src="/img/daily/softmax3.png" width="50%"></center><br><center>Fig.3 Differentiated Softmax [Chen et al., 2015]</center>

<p><code>CNN-Softmax</code>学习产生word embedding的函数，而不是单独学习所有输出单词的embedding，如图Fig.4所示。</p>
<center><img src="/img/daily/softmax4.png" width="40%"></center><br><center>Fig.3 CNN-Softmax [Jozefowicz et al., 2016]</center>

<h3 id="sampling-based-Approaches"><a href="#sampling-based-Approaches" class="headerlink" title="sampling-based Approaches"></a><center><strong>sampling-based Approaches</strong></center></h3><p><code>Margin-based Hinge Loss</code>的出发点是只学习两种分类，一种是正确的词，一种是不正确的词。正确的词的得分较高，不正确的词的得分较低，即最大化：<br>$$<br>\sum_{x\in X}\sum_{w\in V}max{(0,1-f(x)+f(x^{(w)})},<br>$$<br>其中，$x^{(w)}$是一个<code>corrupted</code> window (目标词由任意的词来代替)，$f(x)$是模型输出的分数。</p>
<p><code>Noise Contrastive Estimation</code>的目的是将target word跟noise区分开，如图Fig.5所示。这样语言模型被转化为二分类模型，对于每个词按照一定的噪声分布（比如，unigram）获取$k$个噪声采样点，用logistic regression loss最小化cross-entropy函数。随着$k$数目的增长估计softmax。</p>
<center><img src="/img/daily/softmax5.png" width="80%"></center><br><center>Fig.5 Noise Contrastive Estimation (NCE) [Mnih and Teh, 2012]</center>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要介绍了softmax在word embedding方面的应用，资料来源于&lt;a href=&quot;http://www.slideshare.net/SebastianRuder/softmax-approximations-for-learning-word-embeddings-and-language-modeling-sebastian-ruder&quot;&gt;这里&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>FAIR Open-Sources fastText</title>
    <link href="http://cuiyungao.github.io/2016/08/19/fasttext/"/>
    <id>http://cuiyungao.github.io/2016/08/19/fasttext/</id>
    <published>2016-08-19T09:57:42.000Z</published>
    <updated>2016-08-19T14:38:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>fastText是Facebook AI Research (FAIR) lab设计的library，主要用来创建text representation和进行文本分类（详见<a href="http://arxiv.org/abs/1607.04606" target="_blank" rel="external">论文</a>）。</p>
<a id="more"></a>
<p>fastText结合了几个在NLP和machine learning中最成功的概念，包括<code>bag of n-grams</code>, <code>using subword information</code>,以及<code>sharing information across classes with a hidden representation</code>。为了加速计算速度，fastText采用<code>hierarchical softmax</code>来平衡类的分布。</p>
<p>fastText为了有效地处理有大量种类的数据集，采用了hierarchical classifier，而不是flat structure，使得training和testing的复杂度降到linear或者与类的数目成对数关系。为了解决类的分布不均衡的问题，fastText采用了<code>Huffman algorithm</code>建立tree的方法；结果是比较频繁的种类的树的深度较浅。基于fastText，training的时间可以从几天降低到几秒钟，并且在性能上与state-of-the-art的方法接近，如下表所示。</p>
<center><img src="/img/papers/fasttext1.png" width="100%"></center>

<p>在语言的表示形式上，fastText的表现优于word2vec，结果如下表所示。<mark>那么中文的结果怎样？</mark></p>
<center><img src="/img/papers/fasttext2.png" width="100%"></center>


<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a><center><strong>Reference</strong></center></h3><p>[1] <a href="https://code.facebook.com/posts/1438652669495149/fair-open-sources-fasttext" target="_blank" rel="external">FAIR open-sources fastText</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;fastText是Facebook AI Research (FAIR) lab设计的library，主要用来创建text representation和进行文本分类（详见&lt;a href=&quot;http://arxiv.org/abs/1607.04606&quot;&gt;论文&lt;/a&gt;）。&lt;/p&gt;
    
    </summary>
    
      <category term="papers" scheme="http://cuiyungao.github.io/categories/papers/"/>
    
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Supervised Sequence Labeling with Recurrent Neural Networks</title>
    <link href="http://cuiyungao.github.io/2016/08/19/supseq/"/>
    <id>http://cuiyungao.github.io/2016/08/19/supseq/</id>
    <published>2016-08-19T07:00:56.000Z</published>
    <updated>2016-08-22T07:59:34.000Z</updated>
    
    <content type="html"><![CDATA[<p>与传统的<code>supervised pattern recognition</code>不同的是，<code>sequence labeling</code>的单独数据点之间并不是相互独立的，后者的输入和labels都是互相关联的序列。<code>Sequence labeling</code>的目的是确定输出labels的位置和类型。<br><a id="more"></a></p>
<p>RNN对sequence labeling比较有效，因为对上下文信息比较灵活（决定forget和remember什么样的信息）；可以接受不同类型的数据和数据的表示形式；可以在序列扭曲中发现sequential patterns。本文主要介绍LSTM和bi-directional RNN。</p>
<h3 id="Generative-and-Discriminative-Methods"><a href="#Generative-and-Discriminative-Methods" class="headerlink" title="Generative and Discriminative Methods"></a><center><strong>Generative and Discriminative Methods</strong></center></h3><p>直接计算类的概率$p(C_k|x)$的方法被认为是discriminative methods；有些情况下，需要先计算类的条件密度$p(x|C_k)$，基于Bayes’ rule来计算，即$p(C_k|x)=\frac{p(x|C_k)p(C_k)}{p(x)}$，其中$p(x)=\sum_k p(x|C_k)p(C_k)$，这样的方法被认为是<code>generative</code>的方法。<code>Generative methods</code>的好处是每一个类都可以独立于其它的类来进行训练，而<code>discriminative methods</code>在每一次新的类加入时都要被重新训练。但是，后者的分类结果较好，因为它的目标是找到类的边界。文章集中在<code>discriminative sequence labeling</code>。</p>
<p>Sequence classification通常分为三类，越来越细地分为temporal classification, segment classification和sequence classification。Temporal classification中的数据是<code>weakly labelled</code>在target sequences之外；segment classification则是temporal classification的一个特殊例子，数据必须用target sequences来进行<code>strongly labelled</code>；sequence classification则指的是每个输入序列只能对应一个类，比如识别单个的手写字母。</p>
<p>Neural networks当中的activation functions都是可求导的，e.g.，<br>$$<br>\frac{\partial tanh(x)}{\partial x} = 1-tanh(x)^2 \\<br>\frac{\partial \sigma (x)}{\partial x} = \sigma(x)(1-\sigma(x)),<br>$$<br>由于activation functions是将无限的输入域映射到有限的输出域，所以通常也被称为<code>squashing functions</code>。</p>
<h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a><center><strong>RNN</strong></center></h3><p>RNN跟Neural Network的不同之处在于NN只能从输入映射到输入，而RNN可以将历史输入映射到输出，也就是说RNN可以将历史的输入保存在网络的中间状态，并影响到网络的输出。<br>Forward Pass:假设时间$t$输入$i$的值为$x_i^t$，$a_j^t$和$b_j^t$分别是时间$t$对单元$j$的输入和激活函数。对于隐含单元，有：<br>$$<br>a_h^t = \sum_{i=1}^I w_{ih}x_i^t + \sum_{h^{\prime}=1}^H w_{h^{\prime}h}b_{h^{\prime}}^{t-1},<br>$$<br>其中，$b_h^t=\theta_h(a_h^t)$，研究发现$b_i^0$初始化成非零的值效果会更好一些。从隐含层到输出层可以由下式得到：<br>$$<br>a_k^t = \sum_{h=1}^H w_{hk}b_h^t。<br>$$</p>
<p>Backward Pass:权重的更新通常有两种方法，<code>real time recurrent learning</code> (RTRL)和<code>backpropagation through time</code> (BPTT)。文章主要关注后者，因为后者概念上更简单，而且在计算时间上更高效（虽然memory上不是很高效）。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;与传统的&lt;code&gt;supervised pattern recognition&lt;/code&gt;不同的是，&lt;code&gt;sequence labeling&lt;/code&gt;的单独数据点之间并不是相互独立的，后者的输入和labels都是互相关联的序列。&lt;code&gt;Sequence labeling&lt;/code&gt;的目的是确定输出labels的位置和类型。&lt;br&gt;
    
    </summary>
    
      <category term="papers" scheme="http://cuiyungao.github.io/categories/papers/"/>
    
    
      <category term="RNN" scheme="http://cuiyungao.github.io/tags/RNN/"/>
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
      <category term="Sequence Labeling" scheme="http://cuiyungao.github.io/tags/Sequence-Labeling/"/>
    
  </entry>
  
  <entry>
    <title>Sequence Labeling with Discriminative Models</title>
    <link href="http://cuiyungao.github.io/2016/08/19/sequence/"/>
    <id>http://cuiyungao.github.io/2016/08/19/sequence/</id>
    <published>2016-08-19T03:01:09.000Z</published>
    <updated>2016-08-19T06:50:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>总结了UIUC CS498JH: Introduction to NLP (Fall 2012)关于sequence labeling的<a href="https://courses.engr.illinois.edu/cs498jh/Slides/Lecture08HO.pdf" target="_blank" rel="external">课件</a>。</p>
<a id="more"></a>
<p>Sequence labeling包含4个基本任务，即<code>POS tagging</code>，<code>NP chunking</code>，<code>shallow parsing</code>和<code>named entity recognition</code>。这里的<code>discriminative model</code>可以理解为<code>generative model</code>，也就是根据当前word生成其种类，主要用到了<code>maximum entropy classifiers</code>和<code>MEMMs</code>。</p>
<h3 id="Tasks-in-Sequence-Labeling"><a href="#Tasks-in-Sequence-Labeling" class="headerlink" title="Tasks in Sequence Labeling"></a><center><strong>Tasks in Sequence Labeling</strong></center></h3><p>POS tagging是给每个词定义词性，如Fig.1所示。NP chunking是识别所有的名词词组，如Fig.2所示。</p>
<center><img src="/img/daily/sequence1.png" width="70%"></center><br><center>Fig.1 POS Tagging</center>

<center><img src="/img/daily/sequence2.png" width="70%"></center><br><center>Fig.2 NP Chunking</center><br>更具体地，我们定义三种新的tag，如图Fig.3：<br>- B-NP:名词词组的开头；<br>- I-NP:名词词组的中间；<br>- O:名词词组的外面。<br><br><center><img src="/img/daily/sequence3.png" width="70%"></center><br><center>Fig.3 BIO Encoding</center>

<p>Shallow parsing识别所有的non-resursive的名词词组，包括动词词组(“VP”)和介词词组(“PP”)。Named entity recognition则找到所有提到的entities，比如people, organizations, locations和dates。这两种tasks都有BIO encoding的表示形式。</p>
<h3 id="Graphical-Models"><a href="#Graphical-Models" class="headerlink" title="Graphical Models"></a><center><strong>Graphical Models</strong></center></h3><p>Graphical models是概率模型的表示形式，<code>Nodes</code>表示任意变量的分布$P(X)$,<code>Arrows</code>表示依赖关系$P(Y)P(X|Y)$,<code>shaded nodes</code>表示观察到的变量，<code>white nodes</code>表示隐含变量。HMMs可以看做是对输入$\mathbf{w}$的generative models，即$P(\mathbf{w})=\prod_i P(t_i|t_{i-1})P(w_i|t_i)$，如图Fig.4所示。</p>
<center><img src="/img/daily/sequence4.png" width="70%"></center><br><center>Fig.4 HMMs as Generative Models</center>

<p>Sequence Labeling的<mark>定义</mark>是给定输入序列$\mathbf{w}=w_1, …, w_n$，预测其最佳的label sequence $\mathbf{t}=t_1, …, t_n$，即：<br>$$<br>{argmax}_{\substack{t}}P(\mathbf{t}|\mathbf{w}).<br>$$<br>我们用<code>conditional maximum likelihood estimation</code> (conditional MLE)来估计$w$，<br>$$<br>\begin{aligned}<br>\hat{\mathbf{w}}&amp;={argmax}_{\substack{\mathbf{w}}}\prod_i P(c_i|\mathbf{x}_i,\mathbf{w})\\<br>  &amp;= {argmax}_{\substack{\mathbf{w}}}\sum_i log(P(c_i|\mathbf{x}_i,\mathbf{w})) \\<br>  &amp;= {argmax}_{\substack{\mathbf{w}}}\sum_i log(\frac{e^{\sum_j w_j f_j(x_i,c)}}{\sum_{c^{\prime}e^{\sum_j w_j f_j(x_i,c^{\prime})}}}) \\<br>  &amp;= {argmax}_{\substack{\mathbf{w}}}(L_{\mathbf{w}})<br> \end{aligned}<br>$$<br>对其进行求导，我们可以得到下式。</p>
<center><img src="/img/daily/sequence5.png" width="70%"></center>

<p>假设对$P(\mathbf{w})$建模，假定是高斯（正态）分布，我们可以得到下式。</p>
<center><img src="/img/daily/sequence6.png" width="70%"></center>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;总结了UIUC CS498JH: Introduction to NLP (Fall 2012)关于sequence labeling的&lt;a href=&quot;https://courses.engr.illinois.edu/cs498jh/Slides/Lecture08HO.pdf&quot;&gt;课件&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
      <category term="CRF" scheme="http://cuiyungao.github.io/tags/CRF/"/>
    
      <category term="Sequence Labeling" scheme="http://cuiyungao.github.io/tags/Sequence-Labeling/"/>
    
  </entry>
  
  <entry>
    <title>The Unreasonable Confusion of Variational Autoencoders</title>
    <link href="http://cuiyungao.github.io/2016/08/18/vae/"/>
    <id>http://cuiyungao.github.io/2016/08/18/vae/</id>
    <published>2016-08-18T08:03:40.000Z</published>
    <updated>2016-08-18T09:47:47.000Z</updated>
    
    <content type="html"><![CDATA[<p>将deep learning和graphical model结合起来理解<a href="https://jaan.io/unreasonable-confusion/" target="_blank" rel="external">VAE</a>。</p>
<a id="more"></a>
<p>VAE即variational autoencoders，作用是设计复杂的数据的generative models，并fit到大数据集上。下面从两个角度来理解VAE，第一个是neural networks，第二个是概率模型中的variational inference。</p>
<h3 id="Neural-Network-Perspective"><a href="#Neural-Network-Perspective" class="headerlink" title="Neural Network Perspective"></a><center><strong>Neural Network Perspective</strong></center></h3><p>在神经网络语言中，一个VAE包含<code>encoder</code>, <code>decoder</code>和<code>loss function</code>，如下图所示。<code>Encoder</code>是一个神经网络，输入是一个数据点$x$，输出是其hidden representation $z$，这个过程中包含权重和biases $\theta$。<mark><code>Encoder</code>必须有效地压缩数据，并映射到低维空间。</mark>由于低维空间是随机的，encoder输出参数为高斯概率密度$q_{\theta}(z|x)$，我们可以从这个分布采样，得到$z$的噪声值。</p>
<center><img src="/img/daily/VAE1.png" width="90%"></center>

<p><code>Decoder</code>是另外一个神经网络，输入是hidden representation $z$，输出参数为数据的概率分布$p_{\phi}(x|z)$，这个过程中包含权重和biases $\phi$。<mark>由于从较小的维度得到较大的维度，所以信息会丢失。</mark>我们的目标是让信息尽可能丢失得比较少，即<code>encoder</code>要根据hidden representation有效地重建输入$x$。我们定义<code>loss function</code>是带regularizer的negative log-likelihood。因为没有所有点的全局表示形式，我们可以分解这个loss function到仅依赖于单独点$l_i$的项，这样对$N$个点的总的loss为$\sum_{i=1}^N l_i$，其中：<br>$$<br>l_i(\theta, \phi)=-E_{z\sim q_{\theta}(z|x_i)}[log p_{\phi}(x_i|z)] + KL(q_{\theta}(z|x_i)||p(z)),<br>$$<br>第一项是重建损失，或者是对第i个点的expected negative log-likelihood。这个预期是针对encoder在$z$上的分布，目的是尽量地重建数据。第二项是一个regularizer，作用是衡量encoder的分布$q_{\theta}(z|x)$和$p(z)$之间的距离，即在用$q$表示$p$的过程中损失了多少信息。</p>
<p>在VAE中，$p$被认为是正态分布，即$p(z)=Normal(0,1)$, regularizer的作用是<code>保证每个数字的hidden representation $z$足够不同</code>，如果不包含这一项，encoder会给每一个点一个欧式空间中不同的区域，这意味着同一个数字的两个图片也会有不同的表示形式。最后，我们用梯度下降的方法来训练VAE，每个step size $\rho$，参数$\theta$和$\phi$都会被更新，比如$\theta \, \gets \, \theta-\rho\frac{\partial l}{\partial \theta}$。</p>
<h3 id="Probability-Model-Perspective"><a href="#Probability-Model-Perspective" class="headerlink" title="Probability Model Perspective"></a><center><strong>Probability Model Perspective</strong></center></h3><p>在概率模型中，一个VAE包含数据$x$的<code>概率模型</code>和<code>隐含变量</code>$z$，即对于每个数据点$i$，<code>generative process</code>可以写成如下形式：</p>
<ul>
<li>首先计算隐含变量 $z_i \, \sim \, p(z)$；</li>
<li>然后计算数据点的后验概率 $x_i \, \sim \, p(x|z)$。</li>
</ul>
<center><img src="/img/daily/VAE2.png" width="30%"></center><br><center>A Graphical Model</center>

<p>这个模型定义了联合概率$p(x,z)=p(x|z)p(z)$，对于<code>inference</code>，我们的目标是给定观察数据，推测隐含变量的值，即计算后验概率$p(z|x)$，通过Bays，我们得到$p(z|x)=\frac{p(x|z)p(z)}{p(x)}$。分母$p(x)=\int p(x,z)p(z)\mathrm{d}z$的估计需要遍历所有隐含变量，这需要大量的时间，所以我们需要另外想办法来估计这个后验概率。</p>
<p><code>Variational inference</code>用一组分布$q_{\lambda}(z|x)$来对$p(z|x)$进行估计，$\lambda$表示不同的分布，也就是说，假定$q$是高斯分布，那么对每一个数据点的隐含变量$\lambda_{x_i}=(\mu_{x_i}, \sigma_{x_i}^2)$。我们的目标是使得这个估计$q_{\lambda}(z|x)$与真实的$q(z|x)$尽可能接近，由于都是概率，我们很自然地想到KL divergence，即：<br>$$<br>KL(q_{\lambda}(z|x)||p(z|x))=E_q[log q_{\lambda}(z|x)]-E_q[log p(x,z)] + log p(x).<br>$$</p>
<p>由之前的分析可知，$p(x)$很难得到。我们的目的是最小化这个KL divergence，将上式改写如下：<br>$$<br>log p(x) = KL(q_{\lambda}(z|x)||p(z|x))+ELBO(\lambda),<br>$$<br>其中，$ELBO(\lambda)=E_q[log p(x,z)]-E_q[log q_{\lambda}(z|x)]$，所以可以转变成最大化ELBO (Evidence Lower BOund)函数。由于数据点之间并没有共享隐含变量，所以总的目标函数可以认为是每个点的目标函数的和，也使得我们可以用梯度下降来更新迭代$\lambda$。所以单独一个点的ELBO可以写成 (需要进行化简)：<br>$$<br>ELBO_i(\lambda)=E_{q_{\lambda}(z|x_i)}[log p(x_i|z)]-KL(q_{\lambda}(z|x_i)||p(z))。<br>$$</p>
<h3 id="Combination-of-the-Two-Models"><a href="#Combination-of-the-Two-Models" class="headerlink" title="Combination of the Two Models"></a><center><strong>Combination of the Two Models</strong></center></h3><p>从上面的分析可以看出，VAE主要分为两部分，<code>inference network</code> (encoder)和<code>generative network</code> (decoder)。前者是参数化后验概率$q_{\theta}(z|x,\lambda)$，后者是参数化$p(x|z)$。由于没有全局的隐含变量，我们可以用梯度下降(e.g., minibatch)最大化ELBO。统一概率模型跟神经网络的参数，ELBO可以写成：<br>$$<br>ELBO_i(\theta, \phi) = E_{q_{\theta}(z|x_i)}[log p_{phi}(x_i|z)]-KL(q_{\theta}(z|x_i)||p(z)).<br>$$<br>所以，$ELBO_i(\theta, \phi)=-l_i(\theta, \phi)$。</p>
<p>我们从概率模型的角度，定义了目标函数（ELBO)和inference algorithm (gradient ascent on the ELBO)。<mark>不懂inference到底是哪个过程？？</mark>在神经网络中，inference通常指给定新的之前没有遇到的点来预测隐含的表示形式；而在概率模型中，inference意味着给定观察到的点来预测隐含变量的值。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a><center><strong>Reference</strong></center></h3><p>[1] <a href="https://jaan.io/unreasonable-confusion/" target="_blank" rel="external">The unreasonable confusion of variational autoencoders.</a><br>[2] <a href="https://github.com/altosaar/vae/blob/master/vae.py" target="_blank" rel="external">代码.</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;将deep learning和graphical model结合起来理解&lt;a href=&quot;https://jaan.io/unreasonable-confusion/&quot;&gt;VAE&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
      <category term="VAE" scheme="http://cuiyungao.github.io/tags/VAE/"/>
    
  </entry>
  
  <entry>
    <title>Learning Text Representation Using Recurrent Convolutional Neural Network with Highway Layers</title>
    <link href="http://cuiyungao.github.io/2016/08/17/RCNN/"/>
    <id>http://cuiyungao.github.io/2016/08/17/RCNN/</id>
    <published>2016-08-17T09:50:55.000Z</published>
    <updated>2016-08-17T12:58:23.000Z</updated>
    
    <content type="html"><![CDATA[<p>发表在SIGIR’16上。Wen, Y., Zhang, W., Luo, R., &amp; Wang, J. (2016). Learning text representation using recurrent convolutional neural network with highway layers. arXiv preprint arXiv:1606.06905.</p>
<a id="more"></a>
<p>作者提出了<mark>新的结合RNN和CNN的方式</mark>，即在中间加了一层highway，这层的作用是把RNN的输出进行特征选择，其结果作为CNN的输入。</p>
<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a><center><strong>Motivation</strong></center></h3><p>RNN跟CNN在NLP领域非常流行，但是两者各有优劣势。RNN虽然考虑了单词的顺序，但是它的问题在于后面的单词相比于前面单词的表示形式更能影响结果，对于情感分析这样的task，重要的单词预示着正确的情感，但是可能出现在文档的任何位置，所以RNN的结果并不是很好。而CNN可以很自然地解决这个问题，因为它通过max-pooling层平等地看待所有的词；但是CNN也有自己的问题，比如需要确定window的大小以及众多的filter参数。</p>
<p>RCNN模型旨在结合这两者的优点。模型对整个文档采用bi-directional recurrent neural network，并结合词跟它的上下文来表示这个词，filter用来计算latent semantic vectors。最终，max-pooling用来获取最重要的因素，并形成固定大小的句子表示。</p>
<p>这篇文章改进已有的RCNN模型，在bi-directional RNN和CNN之间插入了一个<mark>highway network</mark>，形成<mark>RCNN-HW</mark>模型，这个highway layer的作用是作为中间层来单独选择每个词的表示形式中的features。</p>
<h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a><center><strong>Model</strong></center></h3><p>整个模型框图如下所示。RNN能够处理当前的数据实例并且保留历史信息，常用的比如LSTM和GRU，后者比较简洁且<mark>性能较好（really??）</mark>，所以文章采用GRU。表达式如下：<br>$$<br>\mathbf{r}_t = \sigma(\mathbf{W}_r\mathbf{x}_t+\mathbf{U}_r\mathbf{h}_{t-1}+\mathbf{b}_r) \\<br>\mathbf{z}_t = \sigma(\mathbf{W}_z\mathbf{x}_t+\mathbf{U}_z\mathbf{h}_{t-1}+\mathbf{b}_z) \\<br>\mathbf{\tilde{h}}_t = tanh(\mathbf{W}_h\mathbf{x}_t+\mathbf{U}_h(\mathbf{r}_t\odot\mathbf{h}_{t-1})+\mathbf{b}_h) \\<br>\mathbf{h}_t = \mathbf{z}_t\odot\mathbf{h}_{t-1} + (1-\mathbf{z}_t)\odot\mathbf{\tilde{h}}_t,<br>$$<br>其中，$\odot$表示element-wise multiplication，$\mathbf{W,U,b}$表示输入、recurrent的权重和biases。所以，单词的表示形式$\mathbf{\tilde{x}}_t$可以结合上下文得到：<br>$$<br>\mathbf{\tilde{x}}_t = [\overleftarrow{\mathbf{h}_t}||\mathbf{x}_t||\overrightarrow{\mathbf{h}_t}].<br>$$</p>
<center><img src="/img/papers/RCNN1.png" width="70%"></center>

<p><code>One-layer</code> highway network是RNN和CNN的中间层，表示如下：<br>$$<br>\mathbf{y}_t=\tau\odot g(\mathbf{W}_H\mathbf{\tilde{x}}_t+\mathbf{b}_H)+(1-\tau)\odot\mathbf{\tilde{x}}_t,<br>$$<br>其中，$g$是非线性函数，$\tau$为<code>&quot;transform gate&quot;</code>，即$\tau=\sigma(\mathbf{W}_{\tau}\mathbf{x}_t+\mathbf{b}_{\tau})$。<mark>这个highway network的设计跟GRU的<code>update gate</code>$\mathbf{z}$非常相似，</mark>其作用是一部分输入信息被不变地传递到输出，其它的信息则需要进行非线性变换。<mark>实验说明这样的设计可以获取重要的信息。</mark></p>
<p>在CNN阶段，我们将window size设置为1，因为bidirectional RNN和highway layer已经获取了每个单词的上下文信息。</p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a><center><strong>Experiments</strong></center></h3><p>文章提供了<a href="https://github.com/wenying45/deep_learning_tutorial" target="_blank" rel="external">源代码</a>。情感分析实验阶段，比较了六种方法，分别是：Sum-of-Word-Vector (COW), LSTM, Bi-LSTM, CNN, CNN+LSTM,和RCNN，结果如下图。这些网络用来学习文本的表示形式，之后用softmax进行预测。实验没有采用pre-training model (e.g., word2vec)或者regularizer (e.g., Dropout)，这些使实验结果更好。情感分类的实验结果表明，CNN的效果要好于RNN，可能是由于RNN很难被训练，并且对超参数和后面的单词比较敏感。RCNN的效果最佳，因为它综合了CNN获取局部信息的优势和RNN获取全局信息的优势。CNN-LSTM是用CNN的局部信息作为RNN的输入，但是局部信息并没有时序关系。</p>
<center><img src="/img/papers/RCNN2.png" width="70%"></center>

<p>文章还比较了不同的文本长度对实验结果的影响，发现对于短文本，这几种方法都不是很好；而对于较长的文本，RCNN的优势就体现出来，如下图所示。RCNN不光可以保留较长的文本信息，还可以通过CNN的max-pooling减少噪声；RCNN-HW则通过highway layer进一步减少噪声，进行特征选择，所以在较长输入时效果最好。</p>
<center><img src="/img/papers/RCNN3.png" width="70%"></center>

<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a><center><strong>Reference</strong></center></h3><p>[1] <a href="http://arxiv.org/pdf/1606.06905v2.pdf" target="_blank" rel="external">Learning Text Representation Using Recurrent Convolutional Neural Network with Highway Layers</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;发表在SIGIR’16上。Wen, Y., Zhang, W., Luo, R., &amp;amp; Wang, J. (2016). Learning text representation using recurrent convolutional neural network with highway layers. arXiv preprint arXiv:1606.06905.&lt;/p&gt;
    
    </summary>
    
      <category term="papers" scheme="http://cuiyungao.github.io/categories/papers/"/>
    
    
      <category term="RNN" scheme="http://cuiyungao.github.io/tags/RNN/"/>
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
      <category term="CNN" scheme="http://cuiyungao.github.io/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>What to do about Ad Blocking Impact on Customer Experience?</title>
    <link href="http://cuiyungao.github.io/2016/08/17/adblock/"/>
    <id>http://cuiyungao.github.io/2016/08/17/adblock/</id>
    <published>2016-08-17T09:02:25.000Z</published>
    <updated>2016-08-17T12:52:09.000Z</updated>
    
    <content type="html"><![CDATA[<p>这是一篇总结ad blocking对用户体验影响的<a href="http://apmblog.dynatrace.com/2016/08/09/ad-blocking-impact-customer-experience/" target="_blank" rel="external">报道</a>。</p>
<a id="more"></a>
<p>用户对ad比较反感的原因，总结有三点：</p>
<ol>
<li>Ads是获取网页的速度减慢；</li>
<li>Ads干扰了用户希望浏览的内容；</li>
<li>用户担心Ads会泄露个人隐私信息。</li>
</ol>
<p>用户使用Ad blocker已经非常普遍，据报道有20%的广告被拦截，从而造成了广告收入的损失。下面分析这三个方面的根本原因。<br>第一点的原因是广告的数量，插件和第三方服务太多。对于如何测量用户体验，作者提出了用response time。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是一篇总结ad blocking对用户体验影响的&lt;a href=&quot;http://apmblog.dynatrace.com/2016/08/09/ad-blocking-impact-customer-experience/&quot;&gt;报道&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="Ad" scheme="http://cuiyungao.github.io/tags/Ad/"/>
    
  </entry>
  
  <entry>
    <title>Consensus Attention-based Neural Networks for Chinese Reading Comprehension</title>
    <link href="http://cuiyungao.github.io/2016/08/17/CAS/"/>
    <id>http://cuiyungao.github.io/2016/08/17/CAS/</id>
    <published>2016-08-17T06:51:22.000Z</published>
    <updated>2016-08-17T08:29:05.000Z</updated>
    
    <content type="html"><![CDATA[<p>Cui, Y., Liu, T., Chen, Z., Wang, S., &amp; Hu, G. (2016). Consensus Attention-based Neural Networks for Chinese Reading Comprehension. arXiv preprint arXiv:1607.02250.</p>
<a id="more"></a>
<p>这篇文章是哈工大与科大讯飞合作的作品。文章的主要贡献在于：</p>
<ol>
<li>提供了中文阅读理解的数据集，包括人民日报新闻 (People Daily news)和儿童童话 (Children’s Fairy Tale, CFT)；</li>
<li>完善了已有的attention-based NN model，提出了consensus attention-based neural network architecture，即consensus attention sum reader (CAS Reader)。</li>
</ol>
<h3 id="Problem-Definition"><a href="#Problem-Definition" class="headerlink" title="Problem Definition"></a><center><strong>Problem Definition</strong></center></h3><p><code>Cloze-style reading comprehension problem</code>致力于理解给定的上下文或者文档，回答根据文档的属性所得的问题，并且回答是文档当中存在的一个词。可以由下面的三元组表示：<br>$&lt;\mathcal{D}, \mathcal{Q}, \mathcal{A}&gt;$，<br>其中$\mathcal{D}$为文档，$\mathcal{Q}$为query，$\mathcal{A}$为针对query的回答。</p>
<p>这种问题的解决通常采用attention-based neural network的方法，但是这种方法需要<mark>大量的训练数据</mark>来训练预测的可靠模型。现有的方法是自动地生成大量的训练数据。</p>
<h3 id="Consensus-Attention-Sum-Reader"><a href="#Consensus-Attention-Sum-Reader" class="headerlink" title="Consensus Attention Sum Reader"></a><center><strong>Consensus Attention Sum Reader</strong></center></h3><p>本文的算法是基于[2]的改进，目的是直接从文档中估计答案，而不是从所有的vocabularies估计。作者发现<mark>直接将query进行RNN之后的表示形式不足以表示query的整个信息</mark>，作者利用query的每一个<code>time slices</code>，建立了不同steps之间的<code>consensus attention</code>。<mark>感觉最近是不是attention上的LSTM很火？这个可以改善一些常见的topic么？</mark></p>
<p>一般形式是，给定一组训练三元组$&lt;\mathcal{D}, \mathcal{Q}, \mathcal{A}&gt;$，首先将文档$\mathcal{D}$和query $\mathcal{Q}$的one-hot表示形式转换成共享embedding matrix $W_e$的连续的表示形式。<mark>由于query通常比文档短，通过共享embedding weights，query的表示形式可以受益于文档的表示形式</mark>，这比分开的embedding matrices有效。</p>
<p>之后，用两个不同的<code>bi-directional RNNs</code>得到文档和query的上下文表示形式。<mark>这种方法可以获取之前和之后的上下文信息。</mark>文章采用bi-directional Gated Recurrent Unit (GRU)，如下：<br>$$<br>e(x) = W_e\times x, \; where \, x\in\mathcal{D,Q} \\<br>\overrightarrow{h_s} = \overrightarrow{GRU}(e(x)) \\<br>\overleftarrow{h_s} = \overleftarrow{GRU}(e(x)) \\<br>h_s = [\overrightarrow{h_s};\overleftarrow{h_s}].<br>$$</p>
<p>文档的<code>attention</code>即一个概率分布，这里用$h_{doc}$和$h_{query}$分别表示文档和query的上下文表示形式，都是三维的张量。这两个张量的内积表示每个文档单词在时间$t$对query单词的重要性。概率分布用softmax来获得，即：<br>$$<br>\alpha(t) = softmax(h_{doc}\odot h_{query}(t)).<br>$$</p>
<p>$\alpha(t)$即文档的<code>attention</code>，且$\alpha(t)=[\alpha(t)_1,\alpha(t)_2, …, \alpha(t)_n]$，$\alpha(t)_i$指的是文档第$i$个词在时间$t$的attention值。<code>Consensus attention</code>由merging function $f$得到，即：<br>$$<br>s=f(\alpha(1), …,\alpha(m)),<br>$$<br>其中，$s$是query最终的attention，$m$是query的长度。Merging function有以下几种heuristics，即：<br>$$<br>   s\propto<br>   \begin{cases}<br>    softmax(\sum_{t=1}^m \alpha(t)),\; if \, mode=sum; \\<br>    softmax(\frac{1}{m}\sum_{t=1}^m \alpha(t)), \; if \, mode=avg; \\<br>    softmax({max}_{t=1,…,m} \alpha(t)_i), \; if \, mode=max.<br>   \end{cases}<br>$$</p>
<p>最终，将结果$s$映射到vocabulary space $V$，并将在文档中同一个词出现在不同位置的attention value相加，如下式所示。（<mark>思考：这个方式跟直接从vocabulary中选取某个词有啥区别？因为文章的一个亮点是从文档当中选取，而不是从vocabularies当中选取。</mark>）</p>
<p>$$<br>P(w|\mathcal{D,Q})=\sum_{i\in I(w,\mathcal{D})} s_i, \, w\in V.<br>$$<br>其中，$I(w,\mathcal{D})$为单词$w$出现在文档$\mathcal{D}$中的位置。整个框架图由下图所示。</p>
<center><img src="/img/daily/CAS1.png" width="100%"></center><br><center>Fig.1 Architecture of the Proposed Consensus Attention Sum Reader (CAS Reader).</center>

<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a><center><strong>Experiments</strong></center></h3><p>框架实现采用Theano和Keras，在Tesla K40 GPU上训练。实验结果在某些数据集上要优于传统的不用Consensus的模型，结果不好的原因是以前的模型只是从entity当中挑选答案，而改进之后的模型是从文档当中选取。在中文分词上的结果表明，有Consensus的Model要优于传统的attention model，结果如下图所示。</p>
<center><img src="/img/daily/CAS2.png" width="80%"></center>

<p>文章另外一个有意思的点是，<mark>机器生成的问题跟人想问的问题不同，对人的问题产生效果比较差</mark>。比如，机器问题“I went to the <em>__</em> this morning .”，而人的问题是“Where did I go this morning ?”。<mark>我比较疑惑的是，这两个问题在中文里面不是一样么？只是加了一个语气词而已，那么效果怎么会差这么大？</mark></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a><center><strong>Reference</strong></center></h3><p>[1] <a href="https://arxiv.org/abs/1607.02250" target="_blank" rel="external">Consensus Attention-based Neural Networks for Chinese Reading Comprehension</a><br>[2] <a href="https://arxiv.org/abs/1603.01547" target="_blank" rel="external">Text Understanding with the Attention Sum Reader Network</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Cui, Y., Liu, T., Chen, Z., Wang, S., &amp;amp; Hu, G. (2016). Consensus Attention-based Neural Networks for Chinese Reading Comprehension. arXiv preprint arXiv:1607.02250.&lt;/p&gt;
    
    </summary>
    
      <category term="papers" scheme="http://cuiyungao.github.io/categories/papers/"/>
    
    
      <category term="RNN" scheme="http://cuiyungao.github.io/tags/RNN/"/>
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
      <category term="Attention Model" scheme="http://cuiyungao.github.io/tags/Attention-Model/"/>
    
  </entry>
  
  <entry>
    <title>Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond</title>
    <link href="http://cuiyungao.github.io/2016/08/16/arxiv0810/"/>
    <id>http://cuiyungao.github.io/2016/08/16/arxiv0810/</id>
    <published>2016-08-16T09:16:51.000Z</published>
    <updated>2016-08-17T01:21:43.000Z</updated>
    
    <content type="html"><![CDATA[<p>IBM的作品。Nallapati, R., Zhou, B., glar Gulçehre, Ç., &amp; Xiang, B. Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond.</p>
<a id="more"></a>
<p>所谓的<code>abstractive text summarization</code>就是压缩原始的文档，并保留原文档的主要概念。</p>
<h3 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a><center><strong>Contribution</strong></center></h3><p>文章将在MT (Machine Learning)中的Attentional Encoder-Decoder RNN算法应用到abstractive text summarization，并针对在abstractive text summarization中存在的三个问题（即为<mark>key-words建模，获取sentence-to-word的结构层级，和在training阶段处理少见的或者没有见过的词</mark>）对模型进行改善。</p>
<p>文章的另外一个贡献是提出了新的包含多句子总结的数据集，建立了后面research的benchmarks。</p>
<h3 id="Models"><a href="#Models" class="headerlink" title="Models"></a><center><strong>Models</strong></center></h3><p>模型包含encoder和decoder，encoder是一个bidirectional GRU-RNN，而decoder是一个跟encoder有着相同大小hidden-state size的uni-directional GRU-RNN；在source-hidden states之间是<code>attention mechanism</code>，最终由一个<code>soft-max layer</code>为目标词汇生成单词。文章的model有如下三个改进的方面：</p>
<ol>
<li>Feature-rich encoder;</li>
<li>Switching generator-pointer model;</li>
<li>Hierarchical attention model.</li>
</ol>
<p>第一个改进，为encoder的输入增加新的features，比如parts-of-speech tags, named-entity tags (NER tags),和离散后的TF以及IDF数据。</p>
<p>第二个改进，为decoder增加一个<code>switch</code>，决定用generator还是用一个指针指向文档当中的某一个位置。如果switch打开，则模型还是用以前的方法从目标词库当中生成单词；如果关闭的话，则生成一个指针指向源的某一个位置，这个位置的word就会被添加到summary当中。每一个<code>time step</code>，<code>Switch</code>被定义为基于linear layer的sigmoid函数，如下：<br>$$<br>P(s_i=1)=\sigma(\mathbf{v}^s\cdot(\mathbf{W}_h^sh_i)+\mathbf{W}_e^s\mathbf{E}[o_{i-1}]+\mathbf{W}_c^s\mathbf{c}_i+\mathbf{b}^s)),<br>$$<br>其中$P(s_i=1)$表示在第$i$个time step开关被打开的概率，$\mathbf{c}_i$是attention-weighted context vector，$<br>mathbf{W}_h^s, \mathbf{W}_e^s，\mathbf{W}_c^s, \mathbf{b}^s$和$\mathbf{v}_s$都是开关的参数。<mark>这个是不是就相当于CNN中的<code>filters</code>的概念。</mark>改进后的模型图如下图所示。</p>
<center><img src="/img/daily/arxiv08101.png" width="70%"></center>

<p>第三个改进，在encoder采用<mark>两层</mark>概念，即word level和sentence level。Attention mechanism同时作用在这两层上，即<code>word-level</code>的attention被相应的<code>sentence-level</code>的attention重新校正，定义如下：<br>$$<br>P^a(j) = \frac{P_w^a(j)P_s^a(s(j))}{\sum_{k=1}^{N_d}P_w^a(k)P_s^a(s(k))}，<br>$$<br>其中，$P_w^a(j)$是word-level第j个位置的attention weight，$P_s^a(l)$是原文件中第l个位置sentence-level的attention weight。得到的re-scaled attention被用来计算decoder的hidden state的输入（attention-weighted context vector）。而且，将positional embeddings加到sentence-level RNN的hidden state，来表示文档中句子的位置重要性。这个模型因此能够将关键句子跟关键词联系在一起。</p>
<center><img src="/img/daily/arxiv08102.png" width="70%"></center>

<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a><center><strong>Experiments</strong></center></h3><p>分了好几个实验，比较感兴趣的实验是在CNN/Daily Mail上的实验，实验比较了三个模型，实验结果如图所示。最后一个模型”temp”表示<code>Temporal Attention Model</code>，<mark>解决了entity重复出现的问题，这个可以用在tag identification上面</mark>。另外，感觉这个<code>Temporal Attention Model</code>跟LSTM类似，只不过是在attention上的LSTM。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a><center><strong>Reference</strong></center></h3><p>[1] <a href="https://aclweb.org/anthology/K/K16/K16-1028.pdf" target="_blank" rel="external">Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;IBM的作品。Nallapati, R., Zhou, B., glar Gulçehre, Ç., &amp;amp; Xiang, B. Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond.&lt;/p&gt;
    
    </summary>
    
      <category term="papers" scheme="http://cuiyungao.github.io/categories/papers/"/>
    
    
      <category term="RNN" scheme="http://cuiyungao.github.io/tags/RNN/"/>
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Enriching Word Vectors with Subword information</title>
    <link href="http://cuiyungao.github.io/2016/08/16/enrich/"/>
    <id>http://cuiyungao.github.io/2016/08/16/enrich/</id>
    <published>2016-08-16T02:06:02.000Z</published>
    <updated>2016-08-16T03:30:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章是facebook AI research作品，是fastText的基础。Bojanowski, P., Grave, E., Joulin, A., &amp; Mikolov, T. (2016). Enriching Word Vectors with Subword Information. arXiv preprint arXiv:1607.04606.</p>
<a id="more"></a>
<p>大神Tomas Mikolov的有一篇作品，借鉴参考当中的trick。</p>
<h3 id="Idea"><a href="#Idea" class="headerlink" title="Idea"></a><center><strong>Idea</strong></center></h3><p>这篇文章的主要思路是让word之间可以共享信息，即利用subword的特征。这种表示方法同样对rare words以及不同的语言有效。</p>
<h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a><center><strong>Model</strong></center></h3><p>总的框架类似于Skip-Gram Model，给定一组训练词序集$w_1, …, w_T$，目标是最大化基于$w_t$预测出$w_c$的概率，即：<br>$$<br>\sum_{t=1}^T\sum_{c\in\mathcal{C}_t} log p(w_c|w_t),<br>$$<br>其中$\mathcal{C}_t$指$w_t$周围的词的indices。给定一个scoring function $s$，将paris (word, context)映射到$\mathbb{R}$空间，那么一个context word的概率可以定义为：<br>$$<br>p(w_c|w_t)=\frac{e^{s(w_t,w_c)}}{\sum_{j=1}^W e^{s(w_t,j)}}.<br>$$<br>这个scoring function通常定义为$s(w_t,w_c)=\mathbf{u}_{w_t}^T\mathbf{v}_{w_c}$。</p>
<p>考虑一个word当中的$n$-grams $\mathcal{G}_w\subset{1,…,G}$，每个$n$-gram $g$可以表示成一个向量$z_g$，这个word可以表示成其$n$-grams的和。所以scoring function可以表示成：<br>$$<br>s(w,c)=\sum_{g\in\mathcal{G}_w} \mathbf{z}_g^T\mathbf{v}_c.<br>$$<br>为了限制memory，所有的$n$-grams被映射到1~K的整数，论文中K为2 millions，最终每个word被表示成index和$n$-grams的hash values。词的开头和结果加一个special characters，以便于找到前缀和后缀。为了提高模型效率，<mark>最频繁的$P$个单词不用$n$-grams来表示</mark>。</p>
<p>实验结果在similarity tasks上的效果不错。测量方法是模型评分和人工判断之间的Spearman’s rank correlation coefficient。实现采用C++，在运行时间上比Skip-Gram baseline慢了1.5倍，比较的方法当中Skip-Gram和CBOW由C<a href="https://code.google.com/archive/p/word2vec/" target="_blank" rel="external">实现</a>。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章是facebook AI research作品，是fastText的基础。Bojanowski, P., Grave, E., Joulin, A., &amp;amp; Mikolov, T. (2016). Enriching Word Vectors with Subword Information. arXiv preprint arXiv:1607.04606.&lt;/p&gt;
    
    </summary>
    
      <category term="papers" scheme="http://cuiyungao.github.io/categories/papers/"/>
    
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Query Understanding</title>
    <link href="http://cuiyungao.github.io/2016/08/13/query/"/>
    <id>http://cuiyungao.github.io/2016/08/13/query/</id>
    <published>2016-08-13T02:36:37.000Z</published>
    <updated>2016-08-14T08:24:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>Query understanding是很多AI应用场景的基础，比如机器人对话，车载导航等等。</p>
<a id="more"></a>
<p>Query understanding的任务有三个，即纠正用户输入query的错误，精确地引导用户以及准确地理解用户query的目的。Query通常分为两种类型，一种是general query，不需要上下文情境；另外一种是<mark>contextual questions</mark>，比如”Where can I eat cheese cake right now”。第二种query不需要返回一系列的文档，而是需要解析query的目的，用户的地理位置和时间等，提供personal的回答。另外一种分类方法是从答案的角度，一种是<mark>精确需求</mark>，直接给出结果，比如”Who is the president of USA”；第二种则是给出泛泛的回答，让用户自己选择，即<mark>Interactive Question Answering</mark>，如下图所示。</p>
<center><img src="/img/daily/qu1.png" width="100%"></center><br><center>Fig.1 Interactive Question Answering</center>

<h3 id="General-Characteristics"><a href="#General-Characteristics" class="headerlink" title="General Characteristics"></a><center><strong>General Characteristics</strong></center></h3><p>Query understanding还有一个非常有意思的应用是个人助理（比如微软Cortana，苹果Siri，Google Now，百度度秘），可以为用户提供各种服务，比如搜索，会议安排，闹铃设置，打电话，发短信，播放音乐等等。这个可以被称为<code>Deep Search</code>，即Search with a Deep Understanding，应用NLP和knowledge context (用户之前的搜索和浏览活动，兴趣爱好，地点，询问的时间以及当时的天气等等)，通常有如下特征：</p>
<ol>
<li>对query的语义理解。不同语义层次的理解如下图所示，展示了不同level的语义理解。</li>
</ol>
<center><img src="/img/daily/qu2.png" width="90%"></center><br><center>Fig.2 Matching at Different Semantic Levels</center>

<ol>
<li>对上下文和前面任务的理解。这个可以有效地理解有歧义的词汇。Fig.3显示了含有interleaved tasks的一个搜索情景。<code>Reference Query</code>表示的是当前用户的query，<code>On-Task Queries</code>表示跟<code>Reference Query</code>相同任务的query；<code>Off-Task Queries</code>表示的是跟当前query任务不同的query。</li>
</ol>
<center><img src="/img/daily/qu3.png" width="90%"></center><br><center>Fig.3 A Search Context with Interleaved Tasks</center>

<ol>
<li>用户理解和个性化。不同的用户有不同的兴趣爱好等。</li>
</ol>
<h3 id="Phases-in-Deep-Query-Understanding"><a href="#Phases-in-Deep-Query-Understanding" class="headerlink" title="Phases in Deep Query Understanding"></a><center><strong>Phases in Deep Query Understanding</strong></center></h3><p>Query understanding主要有三个过程，如Fig.4所示，包括query refinement, query suggestion和query intent detection，最终给query打tag，有了tag之后进入<code>Answer Generation Module</code>。</p>
<center><img src="/img/daily/qu4.png" width="100%"></center><br><center> Fig.4 Query Understanding Module Broad Components</center>

<p><code>Query refinement</code>主要进行spelling correction, acronym expansion, word splitting, words merger和phrase segmentation。纠正之后的query进入<code>query suggestion</code>来发现和推荐可以得到更好搜索结果的queries。在循环一次或者多次<code>query correction-&gt;query suggestion-&gt;user query</code>这个过程后，完善的query进入到<code>query expansion</code>模块，补充更多近似的queries，以减少由于字符的错误匹配产生的文章结果。这个query set之后输入到<code>query intention detection</code>模块，精确地推测query的意图。<code>Query classification</code>将query粗分类到某一个field里面，比如运动、娱乐、天气等，结果被输入到<code>semantic tagging</code>部分进行意图检测。Fig.5描述了一个例子。</p>
<center><img src="/img/daily/qu5.png" width="100%"></center><br><center>Fig.5 Example of Task Done at Each Component</center>

<p><mark>Query classification比document text classification更具有挑战性，因为query相对较短，而且很多query有歧义性。</mark>Query classification对精确地判断用户意图非常重要。对于semantic tagging，经常用的features是n-grams, regular regression, POS (Part of Speech), lexicons和transit features,<mark>总的来说，分为两种，syntactic和semantic的features</mark>。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a><center><strong>Reference</strong></center></h3><p>[1] <a href="https://arxiv.org/pdf/1505.05187.pdf" target="_blank" rel="external">Techniques for deep query understanding.</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Query understanding是很多AI应用场景的基础，比如机器人对话，车载导航等等。&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
      <category term="QU" scheme="http://cuiyungao.github.io/tags/QU/"/>
    
  </entry>
  
  <entry>
    <title>CNN总结</title>
    <link href="http://cuiyungao.github.io/2016/08/12/cnn/"/>
    <id>http://cuiyungao.github.io/2016/08/12/cnn/</id>
    <published>2016-08-12T03:00:57.000Z</published>
    <updated>2016-08-21T09:25:41.000Z</updated>
    
    <content type="html"><![CDATA[<p>已经有很多大牛对CNN的算法进行了总结，这里就对这些资料进行汇总。</p>
<a id="more"></a>
<p>CNN已经被各种大牛解说得很好了，其中觉得不是很懂的地方主要还是在back propagation。所以这篇文章主要对back propagation方面进行总结，也汇总CNN的一些重要的点。一些很好的资料也在本文进行标注。</p>
<h3 id="Main-Steps"><a href="#Main-Steps" class="headerlink" title="Main Steps"></a><center><strong>Main Steps</strong></center></h3><p>下面这幅图非常经典，是ConvNet的过程图。在CNN中有4个关键操作：</p>
<ul>
<li>Convolution</li>
<li>Non Linearlity (ReLU)</li>
<li>Pooling or Sub Sampling</li>
<li>Classification (Fully Connected Layer).</li>
</ul>
<p>Convolution这一步是一个linear的操作，主要目的是获取输入图片的特征，通过移动<code>filter</code>（也叫<code>kernel</code>，<code>feature detector</code>）保留pixels之间的空间关系。dot product的结果叫做<code>Convolved Feature</code>或者<code>Activation Map</code>或者<code>Feature Map</code>。<code>filter</code>种类的数量表示convolution之后的深度，一种<code>filter</code>得到一种layer。Feature Map的大小由三个参数决定，即Depth, Stride和Zero-Padding。</p>
<center><img src="/img/daily/cnn1.png" width="100%"></center>

<p>ReLU是对Convolution的结果进行非线性变换，是element wise operation，将所有的负值替换成0。因为现实数据通常是非线性的，所以引入ReLU这个非线性函数，也可以采用tanh或者sigmoid，但是ReLU通常情况下性能较好。</p>
<p>Pooling操作主要目的是减少特征维度，同时保留重要信息，避免overfitting，这使得对输入图片的各种变换、扭曲和移动有效。到这一步，我们可以得到输入图片的high-level的feature。最后通过fully connected layer，我们对这些feature进行分类。比如说我们要使得结果分成$k$类，通过softmax这个activation function，使得这$k$类输出概率的和是1。最后通过back propagation，我们更新迭代参数使分类结果最佳。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a><center><strong>Reference</strong></center></h3><p>[1] <a href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/" target="_blank" rel="external">An Intuitive Explanation of Convolutional Neural Networks</a><br>[2] <a href="http://andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/" target="_blank" rel="external">Backpropagation</a><br>[3] <a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="external">CS231n Convolutional Neural Networks Stanford</a><br>[4] <a href="http://blog.algorithmia.com/introduction-natural-language-processing-nlp/" target="_blank" rel="external">Introduction to Natural Language Processing 2016</a>。这里面包含了各种资料的汇总。<br>[5] <a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/" target="_blank" rel="external">Understanding Convolutions</a>。从概率的角度来理解CNN.<br>[6] <a href="http://arxiv.org/pdf/1512.07108v3.pdf" target="_blank" rel="external">Recent Advances in Convolutional Neural Networks</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;已经有很多大牛对CNN的算法进行了总结，这里就对这些资料进行汇总。&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
      <category term="CNN" scheme="http://cuiyungao.github.io/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>SVM模型小结</title>
    <link href="http://cuiyungao.github.io/2016/08/11/svmbaidu/"/>
    <id>http://cuiyungao.github.io/2016/08/11/svmbaidu/</id>
    <published>2016-08-11T12:06:31.000Z</published>
    <updated>2016-08-12T02:00:27.000Z</updated>
    
    <content type="html"><![CDATA[<p>在百度每周的NLP课程总结，感谢磊磊哥大神。这节课主要讲的是SVM算法，讲到了其中的精髓，感觉受益匪浅，还是记下来以备复习。</p>
<a id="more"></a>
<p>SVM (Support Vector Machine)是机器学习的基础，虽然基本，但是当中延伸出来一些很有意义的idea，比如kernel。</p>
<h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a><center><strong>SVM</strong></center></h3><p>如图所示，假定在$x$空间，存在两分类的点集，标记实心的点为$x^+$，空心的点为$x^-$，$w$为与$H_3$垂直的向量，则$H_3$表示为$w^Tx+b$。<mark>注意：$w$是与分界线垂直的向量。</mark>实心和空心区域的点集可以分别表示为：<br>$$<br>w^Tx^++b\geq 1 \\<br>w^Tx^-+b\leq -1.<br>$$<br>这里的1代表不确定常量（常量在不等式左右两边同除，结果为1）。</p>
<center><img src="/img/daily/svm1.png" width="60%"></center>

<p>现在我们想找到SVM的目标函数。我们的主要目的是maximize两个集合之间的距离，所以取在边界上的两个集合中的点，即$x^+$和$x^-$（<mark>注意：边界上的点即为support vector</mark>），则主要目的转变成maximize这两个点的距离，公式如下：<br>$$<br>\begin{cases}<br>w^Tx^+ + b = 1 \\<br>w^Tx^- + b = -1 \\<br>x^+ - x^- = \lambda.<br>\end{cases}<br>$$<br>在第三个式子左右两边同乘$w^T$，得到$\lambda = \frac{2}{w^Tw}$。所以，$||x^+ - x^-||=\frac{2}{w^Tw}\cdot\sqrt{w^Tw}=\frac{2}{\sqrt{w^Tw}}$，我们想要maximize这个margin值，即minimize这个margin值的倒数。即目标函数为：<br>$$<br>min \; \frac{1}{2}w^Tw \\<br>s.t. \; y_i(w^Tx_i+b) \geq 1, \; \forall i.<br>$$</p>
<p>其中$i$表示所有样本。这是<mark>primal</mark>的表示形式，我们现在用拉格朗日求其<mark>dual</mark>，则有：<br>$$<br>L(w,b,\lambda) = \frac{1}{2}w^Tw+\sum_i \lambda_i [1-y_i(w^Tx_i+b)], \\<br>s.t. \; \sum_i\lambda_i \geq 0.<br>$$<br>所以有，$\frac{\partial L}{\partial w} = w-\sum_i \lambda_iy_ix_i=0$，得到$w=\sum_i\lambda_iy_ix_i$。同理，我们对$b$进行求导，有$\frac{\partial L}{\partial b} = \sum_i\lambda_iy_i=0$。所以化简上面的公式，我们有：<br>$$<br>H({\lambda_i}) = -\frac{1}{2}(\sum_i\lambda_iy_ix_i)^T(\sum_j\lambda_jy_jx_j) + \sum_i\lambda_i, \\<br>s.t. \; \sum_i\lambda_i\geq 0.<br>$$</p>
<p>我们把这个式子写成向量形式，即$H=-\frac{1}{2}\overrightarrow{\lambda}^TK\overrightarrow{\lambda}+\overrightarrow{1}\lambda$，其中$K=(x_iy_i)^T(x_jy_j)$。<mark>注意：由定义可以看出，$\lambda$在不是边界上的点是为0，所以只有边界上的点定义了$w$。</mark>这里就引出来一个非常重要的概念<mark>kernel</mark>，什么样的矩阵可以写成kernel形式呢？<mark><a href="https://en.wikipedia.org/wiki/Mercer%27s_theorem" target="_blank" rel="external">Mercer’s theorem</a></mark>对其进行了阐述。简单来说，就是一个对称的半正定的矩阵就可以写成kernel的表示形式，即symetric, PSD matrix $\implies$ kernel ($x\to\phi(x)$)。这里的kernel有三种形式：</p>
<ul>
<li>linear</li>
<li>polynomial</li>
<li>RBF (类似Gaussian).</li>
</ul>
<p>最后一个RBF相当于映射到无穷维的空间$\phi(x)$，表示形式是$K(x_i, x_j)=\lambda exp(-\frac{||x_i-x_j||_2^2}{\sigma^2})$。总的kernel形式可以表示为$K(i,j)=\phi(x_i)^T\phi(x_j)$。</p>
<p>对于SVM，我们可以将目标函数引入一个slack variable $\zeta_i$，则目标函数为：<br>$$<br>min \; \frac{1}{2}w^Tw+c\sum_i\zeta_i \\<br>s.t. \; y_i(w^Tx_i+b)\geq 1-\zeta_i, \; \forall_i \zeta_i\geq 0.<br>$$</p>
<h3 id="SVR"><a href="#SVR" class="headerlink" title="SVR"></a><center><strong>SVR</strong></center></h3><p>SVR即support vector regression，目的跟regression一样，即用一条线拟合一堆点。其目标函数为：<br>$$<br>min \; \frac{1}{2}w^Tw \\<br>s.t. \; w^Tx_i + b -y \leq \epsilon, \; w^Tx_i+b-y\geq -\epsilon.<br>$$<br>推导过程同SVM。</p>
<h3 id="SVC"><a href="#SVC" class="headerlink" title="SVC"></a><center><strong>SVC</strong></center></h3><p>SVC即support vector clustering，目的是将点集聚类。它将点的类想象成不同的小山谷，分类即找出山谷的分布$P(x)$，投影到高维空间后，点集分布在一个半径为$R$的球内，球外的点即outliers，点集这时可以用一个高斯分布来描述。</p>
<p><center><img src="/img/daily/svm2.png"></center></p>
<p><center><img src="/img/daily/svm3.png"></center><br>目标函数为：<br>$$<br>min \; R^2 \\<br>s.t. \; ||\phi(x_i)-\mu||_2^2 \leq R^2.<br>$$<br>其中，$\mu=\frac{1}{N}\sum_i\phi(x_i)$。要注意两点：</p>
<ol>
<li>$dist(\hat{x}, \mu)$可以写成kernel的形式，即$\phi(x_i)^T\phi(x_i)$;</li>
<li>$R$的设置可以进行分类。</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在百度每周的NLP课程总结，感谢磊磊哥大神。这节课主要讲的是SVM算法，讲到了其中的精髓，感觉受益匪浅，还是记下来以备复习。&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="SVM" scheme="http://cuiyungao.github.io/tags/SVM/"/>
    
  </entry>
  
</feed>
