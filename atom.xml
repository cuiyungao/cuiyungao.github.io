<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Cuiyun Gao&#39;s Daily Digest</title>
  <subtitle>Work Hard, and Play Harder. 如果有一天：你不再寻找爱情，只是去爱；你不再渴望成功，只是去做；你不再追求成长，只是去修行；一切才真正开始~！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://cuiyungao.github.io/"/>
  <updated>2016-08-14T08:22:00.000Z</updated>
  <id>http://cuiyungao.github.io/</id>
  
  <author>
    <name>Cuiyun Gao</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>query</title>
    <link href="http://cuiyungao.github.io/2016/08/13/query/"/>
    <id>http://cuiyungao.github.io/2016/08/13/query/</id>
    <published>2016-08-13T02:36:37.000Z</published>
    <updated>2016-08-14T08:22:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>Query understanding是很多AI应用场景的基础，比如机器人对话，车载导航等等。</p>
<a id="more"></a>
<p>Query understanding的任务有三个，即纠正用户输入query的错误，精确地引导用户以及准确地理解用户query的目的。Query通常分为两种类型，一种是general query，不需要上下文情境；另外一种是<mark>contextual questions</mark>，比如”Where can I eat cheese cake right now”。第二种query不需要返回一系列的文档，而是需要解析query的目的，用户的地理位置和时间等，提供personal的回答。另外一种分类方法是从答案的角度，一种是<mark>精确需求</mark>，直接给出结果，比如”Who is the president of USA”；第二种则是给出泛泛的回答，让用户自己选择，即<mark>Interactive Question Answering</mark>，如下图所示。</p>
<center><img src="/img/daily/qu1.png" width="90%"></center><br><center>Fig.1 Interactive Question Answering</center>

<h3 id="General-Characteristics"><a href="#General-Characteristics" class="headerlink" title="General Characteristics"></a><center><strong>General Characteristics</strong></center></h3><p>Query understanding还有一个非常有意思的应用是个人助理（比如微软Cortana，苹果Siri，Google Now，百度度秘），可以为用户提供各种服务，比如搜索，会议安排，闹铃设置，打电话，发短信，播放音乐等等。这个可以被称为<code>Deep Search</code>，即Search with a Deep Understanding，应用NLP和knowledge context (用户之前的搜索和浏览活动，兴趣爱好，地点，询问的时间以及当时的天气等等)，通常有如下特征：</p>
<ol>
<li>对query的语义理解。不同语义层次的理解如下图所示，展示了不同level的语义理解。</li>
</ol>
<center><img src="/img/daily/qu2.png" width="90%"></center><br><center>Fig.2 Matching at Different Semantic Levels</center>

<ol>
<li>对上下文和前面任务的理解。这个可以有效地理解有歧义的词汇。Fig.3显示了含有interleaved tasks的一个搜索情景。<code>Reference Query</code>表示的是当前用户的query，<code>On-Task Queries</code>表示跟<code>Reference Query</code>相同任务的query；<code>Off-Task Queries</code>表示的是跟当前query任务不同的query。</li>
</ol>
<center><img src="/img/daily/qu3.png" width="90%"></center><br><center>Fig.3 A Search Context with Interleaved Tasks</center>

<ol>
<li>用户理解和个性化。不同的用户有不同的兴趣爱好等。</li>
</ol>
<h3 id="Phases-in-Deep-Query-Understanding"><a href="#Phases-in-Deep-Query-Understanding" class="headerlink" title="Phases in Deep Query Understanding"></a><center><strong>Phases in Deep Query Understanding</strong></center></h3><p>Query understanding主要有三个过程，如Fig.4所示，包括query refinement, query suggestion和query intent detection，最终给query打tag，有了tag之后进入<code>Answer Generation Module</code>。</p>
<center><img src="/img/daily/qu4.png" width="90%"></center><br><center> Fig.4 Query Understanding Module Broad Components</center>

<p><code>Query refinement</code>主要进行spelling correction, acronym expansion, word splitting, words merger和phrase segmentation。纠正之后的query进入<code>query suggestion</code>来发现和推荐可以得到更好搜索结果的queries。在循环一次或者多次<code>query correction-&gt;query suggestion-&gt;user query</code>这个过程后，完善的query进入到<code>query expansion</code>模块，补充更多近似的queries，以减少由于字符的错误匹配产生的文章结果。这个query set之后输入到<code>query intention detection</code>模块，精确地推测query的意图。<code>Query classification</code>将query粗分类到某一个field里面，比如运动、娱乐、天气等，结果被输入到<code>semantic tagging</code>部分进行意图检测。Fig.5描述了一个例子。</p>
<center><img src="/img/daily/qu5.png" width="90%"></center><br><center>Fig.5 Example of Task Done at Each Component</center>

<p><mark>Query classification比document text classification更具有挑战性，因为query相对较短，而且很多query有歧义性。</mark>Query classification对精确地判断用户意图非常重要。对于semantic tagging，经常用的features是n-grams, regular regression, POS (Part of Speech), lexicons和transit features,<mark>总的来说，分为两种，syntactic和semantic的features</mark>。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a><center><strong>Reference</strong></center></h3><p>[1] <a href="https://arxiv.org/pdf/1505.05187.pdf" target="_blank" rel="external">Techniques for deep query understanding.</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Query understanding是很多AI应用场景的基础，比如机器人对话，车载导航等等。&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
      <category term="QU" scheme="http://cuiyungao.github.io/tags/QU/"/>
    
  </entry>
  
  <entry>
    <title>CNN总结</title>
    <link href="http://cuiyungao.github.io/2016/08/12/cnn/"/>
    <id>http://cuiyungao.github.io/2016/08/12/cnn/</id>
    <published>2016-08-12T03:00:57.000Z</published>
    <updated>2016-08-12T03:29:50.000Z</updated>
    
    <content type="html"><![CDATA[<p>已经有很多大牛对CNN的算法进行了总结，这里就对这些资料进行汇总。</p>
<a id="more"></a>
<p>CNN已经被各种大牛解说得很好了，其中觉得不是很懂的地方主要还是在back propagation。所以这篇文章主要对back propagation方面进行总结，也汇总CNN的一些重要的点。一些很好的资料也在本文进行标注。</p>
<h3 id="Main-Steps"><a href="#Main-Steps" class="headerlink" title="Main Steps"></a><center><strong>Main Steps</strong></center></h3><p>下面这幅图非常经典，是ConvNet的过程图。在CNN中有4个关键操作：</p>
<ul>
<li>Convolution</li>
<li>Non Linearlity (ReLU)</li>
<li>Pooling or Sub Sampling</li>
<li>Classification (Fully Connected Layer).</li>
</ul>
<p>Convolution这一步是一个linear的操作，主要目的是获取输入图片的特征，通过移动<code>filter</code>（也叫<code>kernel</code>，<code>feature detector</code>）保留pixels之间的空间关系。dot product的结果叫做<code>Convolved Feature</code>或者<code>Activation Map</code>或者<code>Feature Map</code>。<code>filter</code>种类的数量表示convolution之后的深度，一种<code>filter</code>得到一种layer。Feature Map的大小由三个参数决定，即Depth, Stride和Zero-Padding。</p>
<center><img src="/img/daily/cnn1.png" width="100%"></center>

<p>ReLU是对Convolution的结果进行非线性变换，是element wise operation，将所有的负值替换成0。因为现实数据通常是非线性的，所以引入ReLU这个非线性函数，也可以采用tanh或者sigmoid，但是ReLU通常情况下性能较好。</p>
<p>Pooling操作主要目的是减少特征维度，同时保留重要信息，避免overfitting，这使得对输入图片的各种变换、扭曲和移动有效。到这一步，我们可以得到输入图片的high-level的feature。最后通过fully connected layer，我们对这些feature进行分类。比如说我们要使得结果分成$k$类，通过softmax这个activation function，使得这$k$类输出概率的和是1。最后通过back propagation，我们更新迭代参数使分类结果最佳。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a><center><strong>Reference</strong></center></h3><p>[1] <a href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/" target="_blank" rel="external">An Intuitive Explanation of Convolutional Neural Networks</a><br>[2] <a href="http://andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/" target="_blank" rel="external">Backpropagation</a><br>[3] <a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="external">CS231n Convolutional Neural Networks Stanford</a><br>[4] <a href="http://blog.algorithmia.com/introduction-natural-language-processing-nlp/" target="_blank" rel="external">Introduction to Natural Language Processing 2016</a>。这里面包含了各种资料的汇总。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;已经有很多大牛对CNN的算法进行了总结，这里就对这些资料进行汇总。&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
      <category term="CNN" scheme="http://cuiyungao.github.io/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>SVM模型小结</title>
    <link href="http://cuiyungao.github.io/2016/08/11/svmbaidu/"/>
    <id>http://cuiyungao.github.io/2016/08/11/svmbaidu/</id>
    <published>2016-08-11T12:06:31.000Z</published>
    <updated>2016-08-12T02:00:27.000Z</updated>
    
    <content type="html"><![CDATA[<p>在百度每周的NLP课程总结，感谢磊磊哥大神。这节课主要讲的是SVM算法，讲到了其中的精髓，感觉受益匪浅，还是记下来以备复习。</p>
<a id="more"></a>
<p>SVM (Support Vector Machine)是机器学习的基础，虽然基本，但是当中延伸出来一些很有意义的idea，比如kernel。</p>
<h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a><center><strong>SVM</strong></center></h3><p>如图所示，假定在$x$空间，存在两分类的点集，标记实心的点为$x^+$，空心的点为$x^-$，$w$为与$H_3$垂直的向量，则$H_3$表示为$w^Tx+b$。<mark>注意：$w$是与分界线垂直的向量。</mark>实心和空心区域的点集可以分别表示为：<br>$$<br>w^Tx^++b\geq 1 \\<br>w^Tx^-+b\leq -1.<br>$$<br>这里的1代表不确定常量（常量在不等式左右两边同除，结果为1）。</p>
<center><img src="/img/daily/svm1.png" width="60%"></center>

<p>现在我们想找到SVM的目标函数。我们的主要目的是maximize两个集合之间的距离，所以取在边界上的两个集合中的点，即$x^+$和$x^-$（<mark>注意：边界上的点即为support vector</mark>），则主要目的转变成maximize这两个点的距离，公式如下：<br>$$<br>\begin{cases}<br>w^Tx^+ + b = 1 \\<br>w^Tx^- + b = -1 \\<br>x^+ - x^- = \lambda.<br>\end{cases}<br>$$<br>在第三个式子左右两边同乘$w^T$，得到$\lambda = \frac{2}{w^Tw}$。所以，$||x^+ - x^-||=\frac{2}{w^Tw}\cdot\sqrt{w^Tw}=\frac{2}{\sqrt{w^Tw}}$，我们想要maximize这个margin值，即minimize这个margin值的倒数。即目标函数为：<br>$$<br>min \; \frac{1}{2}w^Tw \\<br>s.t. \; y_i(w^Tx_i+b) \geq 1, \; \forall i.<br>$$</p>
<p>其中$i$表示所有样本。这是<mark>primal</mark>的表示形式，我们现在用拉格朗日求其<mark>dual</mark>，则有：<br>$$<br>L(w,b,\lambda) = \frac{1}{2}w^Tw+\sum_i \lambda_i [1-y_i(w^Tx_i+b)], \\<br>s.t. \; \sum_i\lambda_i \geq 0.<br>$$<br>所以有，$\frac{\partial L}{\partial w} = w-\sum_i \lambda_iy_ix_i=0$，得到$w=\sum_i\lambda_iy_ix_i$。同理，我们对$b$进行求导，有$\frac{\partial L}{\partial b} = \sum_i\lambda_iy_i=0$。所以化简上面的公式，我们有：<br>$$<br>H({\lambda_i}) = -\frac{1}{2}(\sum_i\lambda_iy_ix_i)^T(\sum_j\lambda_jy_jx_j) + \sum_i\lambda_i, \\<br>s.t. \; \sum_i\lambda_i\geq 0.<br>$$</p>
<p>我们把这个式子写成向量形式，即$H=-\frac{1}{2}\overrightarrow{\lambda}^TK\overrightarrow{\lambda}+\overrightarrow{1}\lambda$，其中$K=(x_iy_i)^T(x_jy_j)$。<mark>注意：由定义可以看出，$\lambda$在不是边界上的点是为0，所以只有边界上的点定义了$w$。</mark>这里就引出来一个非常重要的概念<mark>kernel</mark>，什么样的矩阵可以写成kernel形式呢？<mark><a href="https://en.wikipedia.org/wiki/Mercer%27s_theorem" target="_blank" rel="external">Mercer’s theorem</a></mark>对其进行了阐述。简单来说，就是一个对称的半正定的矩阵就可以写成kernel的表示形式，即symetric, PSD matrix $\implies$ kernel ($x\to\phi(x)$)。这里的kernel有三种形式：</p>
<ul>
<li>linear</li>
<li>polynomial</li>
<li>RBF (类似Gaussian).</li>
</ul>
<p>最后一个RBF相当于映射到无穷维的空间$\phi(x)$，表示形式是$K(x_i, x_j)=\lambda exp(-\frac{||x_i-x_j||_2^2}{\sigma^2})$。总的kernel形式可以表示为$K(i,j)=\phi(x_i)^T\phi(x_j)$。</p>
<p>对于SVM，我们可以将目标函数引入一个slack variable $\zeta_i$，则目标函数为：<br>$$<br>min \; \frac{1}{2}w^Tw+c\sum_i\zeta_i \\<br>s.t. \; y_i(w^Tx_i+b)\geq 1-\zeta_i, \; \forall_i \zeta_i\geq 0.<br>$$</p>
<h3 id="SVR"><a href="#SVR" class="headerlink" title="SVR"></a><center><strong>SVR</strong></center></h3><p>SVR即support vector regression，目的跟regression一样，即用一条线拟合一堆点。其目标函数为：<br>$$<br>min \; \frac{1}{2}w^Tw \\<br>s.t. \; w^Tx_i + b -y \leq \epsilon, \; w^Tx_i+b-y\geq -\epsilon.<br>$$<br>推导过程同SVM。</p>
<h3 id="SVC"><a href="#SVC" class="headerlink" title="SVC"></a><center><strong>SVC</strong></center></h3><p>SVC即support vector clustering，目的是将点集聚类。它将点的类想象成不同的小山谷，分类即找出山谷的分布$P(x)$，投影到高维空间后，点集分布在一个半径为$R$的球内，球外的点即outliers，点集这时可以用一个高斯分布来描述。</p>
<p><center><img src="/img/daily/svm2.png"></center></p>
<p><center><img src="/img/daily/svm3.png"></center><br>目标函数为：<br>$$<br>min \; R^2 \\<br>s.t. \; ||\phi(x_i)-\mu||_2^2 \leq R^2.<br>$$<br>其中，$\mu=\frac{1}{N}\sum_i\phi(x_i)$。要注意两点：</p>
<ol>
<li>$dist(\hat{x}, \mu)$可以写成kernel的形式，即$\phi(x_i)^T\phi(x_i)$;</li>
<li>$R$的设置可以进行分类。</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在百度每周的NLP课程总结，感谢磊磊哥大神。这节课主要讲的是SVM算法，讲到了其中的精髓，感觉受益匪浅，还是记下来以备复习。&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="SVM" scheme="http://cuiyungao.github.io/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>fastText与Word2Vec之间的比较</title>
    <link href="http://cuiyungao.github.io/2016/08/09/fastvsword/"/>
    <id>http://cuiyungao.github.io/2016/08/09/fastvsword/</id>
    <published>2016-08-09T01:29:03.000Z</published>
    <updated>2016-08-09T02:08:42.000Z</updated>
    
    <content type="html"><![CDATA[<p>本篇文章总结了试验fastText跟Word2Vec在embedding上的performance，源于<a href="http://nbviewer.jupyter.org/github/jayantj/gensim/blob/683720515165a332baed8a2a46b6711cefd2d739/docs/notebooks/Word2Vec_FastText_Comparison.ipynb" target="_blank" rel="external">这篇文章</a>。</p>
<a id="more"></a>
<h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a><center><strong>Dataset</strong></center></h3><p>训练embedding的数据集有两个，一个是<code>text8</code> corpus，一个是nltk自带的<code>brown</code> corpus。Groundtruth是<code>questtions-words</code>文本，可以从<a href="https://raw.githubusercontent.com/arfon/word2vec/master/questions-words.txt" target="_blank" rel="external">这里</a>下载。</p>
<h4 id="text8-Corpus-Download"><a href="#text8-Corpus-Download" class="headerlink" title="text8 Corpus Download"></a>text8 Corpus Download</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">wget http://mattmahoney.net/dc/text8.zip</div></pre></td></tr></table></figure>
<h4 id="brown-Corpus-Download"><a href="#brown-Corpus-Download" class="headerlink" title="brown Corpus Download"></a>brown Corpus Download</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">import nltk</div><div class="line"># 从当中选择brown corpus进行下载</div><div class="line">nltk.download()</div><div class="line"></div><div class="line"># Generate brown corpus text file</div><div class="line">with open(&apos;brown_corp.txt&apos;, &apos;w+&apos;) as f:</div><div class="line">    for word in nltk.corpus.brown.words():</div><div class="line">        f.write(&apos;&#123;word&#125; &apos;.format(word=word))</div></pre></td></tr></table></figure>
<h3 id="Model-Training"><a href="#Model-Training" class="headerlink" title="Model Training"></a><center><strong>Model Training</strong></center></h3><p>用fastText和Word2Vec分别对上述两个数据集进行训练，得到word embeddings。</p>
<h4 id="fastText-Training"><a href="#fastText-Training" class="headerlink" title="fastText Training"></a>fastText Training</h4><p>下载<a href="https://github.com/facebookresearch/fastText" target="_blank" rel="external">fastText</a>源码，对上述两个数据集进行训练。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">./fasttext skipgram -input brown_corp.txt -output brown_ft</div><div class="line">./fasttext skipgram -input text8 -output text8_ft</div></pre></td></tr></table></figure></p>
<h4 id="Word2Vec-Training"><a href="#Word2Vec-Training" class="headerlink" title="Word2Vec Training"></a>Word2Vec Training</h4><p>Word2Vec的训练基于gensim，采用<code>logging</code>来对过程进行输出。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">from nltk.corpus import brown</div><div class="line">from gensim.models import Word2Vec</div><div class="line">from gensim.models.word2vec import Text8Corpus</div><div class="line">import logging</div><div class="line"></div><div class="line">logging.basicConfig(format=&apos;%(asctime)s : %(levelname)s : %(message)s&apos;)</div><div class="line">logging.root.setLevel(level=logging.INFO)</div><div class="line"></div><div class="line">MODELS_DIR = &apos;models/&apos;</div><div class="line"></div><div class="line">brown_gs = Word2Vec(brown.sents())</div><div class="line">brown_gs.save_word2vec_format(MODELS_DIR + &apos;brown_gs.vec&apos;)</div><div class="line"></div><div class="line">text8_gs = Word2Vec(Text8Corpus(&apos;text8&apos;))</div><div class="line">text8_gs.save_word2vec_format(MODELS_DIR + &apos;text8_gs.vec&apos;)</div></pre></td></tr></table></figure></p>
<h3 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a><center><strong>Comparison</strong></center></h3><p>用<code>questions-words.txt</code>提供的数据作为Groundtruth，从<code>semantic</code>和<code>syntactic</code>两方面来对两种embedding的方法进行比较。</p>
<h4 id="Based-on-Brown-Corpus"><a href="#Based-on-Brown-Corpus" class="headerlink" title="Based on Brown Corpus"></a>Based on Brown Corpus</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">from gensim.models import Word2Vec</div><div class="line"></div><div class="line">def print_accuracy(model, questions_file):</div><div class="line">    print(&apos;Evaluating...\n&apos;)</div><div class="line">    acc = model.accuracy(questions_file)</div><div class="line">    for section in acc:</div><div class="line">        correct = len(section[&apos;correct&apos;])</div><div class="line">        total = len(section[&apos;correct&apos;]) + len(section[&apos;incorrect&apos;])</div><div class="line">        total = total if total else 1</div><div class="line">        accuracy = 100*float(correct)/total</div><div class="line">        print(&apos;&#123;:d&#125;/&#123;:d&#125;, &#123;:.2f&#125;%, Section: &#123;:s&#125;&apos;.format(correct, total, accuracy, section[&apos;section&apos;]))</div><div class="line">    sem_correct = sum((len(acc[i][&apos;correct&apos;]) for i in range(5)))</div><div class="line">    sem_total = sum((len(acc[i][&apos;correct&apos;]) + len(acc[i][&apos;incorrect&apos;])) for i in range(5))</div><div class="line">    print(&apos;\nSemantic: &#123;:d&#125;/&#123;:d&#125;, Accuracy: &#123;:.2f&#125;%&apos;.format(sem_correct, sem_total, 100*float(sem_correct)/sem_total))</div><div class="line"></div><div class="line">    syn_correct = sum((len(acc[i][&apos;correct&apos;]) for i in range(5, len(acc)-1)))</div><div class="line">    syn_total = sum((len(acc[i][&apos;correct&apos;]) + len(acc[i][&apos;incorrect&apos;])) for i in range(5,len(acc)-1))</div><div class="line">    print(&apos;Syntactic: &#123;:d&#125;/&#123;:d&#125;, Accuracy: &#123;:.2f&#125;%\n&apos;.format(syn_correct, syn_total, 100*float(syn_correct)/syn_total))</div><div class="line"></div><div class="line">MODELS_DIR = &apos;models/&apos;</div><div class="line"></div><div class="line">word_analogies_file = &apos;questions-words.txt&apos;</div><div class="line">print(&apos;\nLoading FastText embeddings&apos;)</div><div class="line">ft_model = Word2Vec.load_word2vec_format(MODELS_DIR + &apos;brown_ft.vec&apos;)</div><div class="line">print(&apos;Accuracy for FastText:&apos;)</div><div class="line">print_accuracy(ft_model, word_analogies_file)</div><div class="line"></div><div class="line">print(&apos;\nLoading Gensim embeddings&apos;)</div><div class="line">gs_model = Word2Vec.load_word2vec_format(MODELS_DIR + &apos;brown_gs.vec&apos;)</div><div class="line">print(&apos;Accuracy for word2vec:&apos;)</div><div class="line">print_accuracy(gs_model, word_analogies_file)</div></pre></td></tr></table></figure>
<p>结果如下：</p>
<blockquote><br>Loading FastText embeddings<br>Accuracy for FastText:<br>Evaluating…<br><br>0/1, 0.00%, Section: capital-common-countries<br>0/1, 0.00%, Section: capital-world<br>0/1, 0.00%, Section: currency<br>0/1, 0.00%, Section: city-in-state<br>36/182, 19.78%, Section: family<br>498/702, 70.94%, Section: gram1-adjective-to-adverb<br>110/132, 83.33%, Section: gram2-opposite<br>675/1056, 63.92%, Section: gram3-comparative<br>140/210, 66.67%, Section: gram4-superlative<br>426/650, 65.54%, Section: gram5-present-participle<br>0/1, 0.00%, Section: gram6-nationality-adjective<br>153/1260, 12.14%, Section: gram7-past-tense<br>318/552, 57.61%, Section: gram8-plural<br>245/342, 71.64%, Section: gram9-plural-verbs<br>2601/5086, 51.14%, Section: total<br><br>Semantic: 36/182, Accuracy: <mark>19.78%</mark><br>Syntactic: 2565/4904, Accuracy: <mark>52.30%</mark><br><br><br>Loading Gensim embeddings<br>Accuracy for word2vec:<br>Evaluating…<br><br>0/1, 0.00%, Section: capital-common-countries<br>0/1, 0.00%, Section: capital-world<br>0/1, 0.00%, Section: currency<br>0/1, 0.00%, Section: city-in-state<br>54/182, 29.67%, Section: family<br>8/702, 1.14%, Section: gram1-adjective-to-adverb<br>0/132, 0.00%, Section: gram2-opposite<br>72/1056, 6.82%, Section: gram3-comparative<br>0/210, 0.00%, Section: gram4-superlative<br>14/650, 2.15%, Section: gram5-present-participle<br>0/1, 0.00%, Section: gram6-nationality-adjective<br>28/1260, 2.22%, Section: gram7-past-tense<br>4/552, 0.72%, Section: gram8-plural<br>8/342, 2.34%, Section: gram9-plural-verbs<br>188/5086, 3.70%, Section: total<br><br>Semantic: 54/182, Accuracy: <mark>29.67%</mark><br>Syntactic: 134/4904, Accuracy: <mark>2.73%</mark><br></blockquote>

<p>从运行结果可以看到，fastText的semantic accuracy比Word2Vec要稍微差一点儿，但是Syntactic accuracy的效果明显优于Word2Vec。这是因为<a href="https://arxiv.org/abs/1607.04606" target="_blank" rel="external">1</a>中提到，fastText中word embeddings是由他们的n-gram embeddings来表示，所以形态上相似的词的embeddings也会比较类似。比如：<br>$$<br>embedding(amazing)-embedding(amazingly) = embedding(calm)-embedding(calmly).<br>$$</p>
<h4 id="Based-on-text8-Corpus"><a href="#Based-on-text8-Corpus" class="headerlink" title="Based on text8 Corpus"></a>Based on text8 Corpus</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">print(&apos;Loading FastText embeddings&apos;)</div><div class="line">ft_model = Word2Vec.load_word2vec_format(MODELS_DIR + &apos;text8_ft.vec&apos;)</div><div class="line">print(&apos;Accuracy for FastText:&apos;)</div><div class="line">print_accuracy(ft_model, word_analogies_file)</div><div class="line"></div><div class="line">print(&apos;Loading Gensim embeddings&apos;)</div><div class="line">gs_model = Word2Vec.load_word2vec_format(MODELS_DIR + &apos;text8_gs.vec&apos;)</div><div class="line">print(&apos;Accuracy for word2vec:&apos;)</div><div class="line">print_accuracy(gs_model, word_analogies_file)</div></pre></td></tr></table></figure>
<p>结果如下：</p>
<blockquote><br>Loading FastText embeddings<br>Accuracy for FastText:<br>Evaluating…<br><br>322/506, 63.64%, Section: capital-common-countries<br>609/1452, 41.94%, Section: capital-world<br>36/268, 13.43%, Section: currency<br>286/1520, 18.82%, Section: city-in-state<br>134/306, 43.79%, Section: family<br>556/756, 73.54%, Section: gram1-adjective-to-adverb<br>186/306, 60.78%, Section: gram2-opposite<br>838/1260, 66.51%, Section: gram3-comparative<br>270/506, 53.36%, Section: gram4-superlative<br>556/992, 56.05%, Section: gram5-present-participle<br>1293/1371, 94.31%, Section: gram6-nationality-adjective<br>490/1332, 36.79%, Section: gram7-past-tense<br>888/992, 89.52%, Section: gram8-plural<br>365/650, 56.15%, Section: gram9-plural-verbs<br>6829/12217, 55.90%, Section: total<br><br>Semantic: 1387/4052, Accuracy: <mark>34.23%</mark><br>Syntactic: 5442/8165, Accuracy: <mark>66.65%</mark><br><br>Loading Gensim embeddings<br>Accuracy for word2vec:<br>Evaluating…<br><br>153/506, 30.24%, Section: capital-common-countries<br>248/1452, 17.08%, Section: capital-world<br>27/268, 10.07%, Section: currency<br>172/1571, 10.95%, Section: city-in-state<br>218/306, 71.24%, Section: family<br>88/756, 11.64%, Section: gram1-adjective-to-adverb<br>45/306, 14.71%, Section: gram2-opposite<br>716/1260, 56.83%, Section: gram3-comparative<br>179/506, 35.38%, Section: gram4-superlative<br>325/992, 32.76%, Section: gram5-present-participle<br>702/1371, 51.20%, Section: gram6-nationality-adjective<br>343/1332, 25.75%, Section: gram7-past-tense<br>401/992, 40.42%, Section: gram8-plural<br>219/650, 33.69%, Section: gram9-plural-verbs<br>3836/12268, 31.27%, Section: total<br><br>Semantic: 818/4103, Accuracy: <mark>19.94%</mark><br>Syntactic: 3018/8165, Accuracy: <mark>36.96%</mark><br></blockquote>

<p>实验结果可以看出，用在较大的数据集上，fastText的优势表现得更加明显，当然word2vec的Syntactic accuracy提高得也比较明显。所以总的来看，<mark>fastText比word2vec在word embedding上更好，特别是对于syntactic information。</mark></p>
<h4 id="实验中用到的Hyperparameters"><a href="#实验中用到的Hyperparameters" class="headerlink" title="实验中用到的Hyperparameters"></a>实验中用到的Hyperparameters</h4><p>Gensim word2vec和fastText用了相似的参数，dim_size = 100, window_size = 5, num_epochs = 5。但是它们的模型完全不同，尽管有很多相似性。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a><center><strong>Reference</strong></center></h3><p>[1]<a href="https://arxiv.org/abs/1607.04606" target="_blank" rel="external">Enriching Word Vectors with Subword Information</a><br>[2]<a href="https://arxiv.org/pdf/1301.3781v3.pdf" target="_blank" rel="external">Efficient Estimation of Word Representations in Vector Space</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇文章总结了试验fastText跟Word2Vec在embedding上的performance，源于&lt;a href=&quot;http://nbviewer.jupyter.org/github/jayantj/gensim/blob/683720515165a332baed8a2a46b6711cefd2d739/docs/notebooks/Word2Vec_FastText_Comparison.ipynb&quot;&gt;这篇文章&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
      <category term="Word Embedding" scheme="http://cuiyungao.github.io/tags/Word-Embedding/"/>
    
      <category term="fastText" scheme="http://cuiyungao.github.io/tags/fastText/"/>
    
  </entry>
  
  <entry>
    <title>用户在线广告点击行为的预测</title>
    <link href="http://cuiyungao.github.io/2016/08/04/adclick/"/>
    <id>http://cuiyungao.github.io/2016/08/04/adclick/</id>
    <published>2016-08-04T05:46:40.000Z</published>
    <updated>2016-08-11T02:32:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>本篇是对ad点击行为进行预测的总结，包括张伟楠在携程技术中心的一个<a href="http://mp.weixin.qq.com/s?__biz=MzAwNDI4ODcxNA==&amp;mid=2652243946&amp;idx=2&amp;sn=de8bb9a36fe9f2f0c97f30aaec87063f&amp;scene=2&amp;srcid=0727O4JXBdr5NU7sFOD48J69&amp;from=timeline&amp;isappinstalled=0#wechat_redirect" target="_blank" rel="external">演讲</a>以及北大硕士Kintocai的<a href="http://www.52cs.org/?p=1046" target="_blank" rel="external">总结</a>。集中于深度学习在Multi-field Categorical这类数据集上的应用。</p>
<a id="more"></a>
<p>总结了FM和FNN算法在处理多值分类数据方面的优势，并把这两种算法与神经网络在特征变量处理方面的差异做了对比，最后通过用户在线广告点击行为预测比较了LR、FM、FNN、CCPM、PNN-I等算法的效果。</p>
<h3 id="Multi-field-Categorical-Data"><a href="#Multi-field-Categorical-Data" class="headerlink" title="Multi-field Categorical Data"></a><center><strong>Multi-field Categorical Data</strong></center></h3><p>目前深度学习主要用在连续的数据集上，比如视觉、语音识别和自然语言处理上。但是用户点击率预测这种课题是一个离散的数据<code>Multi-field Categorical Data</code>，会有多种不同的字段，比如:[Weekday=Wednesday, Gender=Male, City=Shanghai, …]，我们想要<mark>识别这些特征之间的关系</mark>。传统的做法是应用<code>One-Hot Binary</code>的编码方式去处理这类数据，比如Weekday有7个取值，我们就将其编译为7维的二进制向量，其中只有Wednesday是1，其它都是0，因为它只有一个特征值；Gender只有两维，其中一维是1；如果有一万个城市的话，那City就有一万维，只有Shanghai取值为1，其它是0。</p>
<center><img src="/img/daily/adclick1.jpg" width="60%"></center>

<p>最终会得到一个高维稀疏向量。这个数据集不能直接用神经网络训练，因为如果直接用One-Hot Binary进行编码的话，那输入特征至少有一百万，第一层至少需要500个节点，那么第一层我们就需要训练5亿个参数，那就需要20亿或者50亿的数据集，而获得如此大的数据集比较困难。</p>
<h3 id="FM-FNN以及PNN模型"><a href="#FM-FNN以及PNN模型" class="headerlink" title="FM, FNN以及PNN模型"></a><center><strong>FM, FNN以及PNN模型</strong></center></h3><p>作者将FM跟Neural Network结合在一起，改进了之前的NN模型。FM (Factorization Machine)被认为是最有效的<code>embedding model</code>：</p>
<center><img src="/img/daily/adclick2.jpg" width="60%"></center>

<p>第一部分仍然是Logistic Regression，第二部分是特征之间跟目标变量之间的关系（通过两两向量之间的点积，点积大于0，表示这两个特征的组合跟目标值是正相关的）。这种算法在推荐系统领域应用比较广泛。作者用FM算法对底层输入field的one-hot binary编码进行embedding，把稀疏的二进制特征向量映射到<code>dense real</code>层，之后再把<code>dense real</code>层作为输入变量进行建模，这样就避免了高维二进制输入数据的计算复杂度。模型图如下：</p>
<center><img src="/img/daily/adclick3.jpg" width="80%"></center>

<p><mark>FNN跟一般的NN算法的区别是：</mark>大部分神经网络模型对向量之间的处理采用的是<mark>加法</mark>操作，相当于逻辑“或”；而FM则是通过向量之间的<mark>乘法</mark>来衡量两者之间的关系，这就相当于逻辑“且”。显然“且”比“或”更能严格区分目标变量。如下图所示。在第二层对向量的乘积处理中（比如上图蓝色节点直接为两个向量乘积，其连接边上没有参数需要学习），每一个field都只会被映射到一个low-dimensional vector，且field和field之间没有相互影响。</p>
<center><img src="/img/daily/adclick4.jpg" width="80%"></center>

<p>乘法关系的建模，可以采用内积或者外积，如下图。外积得到的矩阵中，只有对角线有值，即内积的结果，所以<mark>内积操作可以看做是外积操作的一种特殊情况</mark>。</p>
<center><img src="/img/daily/adclick5.jpg" width="80%"></center>

<p>PNN的神经网络图如下所示。进行embedding之后的输入数据有两种处理方法，一种是对该层的任意两个feature进行内积或者外积处理得到上图的蓝色节点，另外一种是把这些feature直接和1相乘复制到上一层的Z中，然后Z和P接在一起作为神经网络的输入层。</p>
<center><img src="/img/daily/adclick6.jpg" width="80%"></center>

<p><mark>对特征进行内积或者外积处理会产生一个复杂度的问题</mark>：weight矩阵比较庞大。解决方法：由于weight矩阵是个对称矩阵，可以用factorization来处理这个对称阵，把它转换成矩阵乘矩阵的转置，这样就会减少需要训练的参数，如下图所示。</p>
<center><img src="/img/daily/adclick7.jpg" width="60%"></center>

<h3 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a><center><strong>Metrics</strong></center></h3><p>评估模型主要看以下几个指标：</p>
<ul>
<li>Area under ROC curve (AUC)</li>
<li>Log loss</li>
<li>Root mean squared error (RMSE)</li>
<li>Relative Information Gain (RIG)</li>
</ul>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a><center><strong>Experiments</strong></center></h3><p>实验比较了dropout、hidden layer的层数、迭代次数、不同层级节点分布形态以及不同的Activation Function的效果。</p>
<h3 id="Summary-of-PNN"><a href="#Summary-of-PNN" class="headerlink" title="Summary of PNN"></a><center><strong>Summary of PNN</strong></center></h3><ol>
<li>深度学习在Multi-field的数据集上也有显著的应用效果；</li>
<li>通过外积和内积找到特征之间的相关关系；</li>
<li>在广告点击率的预测中，PNN效果优于其他模型。</li>
</ol>
<h3 id="Wide-and-Deep-Learning"><a href="#Wide-and-Deep-Learning" class="headerlink" title="Wide and Deep Learning"></a><center><strong>Wide and Deep Learning</strong></center></h3><p>这个<a href="https://arxiv.org/abs/1606.07792" target="_blank" rel="external">模型</a>结合了离散LR以及DNN的方法，category feature进行embedding输入到DNN，其它特征通过LR学习。LR是feature interaction，比较细粒度；而DNN则强调generalization，可以结合这两者的优势。把position bias特征放在最后一个隐层很有意义，可以避免一些不适合结合的features。</p>
<center><img src="/img/daily/adclick8.png"></center>

<h3 id="DNN-CTR-Prediction"><a href="#DNN-CTR-Prediction" class="headerlink" title="DNN CTR Prediction"></a><center><strong>DNN CTR Prediction</strong></center></h3><p>通过embedding的方式把离散特征转化为Dense Feature， 输入层同时可以增加其他Dense 特征， 比如CTR统计特征、similarity 特征、pv、click等等。每一个隐层都可以增加新的特征输入， 尤其是最后一个隐层。</p>
<center><img src="/img/daily/adclick9.png"></center>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇是对ad点击行为进行预测的总结，包括张伟楠在携程技术中心的一个&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzAwNDI4ODcxNA==&amp;amp;mid=2652243946&amp;amp;idx=2&amp;amp;sn=de8bb9a36fe9f2f0c97f30aaec87063f&amp;amp;scene=2&amp;amp;srcid=0727O4JXBdr5NU7sFOD48J69&amp;amp;from=timeline&amp;amp;isappinstalled=0#wechat_redirect&quot;&gt;演讲&lt;/a&gt;以及北大硕士Kintocai的&lt;a href=&quot;http://www.52cs.org/?p=1046&quot;&gt;总结&lt;/a&gt;。集中于深度学习在Multi-field Categorical这类数据集上的应用。&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
      <category term="Ad Click" scheme="http://cuiyungao.github.io/tags/Ad-Click/"/>
    
  </entry>
  
  <entry>
    <title>Tweet2Vec:Learning Tweet Embeddings Using Character-Level CNN-LSTM Encoder-Decoder</title>
    <link href="http://cuiyungao.github.io/2016/08/03/tweet2vec/"/>
    <id>http://cuiyungao.github.io/2016/08/03/tweet2vec/</id>
    <published>2016-08-03T12:41:08.000Z</published>
    <updated>2016-08-03T14:15:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>SIGIR’16的文章，来自MIT Media Lab。Vosoughi, Soroush, Prashanth Vijayaraghavan, and Deb Roy. “Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM Encoder-Decoder.”</p>
<a id="more"></a>
<h3 id="Major-Idea"><a href="#Major-Idea" class="headerlink" title="Major Idea"></a><center><strong>Major Idea</strong></center></h3><p>作者利用tweet用户的hashtag作为groundtruth，基于Character-level对每一条的embeddings进行学习，对hashtag进行推测，并与已有的word2vec跟Glove进行对比，发现Tweet2Vec的准确率最高。</p>
<h3 id="Techniques"><a href="#Techniques" class="headerlink" title="Techniques"></a><center><strong>Techniques</strong></center></h3><p>基于Character-level，以及作者曾经提出Bi-directional gated recurrent unit (Bi-GRU)’14来学习tweet的表示形式。</p>
<h4 id="Character-level-CNN-Tweet-Model"><a href="#Character-level-CNN-Tweet-Model" class="headerlink" title="Character-level CNN Tweet Model"></a>Character-level CNN Tweet Model</h4><p>作者采用temporal convolutional和temporal max-pooling operations，即计算一维的输入和输出之间的convolution和pooling函数。给定一个离散的输入函数$f(x)\in [1,l]\mapsto \mathbb{R}$，一个离散的kernel函数$k(x)\in [1,m]\mapsto \mathbb{R}$，stride $s$，$k(x)$和$f(x)$之间的卷积$g(y)\in [1,(l-m+1)/s]\mathbb{R}$, 对$f(x)$的pooling操作$h(y)\in [1, (l-m+1)/s]\mapsto \mathbb{R}$：<br>$$<br>g(y) = \sum_{x=1}^m k(x)\cdot f(y\cdot s - x + c) \\<br>h(y) = max_{x=1}^m f(y\cdot s - x + c)<br>$$<br>其中，$c=m-s+1$是一个补偿常量。tweet的character set包含英语字母，数字，特殊字符和不明确的字符，总共统计了70个字符。每个字符被encode成一个one-hot vector $x_i\in {0,1}^{70}$，单个tweet的最大长度是150，所以每个tweet被表示成了一个binary matrix $x_{1…150}\in {0,1}^{150\times70}$。表示成matix的tweet输入到一个包含4层一维卷基层的deep model。每个卷基层操作采用filter $w\in \mathbb{R}^l$来获取n-gram的character feature。一般来说，对于一个tweet $s$，在层$h$的一个特征$c_i$由下面式子生成：<br>$$<br>c_i^{(h)} (s) = g(w^{(h)}\cdot \hat{c}_i^{(h-1)} + b^{(h)})<br>$$<br>其中，$\hat{c}_i^{(0)}=x_{i…i+l-1}$, $b^{(h)}\in \mathbb{R}$是h层的bias，$g$是一个rectified linear unit。整体框架如下图所示：</p>
<center><img src="/img/papers/tweet2vec.png"></center>

<h4 id="Long-Short-Term-Memory-LSTM"><a href="#Long-Short-Term-Memory-LSTM" class="headerlink" title="Long-Short Term Memory (LSTM)"></a>Long-Short Term Memory (LSTM)</h4><p>给定一个输入序列$X=(x_1, x_2, …, x_N)$，LSTM计算hidden vector sequence $h=(h_1, h_2, …, h_N)$和output vector sequence $Y=(y_1, y_2, …, y_N)$。每一个时间步骤，一个模块的输出是由一组gates (前一个hidden state $h_{t1}$组成的函数)和当前时间步骤的输入控制的。forget gate $f_t$，input gate $i_t$和output gate $o_t$。这些gates集中决定了当前memory cess $c_t$的过渡和当前的hidden state $h_t$。LSTM的过渡函数定义如下：<br>$$<br>i_t = \sigma(W_i\cdot [h_{t-1}, x_t]+b_i) \\<br>f_t = \sigma(W_f\cdot [h_{t-1}, x_t]+b_f) \\<br>l_t = tanh(W_l\cdot [h_{t-1}, x_t]+b_l) \\<br>o_t = \sigma(W_o\cdot [h_{t-1}, x_t]+b_o) \\<br>c_t = f_t\odot c_{t-1} + i_t\odot l_t \\<br>h_t = o_t\odot tanh(c_t)<br>$$<br>其中，$\odot$表示component-wise multiplicaiton。$f_t$控制过去的memory cell要舍弃的信息，而$i_t$控制新的信息储存在current memory cell的程度，$o_t$是基于memory cell $c_t$的输出。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">LSTM是学习`long-term denpendencies`，所以在卷基层之后用LSTM学习获取特征的序列中存在的依赖。</div></pre></td></tr></table></figure></p>
<p>在seq-to-seq中，LSTM定义了output上的分布，之后用softmax来序列预测tokens。<br>$$<br>P(Y|X)=\prod_{t\in [1,N]} \frac{exp(g(h_{t-1}, y_t))}{\sum_{y^{\prime} exp(g(h_{t1}, y_t^{\prime}))}}<br>$$<br>其中，g是activation function。</p>
<h4 id="Combined-Model"><a href="#Combined-Model" class="headerlink" title="Combined Model"></a>Combined Model</h4><p>Figure 1呈现了整个encoder-decoder过程。输入是由matrix呈现的tweet，字符由one-hot vector表示。<br><mark>Encoder部分</mark>，在Character-level CNN的较高层卷积之后不经过pooling，直接作为LSTM的输入，encoder过程可以表示为：<br>$$<br>H^{conv} = CharCNN(T) \\<br>h_t = LSTM(g_t, h_{t-1}))<br>$$<br>其中，g=H^{conv}是一个特征矩阵，每一行代表LSTM的一个time step，$h_t$是$t$时刻的hidden representation。LSTM作用于$H^{conv}$的每一层，生成下一个序列的embedding。最终输出结果$enc_N$用来表示整条tweet。</p>
<p><mark>Decoder部分</mark>，用两层的LSTM作用于encoded representation。每一时间步骤中，字符的预测是：<br>$$<br>P(C_t|\cdot) = softmax(T_t, h_{t-1})<br>$$<br>其中，$C_t$指的是时间$t$的字符，$T_t$表示时刻$t$的one-hot vector。最终的结果是一个decoded tweet matrix $T^{dec}$，与实际的tweet进行比较，并学习模型的参数。</p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>实验用于两个分类任务： tweet semantic relatedness和tweet sentiment classification。3 million tweets。感觉数据量也挺小。准确率在0.6~0.7左右。</p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p><mark>Character level的deep learning的优势:</mark>占用内存少，不需要存储所有word的表示，只需要存储已有的character；不依赖于语言，只需要字符；而且不需要NLP的预处理，比如word segmentation。<br><mark>劣势是：</mark>运行速度慢，在文章的实验中，word2vec的速度是tweet2vec的6~7倍，原因是从word变成character输入意味着GRU的输入量变大。<br>Character level可以用在NLP的很多方面，比如named entity recognition, POS tagging, text classification, and language modeling。</p>
<blockquote><br>文章highlight了tweet2vec的优势，对word segmentation error, spelling mistakes, special characters, interpret emojis, in-vocab tokens非常有效。<br></blockquote>

<p><a href="http://soroush.mit.edu/publications/tweet2vec_vvr.pdf" target="_blank" rel="external">Published paper</a>.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;SIGIR’16的文章，来自MIT Media Lab。Vosoughi, Soroush, Prashanth Vijayaraghavan, and Deb Roy. “Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM Encoder-Decoder.”&lt;/p&gt;
    
    </summary>
    
      <category term="papers" scheme="http://cuiyungao.github.io/categories/papers/"/>
    
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Impactful ML Topics at ICML&#39;16</title>
    <link href="http://cuiyungao.github.io/2016/07/04/memory-network/"/>
    <id>http://cuiyungao.github.io/2016/07/04/memory-network/</id>
    <published>2016-07-04T07:32:54.000Z</published>
    <updated>2016-08-03T03:44:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>Summary from a talk on ICML’16.</p>
<a id="more"></a>
<h3 id="Deep-Residual-Networks"><a href="#Deep-Residual-Networks" class="headerlink" title="Deep Residual Networks"></a><center><strong>Deep Residual Networks</strong></center></h3><p>Residual networks可以在深度增加的同时提高准确率，可以产生许多问题的representations。适当的weight initialization和batch normalization使得网络可以。</p>
<h3 id="Memory-Networks-for-Language-Understanding"><a href="#Memory-Networks-for-Language-Understanding" class="headerlink" title="Memory Networks for Language Understanding"></a><center><strong>Memory Networks for Language Understanding</strong></center></h3><p><code>Memory Networks</code>是结合large memory跟可读可写的learning component的一类模型。结合了<code>Reasoning</code> with <code>attention</code> over <code>memory</code> (RAM)。大部分ML有limited memory，针对”low level” tasks，比如object detection。</p>
<p>得到word Embedding的过程是一个<code>encoding</code>的过程。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Summary from a talk on ICML’16.&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Find Researchers Here</title>
    <link href="http://cuiyungao.github.io/2016/07/04/researcher/"/>
    <id>http://cuiyungao.github.io/2016/07/04/researcher/</id>
    <published>2016-07-04T07:27:10.000Z</published>
    <updated>2016-07-04T07:32:39.000Z</updated>
    
    <content type="html"><![CDATA[<p>With researchers in some fields related to NLP and deep learning contained.</p>
<a id="more"></a>
<h3 id="Natural-Language-Processing"><a href="#Natural-Language-Processing" class="headerlink" title="Natural Language Processing"></a><center><strong>Natural Language Processing</strong></center></h3><ul>
<li>Jason Weston<br><a href="http://www.thespermwhale.com/jaseweston/" target="_blank" rel="external">Link.</a> Research Scientist at Facebook.</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;With researchers in some fields related to NLP and deep learning contained.&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Predicting Amazon Ratings Using Neural Networks</title>
    <link href="http://cuiyungao.github.io/2016/06/30/predictingamazon/"/>
    <id>http://cuiyungao.github.io/2016/06/30/predictingamazon/</id>
    <published>2016-06-30T09:41:17.000Z</published>
    <updated>2016-06-30T09:51:31.000Z</updated>
    
    <content type="html"><![CDATA[<p>The paper compares 3 different methods, bag-of-words model, word2vec (using RandomForests) and word2vec (using K Nearest Neighbors).</p>
<a id="more"></a>
<p>Word vectors经常被用作features应用于许多NLP领域，有两种模型CBOW和Skip-Gram。<code>Doc2vec</code>是word2vec的改进版，区别在于<code>model architecture</code>。在<code>doc2vec</code>中，algorithms是distributed memory (dm) and distributed bag of word (dbow)，但是其分类效果并不如word2vec。</p>
<p>结果显示word2vec的准确率是最高的，而且运行时间最短。</p>
<p>Original paper can be download <a href="http://cseweb.ucsd.edu/~jmcauley/cse190/reports/sp15/014.pdf" target="_blank" rel="external">here</a>.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The paper compares 3 different methods, bag-of-words model, word2vec (using RandomForests) and word2vec (using K Nearest Neighbors).&lt;/p&gt;
    
    </summary>
    
      <category term="papers" scheme="http://cuiyungao.github.io/categories/papers/"/>
    
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
      <category term="Rating Prediction" scheme="http://cuiyungao.github.io/tags/Rating-Prediction/"/>
    
  </entry>
  
  <entry>
    <title>User Modeling with Neural Network for Review Rating Prediction</title>
    <link href="http://cuiyungao.github.io/2016/06/30/usermodeling/"/>
    <id>http://cuiyungao.github.io/2016/06/30/usermodeling/</id>
    <published>2016-06-30T03:38:13.000Z</published>
    <updated>2016-06-30T09:41:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>[IJCAI’15] User Modeling with Neural Network for Review Rating Prediction. <a href="http://ir.hit.edu.cn/~dytang/" target="_blank" rel="external">Duyu Tang</a>, Bing Qin, Ting Liu<br>Proceeding of The 24th International Joint Conference on Artificial Intelligence.</p>
<a id="more"></a>
<p>作者Duyu Tang的分析集中于sentiment analysis，一年发了很多NLP的顶会文章（EMNLP, ACL, IJCAI, etc.）。</p>
<p>本文通过考虑user information来改善review rating prediction。启发源于lexical composition model，将<code>compositional modifier</code>作为一个matrix, 用<code>matrix-vector mulplication</code>作为compositon function。本文延伸lexical semantic compositio models，建立了<code>user-word composition vector model</code> (UWCVM)，并将结果作为feature用supervised learning framework来进行review rating prediction。</p>
<h3 id="Problem-Definition"><a href="#Problem-Definition" class="headerlink" title="Problem Definition"></a><center><strong>Problem Definition</strong></center></h3><p>Review rating prediction可以看做是a classificaiton/regression problem，由Pang and Lee <a href="http://www.cs.cornell.edu/home/llee/papers/pang-lee-stars.pdf" target="_blank" rel="external">2005</a>最开始做，采用<code>metric labeling</code> framework。Rating的performance很大程度上依赖于<code>feature representation</code>的选择。</p>
<p>Problem: Given a review $r_{k_j}$ comprised of $n$ words $\{w_1,w_2,…,w_n\}$ written by user $u_k$ as input, review rating prediction aims at infering the numeric rating (1~4 or 1~5 stars) of $r_{k_j}$. The problem can be regarded as a multi-class classification problem by inferring a discrete rating score.</p>
<h3 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a><center><strong>Methodology</strong></center></h3><p>The method includes two <code>composition models</code>, the user-word composition vector model (<strong>DWCVM</strong>) and the document composition vector model (<strong>DCVM</strong>). The former model将原始的word vectors加入user information，the latter model则将modified word vectors转化成review representation，并用其作为feature来进行rating prediction。Training的过程采用Pang and Lee提出的<code>supervised metric labeling</code>的方法。整个过程如下图表示：<br><br></p>
<center><img src="/img/papers/usermodeling.png" width="50%"></center>

<h4 id="User-Word-Composition-Vector-Model"><a href="#User-Word-Composition-Vector-Model" class="headerlink" title="User-Word Composition Vector Model"></a>User-Word Composition Vector Model</h4><p>整个model将original word vectors融入user information。融入的方法有两种：<code>additive</code>和<code>multiplicative</code>。给定两个向量$v_1$和$v_2$，<strong>Additive</strong>的结合方式认为输出向量$p$是a linear function of Cartesian product of $v_1$ and $v_2$，如下：</p>
<p>$$<br>p = \mathbf{A}\times v_1 + \mathbf{B}\times v_2<br>$$<br>其中$\mathbf{A}$和$\mathbf{B}$是matrices parameters，用来encode $v_1$, $v_2$对$p$的贡献。<strong>Multiplicative</strong>的结合方式认为输出向量$p$是a linear function of the tensor product of $v_1$ and $v_2$，如下：</p>
<p>$$<br>p = \mathbf{T}\times v_1 \times v_2 = \mathbf{U}_1 \times v_2<br>$$<br>其中$\mathbf{T}$是一个rank为3的tensor，将$v_1,v_2$的tensor product映射到$p$上。$\mathbf{T}$和$\v_1$的Partial product可以被看做生成新的矩阵$\mathbf{U}_1$，这个矩阵可以modify原始的word vectors。这两种结合方式可以用下图表示：<br><br></p>
<center><img src="/img/papers/usermodeling2.png" width="50%"></center>

<p>文章选用<code>multiplicative composition</code>，因为这种结合方式符合最初的用user information来改善word vectors的想法。后面的实验也验证了multiplicative composition的结合方式准确率要高于additive composition。</p>
<p>为了减少parameter size，user representation被用low-rank plus diagonal approximation来表示：$\mathbf{U}_k=\mathbf{U}_{k1} \times \mathbf{U}_{k2} + diag(u^{\prime})$，其中$\mathbf{U}_{k1}\in \mathbb{R}^{d\times r}$, $\mathbf{U}_{k2}\in \mathbb{R}^{r\times d}$, $u^{\prime}\in \mathbb{R}^d$。$u^{\prime}$是每个user共享的background representation，以应对某些在test set中有而在training set中没有的users，即<code>Out-Of-Vocabulary</code> situation。最终modified word vectors $p_i$：</p>
<p>$$<br>p_i = tanh(\mathbf{e}_{ik}) = tanh(\mathbf{U}_k\times \mathbf{e}_i)<br>\; = tanh((\mathbf{U}_{k1}\times \mathbf{U}_{k2} + diag(u^{\prime})) \times \mathbf{e}_i)<br>$$</p>
<h4 id="Document-Composition-Vector-Model"><a href="#Document-Composition-Vector-Model" class="headerlink" title="Document Composition Vector Model"></a>Document Composition Vector Model</h4><p>文中采用一个简单而有效的方法<a href="http://anthology.aclweb.org/P/P14/P14-1006.pdf" target="_blank" rel="external">paper</a>, recursivley uses <code>biTanh</code> function来生成document representation:</p>
<p>$$<br>biTanh(p) = \sum_{i=1}^n tanh(p_{i-1} + p_i)<br>$$<br>Each sentence将user-modified word vectors作为输入，得到sentence vectors；然后再将sentence vectors输入到<code>biTanh</code>得到最终的document vector $vec(doc)$。<code>biTanh</code>的Recursive use可以看做是two pairs of bag-of-word convolutional neural network，其中window size是2，parameters通过<code>addition</code>和<code>tanh</code>来定义。</p>
<h4 id="Rating-Prediction-with-Metric-Labeling"><a href="#Rating-Prediction-with-Metric-Labeling" class="headerlink" title="Rating Prediction with Metric Labeling"></a>Rating Prediction with Metric Labeling</h4><p>Review representation被用来进行review rating prediction，方法是<code>metric labeling</code> framework。</p>
<h4 id="Model-Training"><a href="#Model-Training" class="headerlink" title="Model Training"></a>Model Training</h4><p>获取back-propagation中对于whole set of parameters的derivative of the loss, 然后用stochastic gradient descent with mini-batch来更新这些parameters。文中用<code>dropout</code>来避免neural network being over-fitting。</p>
<p><strong>Note:</strong> Cases that rating does not match with review texts is not considered. [Zhang et al., SIGIR’14]  <a href="http://dl.acm.org/citation.cfm?id=2609501" target="_blank" rel="external">Paper</a></p>
<h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a><center><strong>Experiment</strong></center></h3><p>实验数据集有两个：<code>Yelp13</code>和<code>RT05</code>。实验结果说明:</p>
<ul>
<li>Semantic composition可以提高预测的准确度，并且结合<span style="color:white">metric labeling</span>可以改善结果，因为它基于”similar items, similar labels”的idea。</li>
<li>一个用户有越多的reviews，则这个用户的rating可以被很好地估计。</li>
<li>SSPE [Tang et al., 2014a]  <a href="http://anthology.aclweb.org/C/C14/C14-1018.pdf" target="_blank" rel="external">paper</a>更适用于short review.</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[IJCAI’15] User Modeling with Neural Network for Review Rating Prediction. &lt;a href=&quot;http://ir.hit.edu.cn/~dytang/&quot;&gt;Duyu Tang&lt;/a&gt;, Bing Qin, Ting Liu&lt;br&gt;Proceeding of The 24th International Joint Conference on Artificial Intelligence.&lt;/p&gt;
    
    </summary>
    
      <category term="papers" scheme="http://cuiyungao.github.io/categories/papers/"/>
    
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
      <category term="Rating Prediction" scheme="http://cuiyungao.github.io/tags/Rating-Prediction/"/>
    
      <category term="Neural Network" scheme="http://cuiyungao.github.io/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>UFLDL Tutorial</title>
    <link href="http://cuiyungao.github.io/2016/06/29/UFLDL/"/>
    <id>http://cuiyungao.github.io/2016/06/29/UFLDL/</id>
    <published>2016-06-29T14:39:40.000Z</published>
    <updated>2016-06-29T14:43:23.000Z</updated>
    
    <content type="html"><![CDATA[<p>The page records the knowledge learnt from UFLDL tutorial provided by Stanford. The resource can be found <a href="http://ufldl.stanford.edu/tutorial/" target="_blank" rel="external">here</a>.</p>
<a id="more"></a>
<p>The tutorial主要介绍unsupervised feature learning和deep learning，以及如何实现相关的算法。由于算法实现主要是matlab，所以主要记录里面的知识要点。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The page records the knowledge learnt from UFLDL tutorial provided by Stanford. The resource can be found &lt;a href=&quot;http://ufldl.stanford.edu/tutorial/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Good-Turing Estimation 大数据处理平滑算法</title>
    <link href="http://cuiyungao.github.io/2016/06/29/goodturing/"/>
    <id>http://cuiyungao.github.io/2016/06/29/goodturing/</id>
    <published>2016-06-29T09:22:55.000Z</published>
    <updated>2016-08-03T04:34:19.000Z</updated>
    
    <content type="html"><![CDATA[<p>参数平滑算法</p>
<a id="more"></a>
<p>参数平滑算法，是在训练数据不足时，采用某种方式对统计结果和概率估计进行必要的调整和修补，以降低由于数据稀疏现象带来的统计误差。</p>
<p>在实际应用中，数据稀疏会产生大量空值，影响后续处理的性能和效果，所以需要平滑算法。</p>
<p><code>Good-Turing</code>估计经常用在数据平滑方面。基本思想是：将统计参数按出现次数聚类，然后用出现次数加1的类来估计当前类。假定，一个元素在文本$W$中出现$r$次的概率是$\theta(r)$，$N_r$表示元素在$W$中正好出现$r$次的个数，即$N_r=|{x_j: \sharp(x_j)=r}|$，满足：</p>
<p>$$<br>N = \sum_{r} rN_r.<br>$$</p>
<p>Good-Turing估计$\theta(r)$为：$\hat{\theta}(r) = \frac{1}{N} (r+1) \frac{N_{r+1}}{N_r}$.</p>
<p>具体的分析过程可以看<a href="http://mp.weixin.qq.com/s?__biz=MjM5ODIzNDQ3Mw==&amp;mid=2649965669&amp;idx=1&amp;sn=297fa2d2b48c4a46135aeaa73d2d437a&amp;scene=0#wechat_redirect" target="_blank" rel="external">这里</a>。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;参数平滑算法&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="Data Mining" scheme="http://cuiyungao.github.io/tags/Data-Mining/"/>
    
  </entry>
  
  <entry>
    <title>Collaborative Filtering (CF)</title>
    <link href="http://cuiyungao.github.io/2016/06/29/cf/"/>
    <id>http://cuiyungao.github.io/2016/06/29/cf/</id>
    <published>2016-06-29T09:22:55.000Z</published>
    <updated>2016-07-19T02:31:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>This webpage contains information and implementation related to collaborative filtering.</p>
<a id="more"></a>
<p>对于recommender system，最常用的两个方法是<code>Content-Based</code>和<code>Collaborative Filtering (CF)</code>。CF基于users对items的态度来推荐items，即<code>&quot;wisdom of the crows&quot;</code>。相反，CB关注items的属性，用items之间的相似度来进行推荐。</p>
<p>CF分为<code>Memory-based CF</code>和<code>Model-based CF</code>。</p>
<h3 id="Implementation-Steps"><a href="#Implementation-Steps" class="headerlink" title="Implementation Steps"></a><center><strong>Implementation Steps</strong></center></h3><p>We use <code>scikit-learn</code> library将数据分成testing set和training set。For example,</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">import pandas as pd</div><div class="line">from sklearn import cross_validation as cv</div><div class="line">header = [&apos;user_id&apos;,&apos;item_id&apos;,&apos;rating&apos;,&apos;timestamp&apos;]</div><div class="line">data = pd.read_csv(&apos;ml-100k/u.data&apos;, sep=&apos;\t&apos;, names=header)</div><div class="line">train_data, test_data = cv.train_test_split(data, test_size=0.25)</div></pre></td></tr></table></figure>
<p>基于<code>cosine similarity</code>，我们得到<code>user_similarity</code>和<code>item_similarity</code>，对于<code>user-based CF</code>,我们用下面的公式：</p>
<p>$$<br>\hat{x}_{k,m} = \bar{x}_k + \frac{\sum_{u_a} sim_u(u_k, u_a)(x_{a,m}-\bar{x}_{u_a})}{\sum_{u_a} \|sim_u(u_k, u_a)\|}<br>$$</p>
<p>Users $k$ and $a$可以被当做是weights。Normalize确保评分在1~5之间。由于有相似taste的两个users可能在评分上一个偏高，一个偏低，所以前面需要加上user的平均rating作为bias。对于<code>item-based CF</code>,公式如下：</p>
<p>$$<br>\hat{x}_{k,m} = \frac{\sum_{i_b} sim_i(i_m,i_b)(x_{k,b})}{\sum_{i_b} \|sim_i(i_m,i_b)\|}<br>$$</p>
<h3 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a><center><strong>Evaluation</strong></center></h3><p>Metric采用<code>Root Mean Squared Error (RMSE)</code>,公式如下：</p>
<p>$$<br>RMSE = \sqrt{\frac{1}{N}\sum (x_i-\hat{x}_i)^2}<br>$$</p>
<p>实现采用<code>sklearn</code>提供的<code>mean_square_error</code> (MSE), RMSE是MSE的平方根。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">from sklearn.metrics import mean_squared_error</div><div class="line">from math import sqrt</div><div class="line">def rmse(prediction, ground_truth):</div><div class="line">  # find nonzeros places in groundtruth and extract from the prediction accordingly</div><div class="line">  prediction = prediction[ground_truth.nonzero()].flatten() </div><div class="line">  ground_truth = ground_truth[ground_truth.nonzero()].flatten()</div><div class="line">  return sqrt(mean_squared_error(prediction, ground_truth))</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">print &apos;User-based CF RMSE: &apos; + str(rmse(user_prediction, test_data_matrix))</div><div class="line">print &apos;Item-based CF RMSE: &apos; + str(rmse(item_prediction, test_data_matrix))</div></pre></td></tr></table></figure>
<p><span style="color:red">Drawback:</span> Memory-based CF doesn’t scale to real-world scenarios and cannot solve cold-start problem well. <code>Model-based</code> CF methods are scalabel and can deal with higher sparsity level than memory-based, but also suffers from cold-start problem.</p>
<p>Original <a href="http://online.cambridgecoding.com/notebooks/eWReNYcAfB/implementing-your-own-recommender-systems-in-python-2" target="_blank" rel="external">Link</a>. Also implemented locally.</p>
<h3 id="Recosystem"><a href="#Recosystem" class="headerlink" title="Recosystem"></a>Recosystem</h3><p>Recommender system using parallel matrix factorization <a href="https://cran.r-project.org/web/packages/recosystem/index.html" target="_blank" rel="external">Link.</a>. The author’s <a href="http://statr.me/2016/07/recommender-system-using-parallel-matrix-factorization/" target="_blank" rel="external">intro</a>.</p>
<h3 id="Fast-Recommendation-for-Activity-Stream"><a href="#Fast-Recommendation-for-Activity-Stream" class="headerlink" title="Fast Recommendation for Activity Stream"></a>Fast Recommendation for Activity Stream</h3><p>Fast Recommendations for Activity Streams Using Vowpal Wabbit <a href="http://blog.getstream.io/fast-recommendations-for-activity-streams-using-vowpal-wabbit" target="_blank" rel="external">Link</a>.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This webpage contains information and implementation related to collaborative filtering.&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="Recommend System" scheme="http://cuiyungao.github.io/tags/Recommend-System/"/>
    
  </entry>
  
  <entry>
    <title>Memorization and Exploration in Recurrent Neural Language Models</title>
    <link href="http://cuiyungao.github.io/2016/06/28/memorization/"/>
    <id>http://cuiyungao.github.io/2016/06/28/memorization/</id>
    <published>2016-06-28T14:27:33.000Z</published>
    <updated>2016-06-28T14:38:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>Deep Learning | Los Angeles Meetup</p>
<a id="more"></a>
<p>The speaker explains <code>recurrent memory networks</code>.<a href="https://www.youtube.com/watch?v=nPUW0akp5s0" target="_blank" rel="external">Link</a></p>
<h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><ul>
<li>LSTM is a powerful sequential model.</li>
<li>It has complicated design of input gate, forget gate, update gate and cell.</li>
<li>LSTM achieves SOTA in many NLP tasks: sentence completion, sentiment analysis, parsing….</li>
</ul>
<h3 id="Recurrent-Memory-Networks"><a href="#Recurrent-Memory-Networks" class="headerlink" title="Recurrent Memory Networks"></a>Recurrent Memory Networks</h3><ol>
<li><p>amplify the power of RNN</p>
</li>
<li><p>offer a way to interpret and discover <code>dependencies</code> in data</p>
</li>
<li><p>outperform TreeLSTM models that explicitly use syntactic information in Sentence Completion Challenge</p>
</li>
</ol>
<h3 id="First-Attempt"><a href="#First-Attempt" class="headerlink" title="First Attempt"></a>First Attempt</h3><p>Combine linearly source context vector, target context (Memory block) and target hidden state LSTM</p>
<ol>
<li><p>Did not see any gain in BLEU</p>
</li>
<li><p>Potentially bias toward target LM</p>
</li>
<li><p><code>Gating combination</code> might be essential for the model</p>
</li>
</ol>
<h3 id="Better-Memorization"><a href="#Better-Memorization" class="headerlink" title="Better Memorization"></a>Better Memorization</h3><p>Better memorization leads to better translation</p>
<ol>
<li>LSTM does not offer any representation advantage compared to vanilla RNN</li>
<li>Gradient in LSTM just flows nicer</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Deep Learning | Los Angeles Meetup&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Data Crawling</title>
    <link href="http://cuiyungao.github.io/2016/06/28/crawler/"/>
    <id>http://cuiyungao.github.io/2016/06/28/crawler/</id>
    <published>2016-06-28T13:26:03.000Z</published>
    <updated>2016-08-03T04:42:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>This webpage contains information about data crawling, such as some useful tools and summaries.</p>
<a id="more"></a>
<h3 id="Data-scraper-for-Facebook-Pages"><a href="#Data-scraper-for-Facebook-Pages" class="headerlink" title="Data scraper for Facebook Pages"></a>Data scraper for Facebook Pages</h3><p><a href="https://github.com/minimaxir/facebook-page-post-scraper" target="_blank" rel="external">Link.</a> Data scraper for Facebook Pages, and also code accompanying the blog post How to Scrape Data From Facebook Page Posts for Statistical Analysis. An example is shown below.</p>
<center><img src="/img/daily/fb_scraper_data.png" width="80%"></center>

<h3 id="Web-Crawler-System-in-Python"><a href="#Web-Crawler-System-in-Python" class="headerlink" title="Web Crawler System in Python"></a>Web Crawler System in Python</h3><p><a href="https://github.com/binux/pyspider" target="_blank" rel="external">Link.</a></p>
<ul>
<li>Write script in Python</li>
<li>Powerful WebUI with script editor, task monitor, project manager and result viewer</li>
<li>MySQL, MongoDB, Redis, SQLite, Elasticsearch; PostgreSQL with SQLAlchemy as database backend</li>
<li>RabbitMQ, Beanstalk, Redis and Kombu as message queue</li>
<li>Task priority, retry, periodical, recrawl by age, etc…</li>
<li>Distributed architecture, Crawl Javascript pages, Python 2&amp;3, etc…</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This webpage contains information about data crawling, such as some useful tools and summaries.&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="Data Mining" scheme="http://cuiyungao.github.io/tags/Data-Mining/"/>
    
      <category term="Crawler" scheme="http://cuiyungao.github.io/tags/Crawler/"/>
    
  </entry>
  
  <entry>
    <title>Generative Topic Embedding:a Continuous Representation of Documents</title>
    <link href="http://cuiyungao.github.io/2016/06/28/acl16/"/>
    <id>http://cuiyungao.github.io/2016/06/28/acl16/</id>
    <published>2016-06-28T12:30:48.000Z</published>
    <updated>2016-08-03T03:40:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>[ACL’16] From 清华<a href="http://bigml.cs.tsinghua.edu.cn/~jun/publications.shtml" target="_blank" rel="external">Zhu Jun</a>‘s Group</p>
<a id="more"></a>
<p><a href="http://www.weibo.com/ttarticle/p/show?id=2309403989692323372292" target="_blank" rel="external">Summary</a> from 洪亮劼</p>
<h3 id="Problem-Definition"><a href="#Problem-Definition" class="headerlink" title="Problem Definition"></a><center><strong>Problem Definition</strong></center></h3><p>Given the hyperparameters $\alpha$, $\gamma$, $\mu$, the learning objective is to find the embeddings $\mathbf{V}$, the topics $\mathbf{T}$, and the word-topic and document-topic distributions $p(\mathbf{Z}_i, \phi_i|d_i, \mathbf{A}, \mathbf{V}, \mathbf{T})$.</p>
<p>Here, $\mathbf{Z},\mathbf{T},\phi$ denote the collection of the document-specific $\{\mathbf{Z}_i\}_{i=1}^M$, $\{\mathbf{T}_i\}_{i=1}^M$, $\{\phi_i\}_{i=1}^M$. $\mathbf{V}$, $\mathbf{A}$, and $\mathbf{T}_i$ stand for the embeddings, the bigram residuals, and the topics, respectively.</p>
<p>As a whole, a generative word embedding model <code>PSDVec</code> (Li et al., 2015, <a href="http://www.aclweb.org/anthology/D15-1183" target="_blank" rel="external">ref</a>), by incorporating topics into it. The new model is named <code>TopicVec</code>.</p>
<h3 id="TopicVec-Model"><a href="#TopicVec-Model" class="headerlink" title="TopicVec Model"></a><center><strong>TopicVec Model</strong></center></h3><p>模型结合了topic modeling和word embeddings。In TopicVec, an embedding link function对一个topic中的word distribution进行建模，而不是LDA中的categorical distribution。这个link function的优势是semantic relatedness在embedding space当中被encoded。其它的过程跟LDA很相似，同样用Dirichlet priors来regularize topic distributions，用variational inference algorithm来进行optimization。这个过程可以得到跟words在同样embedding space的topic embeddings。Topic embeddings的目的是来估计underlying semantic centroids。</p>
<p>The whole process:</p>
<ol>
<li><p>For the $k$-th topic, draw a tpic embedding uniformly from a hyperball of radius $\gamma$, i.e., $t_k\sim Unif(B_{\gamma})$;</p>
</li>
<li><p>For each document $d_i$:<br>(a) Draw the mixing proportions $\phi_i$ from the Dirichlet prior Dir($\alpha$);<br>(b) For the $j$-th word:<br> $\;$ i. Draw topic assignment $z_{ij}$ from the categorical distribution Cat($\phi_i$);<br> $\;$ ii. Draw word $w_{ij}$ from $\mathbf{S}$ according to $P(w_{ij}|w_{i,j-c}:w_{i,j-1},z_{ij},d_i)$.</p>
</li>
</ol>
<center><img src="/img/papers/topicvec.png" width="70%"></center><br><center>Fig1. Graphical representation of TopicVec</center>

<p>The notations are listed in the following table:</p>
<center><img src="/img/papers/topicvec1.png" width="70%"></center>

<h3 id="Word-Embeddings-amp-Topic-Modeling"><a href="#Word-Embeddings-amp-Topic-Modeling" class="headerlink" title="Word Embeddings &amp; Topic Modeling"></a><center><strong>Word Embeddings &amp; Topic Modeling</strong></center></h3><p>Word embedding通过一个小的context window里面的local word collocation patterns,将words映射到一个低维连续的embedding space。而topic modeling通过同一文档里面的global word collocation patterns，将documents映射到一个低维的topic space。这两个可以互补，本文由此得到TopicVec Model。其中，topics由embedding vectors来表示，并在documents之间共享。每个word的probability由local context和topic来决定。Topic embedding由variational inference method产生，同时得到每个document的topic mixing probability。结合topic embedding和topic mixing probability，可以在低维连续空间里面得到每个document的representation。</p>
<p>Euclidean distance不是衡量两个embeddings之间相似度的最优方法，已有的方法大部分采用exponentiated cosine similarity作为link function，所以cosine similarity可能是较好的估计语义相似度的方法。</p>
<h3 id="Inspirations"><a href="#Inspirations" class="headerlink" title="Inspirations"></a><center><strong>Inspirations</strong></center></h3><ul>
<li>从此方法得到的word embeddings，然后再用DRNN等方法来进行classification？</li>
<li>可以用于DTM上么？</li>
</ul>
<p>本文的code可以在这里<a href="https://github.com/askerlee/topicvec" target="_blank" rel="external">下载</a>,Nguyen et al. (2015)的方法与之类似，将word embeddings作为latent features，但是实现速度慢，对large corpus不可行，code可以在<a href="https://github.com/datquocnguyen/LFTM" target="_blank" rel="external">这里</a>找到。 </p>
<p>The original paper can be found <a href="https://arxiv.org/abs/1606.02979" target="_blank" rel="external">here</a>.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[ACL’16] From 清华&lt;a href=&quot;http://bigml.cs.tsinghua.edu.cn/~jun/publications.shtml&quot;&gt;Zhu Jun&lt;/a&gt;‘s Group&lt;/p&gt;
    
    </summary>
    
      <category term="papers" scheme="http://cuiyungao.github.io/categories/papers/"/>
    
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
      <category term="Topic Modeling" scheme="http://cuiyungao.github.io/tags/Topic-Modeling/"/>
    
  </entry>
  
  <entry>
    <title>Study Material for Deep Learning and NLP</title>
    <link href="http://cuiyungao.github.io/2016/06/28/material/"/>
    <id>http://cuiyungao.github.io/2016/06/28/material/</id>
    <published>2016-06-28T12:21:47.000Z</published>
    <updated>2016-06-28T12:46:54.000Z</updated>
    
    <content type="html"><![CDATA[<p>The website summaries the material for understanding deep learning and NLP.</p>
<a id="more"></a>
<h4 id="How-to-Start-Learning-Deep-Learning"><a href="#How-to-Start-Learning-Deep-Learning" class="headerlink" title="How to Start Learning Deep Learning"></a>How to Start Learning Deep Learning</h4><p><a href="http://ofir.io/How-to-Start-Learning-Deep-Learning/" target="_blank" rel="external">Link</a></p>
<h4 id="Book-First-Contact-with-TensorFlow"><a href="#Book-First-Contact-with-TensorFlow" class="headerlink" title="[Book] First Contact with TensorFlow"></a>[Book] First Contact with TensorFlow</h4><p><a href="http://www.jorditorres.org/first-contact-with-tensorflow/" target="_blank" rel="external">Link</a> The book introduces how to use TensorFlow to implement the basic functions of neural network and also parallelism.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The website summaries the material for understanding deep learning and NLP.&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
      <category term="material" scheme="http://cuiyungao.github.io/tags/material/"/>
    
  </entry>
  
  <entry>
    <title>LDA Understanding</title>
    <link href="http://cuiyungao.github.io/2016/06/28/lda/"/>
    <id>http://cuiyungao.github.io/2016/06/28/lda/</id>
    <published>2016-06-28T09:17:25.000Z</published>
    <updated>2016-07-20T06:24:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>LDA introduction and summary.</p>
<a id="more"></a>
<h3 id="Topic-Model-Reading-List"><a href="#Topic-Model-Reading-List" class="headerlink" title="Topic Model Reading List"></a>Topic Model Reading List</h3><p><a href="http://bigml.cs.tsinghua.edu.cn/~jianfei/lda-reading.html" target="_blank" rel="external">Link.</a><br>详细解释：</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;LDA introduction and summary.&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
      <category term="LDA" scheme="http://cuiyungao.github.io/tags/LDA/"/>
    
  </entry>
  
  <entry>
    <title>Useful Source Code for NLP</title>
    <link href="http://cuiyungao.github.io/2016/06/28/code/"/>
    <id>http://cuiyungao.github.io/2016/06/28/code/</id>
    <published>2016-06-28T09:17:25.000Z</published>
    <updated>2016-07-29T03:27:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>This page contains useful code related to NLP.</p>
<a id="more"></a>
<h3 id="TensorFlow-v0-9-now-available-with-improved-mobile-support"><a href="#TensorFlow-v0-9-now-available-with-improved-mobile-support" class="headerlink" title="TensorFlow v0.9 now available with improved mobile support."></a>TensorFlow v0.9 now available with improved mobile support.</h3><p><a href="https://developers.googleblog.com/2016/06/tensorflow-v09-now-available-with.html" target="_blank" rel="external">Link</a> TensorFlow now supports mobile invocation.</p>
<h3 id="Tensorflow-and-theano-CNN-code-for-insurance-QA-question-Answer-matching"><a href="#Tensorflow-and-theano-CNN-code-for-insurance-QA-question-Answer-matching" class="headerlink" title="Tensorflow and theano CNN code for insurance QA(question Answer matching)."></a>Tensorflow and theano CNN code for insurance QA(question Answer matching).</h3><p><a href="https://github.com/white127/insuranceQA-cnn" target="_blank" rel="external">Link.</a> Theano和tensorflow的网络结构都是一致的: word embedings + CNN + max pooling + cosine similarity.目前再insuranceQA的test1数据集上，top-1准确率可以达到62%左右，跟论文上是一致的。这里只提供了CNN的代码，后面测试了LSTM和LSTM+CNN的方法，LSTM+CNN的方法比单纯使用CNN或LSTM效果还要更好一些，在test1上的准确率可以再提示5%-6%.</p>
<h3 id="LSTM-language-model-with-CNN-over-characters-in-TensorFlow"><a href="#LSTM-language-model-with-CNN-over-characters-in-TensorFlow" class="headerlink" title="LSTM language model with CNN over characters in TensorFlow."></a>LSTM language model with CNN over characters in TensorFlow.</h3><p><a href="https://github.com/carpedm20/lstm-char-cnn-tensorflow" target="_blank" rel="external">Link.</a>Tensorflow implementation of Character-Aware Neural Language Models <a href="http://arxiv.org/abs/1508.06615" target="_blank" rel="external">paper</a>.</p>
<h3 id="Topic-Augmented-Neural-Response-Generation-with-a-Joint-Attention-Mechanism"><a href="#Topic-Augmented-Neural-Response-Generation-with-a-Joint-Attention-Mechanism" class="headerlink" title="Topic Augmented Neural Response Generation with a Joint Attention Mechanism"></a>Topic Augmented Neural Response Generation with a Joint Attention Mechanism</h3><p><a href="https://github.com/LynetteXing1991/TAJA-Seq2Seq" target="_blank" rel="external">Link.</a> Use attention model.</p>
<h3 id="Text-input-with-relevant-emoji-sorted-with-deep-learning"><a href="#Text-input-with-relevant-emoji-sorted-with-deep-learning" class="headerlink" title="Text input with relevant emoji sorted with deep learning"></a>Text input with relevant emoji sorted with deep learning</h3><p><a href="http://codepen.io/Idlework/pen/xOgGqM" target="_blank" rel="external">link.</a> After inputing a sentence, the related emojis are shown in the top. I’ve tried the project, and it’s amazing. It uses API from <a href="http://getdango.com/emoji-and-deep-learning.html" target="_blank" rel="external">Dango</a>. The webpage recommends deep learning <a href="http://neuralnetworksanddeeplearning.com/" target="_blank" rel="external">material</a>. I need to read them in detail, which maybe helpful for review analysis.</p>
<h3 id="Information-Extraction-with-Reinforcement-Learning"><a href="#Information-Extraction-with-Reinforcement-Learning" class="headerlink" title="Information Extraction with Reinforcement Learning"></a>Information Extraction with Reinforcement Learning</h3><p><a href="https://github.com/karthikncode/DeepRL-InformationExtraction" target="_blank" rel="external">Link.</a> It’s based on Torch and python.</p>
<h3 id="Visualization-Toolbox-for-Long-Short-Term-Memory-networks-LSTMs"><a href="#Visualization-Toolbox-for-Long-Short-Term-Memory-networks-LSTMs" class="headerlink" title="Visualization Toolbox for Long Short Term Memory networks (LSTMs)"></a>Visualization Toolbox for Long Short Term Memory networks (LSTMs)</h3><p><a href="https://github.com/HendrikStrobelt/LSTMVis" target="_blank" rel="external">Link.</a> Visual Analysis for State Changes in RNNs. More information about LSTMVis, an introduction video, and the link to the live demo can be found <a href="http://lstm.seas.harvard.edu/" target="_blank" rel="external">here</a>.</p>
<h3 id="Attention-Sum-Reader"><a href="#Attention-Sum-Reader" class="headerlink" title="Attention Sum Reader"></a>Attention Sum Reader</h3><p><a href="https://github.com/rkadlec/asreader" target="_blank" rel="external">Link.</a> This is a Theano/Blocks implementation of the Attention Sum Reader model as presented in “Text Comprehension with the Attention Sum Reader Network” available at <a href="http://arxiv.org/abs/1603.01547" target="_blank" rel="external">here</a>. </p>
<h3 id="Wider-amp-deep-learning-better-together-with-TensorFlow"><a href="#Wider-amp-deep-learning-better-together-with-TensorFlow" class="headerlink" title="Wider &amp; deep learning: better together with TensorFlow"></a>Wider &amp; deep learning: better together with TensorFlow</h3><p><a href="https://research.googleblog.com/2016/06/wide-deep-learning-better-together-with.html" target="_blank" rel="external">Link.</a> <span style="color:white">Query的过程就是找出类似的词的过程，这个可以用phrase extraction？是否可以用在rating prediction上面。</span></p>
<h3 id="Minimal-Character-Level-Language-Model"><a href="#Minimal-Character-Level-Language-Model" class="headerlink" title="Minimal Character-Level Language Model"></a>Minimal Character-Level Language Model</h3><p><span style="color:red">需看~！</span><a href="https://github.com/weixsong/min-char-rnn" target="_blank" rel="external">Link.</a> Minimal character-level language model with a Vanilla Recurrent Neural Network, in Python/numpy.</p>
<h3 id="LSTM-Parser"><a href="#LSTM-Parser" class="headerlink" title="LSTM-Parser"></a>LSTM-Parser</h3><p><a href="https://github.com/clab/joint-lstm-parser" target="_blank" rel="external">Link.</a> Based on <code>C++</code> language. Transition-based joint syntactic dependency parser and semantic role labeler using stack LSTM RNN architecture. Paper <code>Greedy, joint syntactic-semantic parsing with stack LSTMs</code> can be downloaded <a href="http://www.taln.upf.edu/content/biblio/785" target="_blank" rel="external">here</a>.</p>
<h3 id="Pre-trained-Word-Embedding-in-Keras"><a href="#Pre-trained-Word-Embedding-in-Keras" class="headerlink" title="Pre-trained Word Embedding in Keras"></a>Pre-trained Word Embedding in Keras</h3><p><a href="http://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html" target="_blank" rel="external">Link.</a> Based on Keras library. In this tutorial, we will walk you through the process of solving a text classification problem using pre-trained word embeddings and a convolutional neural network.</p>
<h3 id="Building-Machine-Learning-Estimator-in-TensorFlow"><a href="#Building-Machine-Learning-Estimator-in-TensorFlow" class="headerlink" title="Building Machine Learning Estimator in TensorFlow"></a>Building Machine Learning Estimator in TensorFlow</h3><p><a href="http://terrytangyuan.github.io/2016/07/08/understand-and-build-tensorflow-estimator/" target="_blank" rel="external">Link.</a> The purpose of this post is to help you better understand the underlying principles of estimators in TensorFlow Learn and point out some tips and hints if you ever want to build your own estimator that’s suitable for your particular application.</p>
<h3 id="TensorLayer-Deep-learning-and-Reinforcement-learning-library"><a href="#TensorLayer-Deep-learning-and-Reinforcement-learning-library" class="headerlink" title="TensorLayer: Deep learning and Reinforcement learning library"></a>TensorLayer: Deep learning and Reinforcement learning library</h3><p><a href="https://github.com/zsdonghao/tensorlayer" target="_blank" rel="external">Link.</a> It was designed to provide a higher-level API to TensorFlow in order to speed-up experimentations. </p>
<h3 id="Neural-Relation-Extraction"><a href="#Neural-Relation-Extraction" class="headerlink" title="Neural Relation Extraction"></a>Neural Relation Extraction</h3><p><a href="https://github.com/thunlp/NRE" target="_blank" rel="external">Link.</a> Neural relation extraction aims to extract relations from plain text with neural models, which has been the state-of-the-art methods for relation extraction. In this project, we provide our implementations of CNN [Zeng et al., 2014] and PCNN [Zeng et al.,2015] and their extended version with sentence-level attention scheme [Lin et al., 2016] </p>
<h3 id="Make-a-Chatting-Robot"><a href="#Make-a-Chatting-Robot" class="headerlink" title="Make a Chatting Robot"></a>Make a Chatting Robot</h3><p><a href="https://github.com/warmheartli/ChatBotCourse" target="_blank" rel="external">Link.</a> 自己动手做聊天机器人教程.</p>
<h3 id="Door-to-Machine-Learning"><a href="#Door-to-Machine-Learning" class="headerlink" title="Door to Machine Learning"></a>Door to Machine Learning</h3><p><a href="https://github.com/warmheartli/MachineLearningCourse" target="_blank" rel="external">Link.</a> 机器学习精简入门教程.</p>
<h3 id="Sequence-Classification-with-LSTM-RNN-with-Keras"><a href="#Sequence-Classification-with-LSTM-RNN-with-Keras" class="headerlink" title="Sequence Classification with LSTM RNN with Keras"></a>Sequence Classification with LSTM RNN with Keras</h3><p><a href="http://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/" target="_blank" rel="external">Link.</a> Python based.实验结果证明加了CNN的效果更好，用dropout避免过拟合，但是实验结果不是那么理想，原因是深度只有3，当层数增加的时候可以实现比不用dropout更好的效果。</p>
<h3 id="Neural-Conversation-Models"><a href="#Neural-Conversation-Models" class="headerlink" title="Neural Conversation Models"></a>Neural Conversation Models</h3><p><a href="https://github.com/pbhatia243/Neural_Conversation_Models" target="_blank" rel="external">Link.</a> TensorFlow based.支持simple seq2seq models和attention based seq2seq models.</p>
<h3 id="深度学习主机环境配置"><a href="#深度学习主机环境配置" class="headerlink" title="深度学习主机环境配置"></a>深度学习主机环境配置</h3><p><a href="http://www.52nlp.cn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%BB%E6%9C%BA%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-ubuntu16-04-geforce-gtx1080-tensorflow" target="_blank" rel="external">Link.</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This page contains useful code related to NLP.&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
      <category term="code" scheme="http://cuiyungao.github.io/tags/code/"/>
    
  </entry>
  
  <entry>
    <title>Deep Recursive Neural Networks for Compositionality in Language</title>
    <link href="http://cuiyungao.github.io/2016/06/28/drnn/"/>
    <id>http://cuiyungao.github.io/2016/06/28/drnn/</id>
    <published>2016-06-28T07:04:35.000Z</published>
    <updated>2016-06-28T09:26:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>Irsoy, Ozan, and Claire Cardie. “Deep recursive neural networks for compositionality in language.” Advances in Neural Information Processing Systems. 2014.</p>
<a id="more"></a>
<h3 id="Recursive-Neural-Network"><a href="#Recursive-Neural-Network" class="headerlink" title="Recursive Neural Network"></a><center><strong>Recursive Neural Network</strong></center></h3><p>Recursive neural networks (RNNs) comprise a class of architecture that can operate on <code>structured input</code>. <span style="color:white">The same set of weights is recursively applied within a structural setting.</span> Given a positional directed acyclic graph, it visits the nodes in topological order, and recursively applies transformations to generate further representations from previously computed representations of children.</p>
<p>A <code>recurrent neural network</code> is simply a <code>recursive</code> neural network with a particular structure (Figure 1c).<br><br></p>
<center><img src="/img/papers/drnn.png" width="60%"></center>

<p><span style="color:red">Problem:</span> Even though RNNs are deep in structure, they lack the capacity for hierarchical representation that exists in conventional deep feed-forward networks and recurrent neural networks.</p>
<h3 id="Recurrent-v-s-Recursive"><a href="#Recurrent-v-s-Recursive" class="headerlink" title="Recurrent v.s. Recursive"></a><center><strong>Recurrent v.s. Recursive</strong></center></h3><p>Recurrent neural networks are deep in time, while recursive neural networks are deep in structure (due to the repeated application of recursive connections). Recently, the notions of depth in time - the result of recurrent connections, and depth in space - the result of stacking multiple layers on top of one another, are distinguished for recurrent neural network. Deep recurrent neural networks were proposed for composing these concepts. They are created by stacking multiple recurrent layers on top of each other. This allows the extra notion of depth to be incorporated into temporal processing. </p>
<p>Inspired by <code>deep recurrent</code> neural networks, <code>deep recursive</code> neural networks are proposed in this paper.</p>
<h3 id="Deep-Recursive-Neural-Networks"><a href="#Deep-Recursive-Neural-Networks" class="headerlink" title="Deep Recursive Neural Networks"></a><center><strong>Deep Recursive Neural Networks</strong></center></h3><p>An important benefit of depth is the hierarchy among <code>hidden representations</code>: every hidden layer conceptually lies in a different representation space and potentially is a more abstract representation of the input than the previous layer.</p>
<p>The DRNN is constructed by <code>stacking</code> multiple layers of individual <code>recursive</code> nets:<br><br></p>
<center><img src="/img/papers/drnnformula.png" width="50%"></center>

<p>where $i$ means the multiple stacked layers, $W_L^{(i)}$, $W_R^{(i)}$, and $b^{(i)}$ are the weight matrices that connect the left and right children to the parent, and a bias vector, respectively. $V^{(i)}$ is the weight matrix that connects the $(i-1)$th hidden layer to the $i$th hidden layer. For the untying shown in Figure 1b, every node is represented in the same space above the first, regardless of their <code>leafness</code>. Figure 2 shows the weights that are untied or shared.<br><br></p>
<center><img src="/img/papers/drnnfig2.png" width="60%"></center>

<p>For <code>prediction</code>, we connect the output layer to <span style="color: red">only</span> the <code>final hidden layer</code>.<br><br></p>
<center><img src="/img/papers/drnnformula2.png" width="20%"></center>

<p>If we connect the output layer to all hidden layers, multiple hidden layers can have <code>synergistic effects</code> on the output and make it more difficult to qualitatively analyze each layer.</p>
<h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a><center><strong>Results</strong></center></h3><p><span style="color:white">Data:</span> Stanford Sentiment Treebank (SST) <a href="http://nlp.stanford.edu/pubs/SocherEtAl_EMNLP2013.pdf" target="_blank" rel="external">link</a></p>
<h4 id="Result-1"><a href="#Result-1" class="headerlink" title="Result 1"></a>Result 1</h4><p>Comparing with multiplicative RNN and the more recent Paragraph Vectors, DRNNs outperform their shallow counterparts of the same size. Deep RNN outperforms the baselines, achieving <code>state-of-the-art</code> performance on the task.<br><br></p>
<center><img src="/img/papers/drnnresult1.png" width="60%"></center>

<p><span style="color: red">Reason:</span> The authors attribute an important contribution of the improvement to <code>dropouts</code>.</p>
<h4 id="Result-2"><a href="#Result-2" class="headerlink" title="Result 2"></a>Result 2</h4><p>For searching <code>nearest neighbor phrases</code>, different layers capture different aspects. <code>One-nor</code> distance mearsure is used.<br><br></p>
<center><img src="/img/papers/drnnresult2.png" width="60%"></center>

<p><span style="color: red">Analysis:</span> The first layer is dominated by one of the words that is composed. The seconde layer takes syntactic similarity more into account. The third layer captures the sentiment.</p>
<p><span style="color: red">Inspiration:</span> Can we apply this into emoji detection?<br><br><br>Paper can be download <a href="https://www.cs.cornell.edu/~oirsoy/files/nips14drsv.pdf" target="_blank" rel="external">here</a>.<br>Code is written in C++, can be found <a href="https://github.com/oir/deep-recursive" target="_blank" rel="external">here</a>.<br>Introduction webpage is <a href="http://www.cs.cornell.edu/~oirsoy/drsv.htm" target="_blank" rel="external">here</a>.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Irsoy, Ozan, and Claire Cardie. “Deep recursive neural networks for compositionality in language.” Advances in Neural Information Processing Systems. 2014.&lt;/p&gt;
    
    </summary>
    
      <category term="papers" scheme="http://cuiyungao.github.io/categories/papers/"/>
    
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
  </entry>
  
</feed>
