<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Cuiyun Gao&#39;s Daily Digest</title>
  <subtitle>Work Hard, and Play Harder. 如果有一天：你不再寻找爱情，只是去爱；你不再渴望成功，只是去做；你不再追求成长，只是去修行；一切才真正开始~！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://cuiyungao.github.io/"/>
  <updated>2016-08-18T09:46:44.000Z</updated>
  <id>http://cuiyungao.github.io/</id>
  
  <author>
    <name>Cuiyun Gao</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>The Unreasonable Confusion of Variational Autoencoders</title>
    <link href="http://cuiyungao.github.io/2016/08/18/vae/"/>
    <id>http://cuiyungao.github.io/2016/08/18/vae/</id>
    <published>2016-08-18T08:03:40.000Z</published>
    <updated>2016-08-18T09:46:44.000Z</updated>
    
    <content type="html"><![CDATA[<p>将deep learning和graphical model结合起来理解<a href="https://jaan.io/unreasonable-confusion/" target="_blank" rel="external">VAE</a>。</p>
<a id="more"></a>
<p>VAE即variational autoencoders，作用是设计复杂的数据的generative models，并fit到大数据集上。下面从两个角度来理解VAE，第一个是neural networks，第二个是概率模型中的variational inference。</p>
<h3 id="Neural-Network-Perspective"><a href="#Neural-Network-Perspective" class="headerlink" title="Neural Network Perspective"></a><center><strong>Neural Network Perspective</strong></center></h3><p>在神经网络语言中，一个VAE包含<code>encoder</code>, <code>decoder</code>和<code>loss function</code>，如下图所示。<code>Encoder</code>是一个神经网络，输入是一个数据点$x$，输出是其hidden representation $z$，这个过程中包含权重和biases $\theta$。<mark><code>Encoder</code>必须有效地压缩数据，并映射到低维空间。</mark>由于低维空间是随机的，encoder输出参数为高斯概率密度$q_{\theta}(z|x)$，我们可以从这个分布采样，得到$z$的噪声值。</p>
<center><img src="/img/daily/VAE1.png" width="90%"></center>

<p><code>Decoder</code>是另外一个神经网络，输入是hidden representation $z$，输出参数为数据的概率分布$p_{\phi}(x|z)$，这个过程中包含权重和biases $\phi$。<mark>由于从较小的维度得到较大的维度，所以信息会丢失。</mark>我们的目标是让信息尽可能丢失得比较少，即<code>encoder</code>要根据hidden representation有效地重建输入$x$。我们定义<code>loss function</code>是带regularizer的negative log-likelihood。因为没有所有点的全局表示形式，我们可以分解这个loss function到仅依赖于单独点$l_i$的项，这样对$N$个点的总的loss为$\sum_{i=1}^N l_i$，其中：<br>$$<br>l_i(\theta, \phi)=-E_{z\sim q_{\theta}(z|x_i)}[log p_{\phi}(x_i|z)] + KL(q_{\theta}(z|x_i)||p(z)),<br>$$<br>第一项是重建损失，或者是对第i个点的expected negative log-likelihood。这个预期是针对encoder在$z$上的分布，目的是尽量地重建数据。第二项是一个regularizer，作用是衡量encoder的分布$q_{\theta}(z|x)$和$p(z)$之间的距离，即在用$q$表示$p$的过程中损失了多少信息。</p>
<p>在VAE中，$p$被认为是正态分布，即$p(z)=Normal(0,1)$, regularizer的作用是<code>保证每个数字的hidden representation $z$足够不同</code>，如果不包含这一项，encoder会给每一个点一个欧式空间中不同的区域，这意味着同一个数字的两个图片也会有不同的表示形式。最后，我们用梯度下降的方法来训练VAE，每个step size $\rho$，参数$\theta$和$\phi$都会被更新，比如$\theta \, \gets \, \theta-\rho\frac{\partial l}{\partial \theta}$。</p>
<h3 id="Probability-Model-Perspective"><a href="#Probability-Model-Perspective" class="headerlink" title="Probability Model Perspective"></a><center><strong>Probability Model Perspective</strong></center></h3><p>在概率模型中，一个VAE包含数据$x$的<code>概率模型</code>和<code>隐含变量</code>$z$，即对于每个数据点$i$，<code>generative process</code>可以写成如下形式：</p>
<ul>
<li>首先计算隐含变量 $z_i \, \sim \, p(z)$；</li>
<li>然后计算数据点的后验概率 $x_i \, \sim \, p(x|z)$。</li>
</ul>
<center><img src="/img/daily/VAE2.png" width="50%"></center><br><center>A Graphical Model</center>

<p>这个模型定义了联合概率$p(x,z)=p(x|z)p(z)$，对于<code>inference</code>，我们的目标是给定观察数据，推测隐含变量的值，即计算后验概率$p(z|x)$，通过Bays，我们得到$p(z|x)=\frac{p(x|z)p(z)}{p(x)}$。分母$p(x)=\int p(x,z)p(z)\mathrm{d}z$的估计需要遍历所有隐含变量，这需要大量的时间，所以我们需要另外想办法来估计这个后验概率。</p>
<p><code>Variational inference</code>用一组分布$q_{\lambda}(z|x)$来对$p(z|x)$进行估计，$\lambda$表示不同的分布，也就是说，假定$q$是高斯分布，那么对每一个数据点的隐含变量$\lambda_{x_i}=(\mu_{x_i}, \sigma_{x_i}^2)$。我们的目标是使得这个估计$q_{\lambda}(z|x)$与真实的$q(z|x)$尽可能接近，由于都是概率，我们很自然地想到KL divergence，即：<br>$$<br>KL(q_{\lambda}(z|x)||p(z|x))=E_q[log q_{\lambda}(z|x)]-E_q[log p(x,z)] + log p(x).<br>$$</p>
<p>由之前的分析可知，$p(x)$很难得到。我们的目的是最小化这个KL divergence，将上式改写如下：<br>$$<br>log p(x) = KL(q_{\lambda}(z|x)||p(z|x))+ELBO(\lambda),<br>$$<br>其中，$ELBO(\lambda)=E_q[log p(x,z)]-E_q[log q_{\lambda}(z|x)]$，所以可以转变成最大化ELBO (Evidence Lower BOund)函数。由于数据点之间并没有共享隐含变量，所以总的目标函数可以认为是每个点的目标函数的和，也使得我们可以用梯度下降来更新迭代$\lambda$。所以单独一个点的ELBO可以写成 (需要进行化简)：<br>$$<br>ELBO_i(\lambda)=E_{q_{\lambda}(z|x_i)}[log p(x_i|z)]-KL(q_{\lambda}(z|x_i)||p(z))。<br>$$</p>
<h3 id="Combination-of-the-Two-Models"><a href="#Combination-of-the-Two-Models" class="headerlink" title="Combination of the Two Models"></a><center><strong>Combination of the Two Models</strong></center></h3><p>从上面的分析可以看出，VAE主要分为两部分，<code>inference network</code> (encoder)和<code>generative network</code> (decoder)。前者是参数化后验概率$q_{\theta}(z|x,\lambda)$，后者是参数化$p(x|z)$。由于没有全局的隐含变量，我们可以用梯度下降(e.g., minibatch)最大化ELBO。统一概率模型跟神经网络的参数，ELBO可以写成：<br>$$<br>ELBO_i(\theta, \phi) = E_{q_{\theta}(z|x_i)}[log p_{phi}(x_i|z)]-KL(q_{\theta}(z|x_i)||p(z)).<br>$$<br>所以，$ELBO_i(\theta, \phi)=-l_i(\theta, \phi)$。</p>
<p>我们从概率模型的角度，定义了目标函数（ELBO)和inference algorithm (gradient ascent on the ELBO)。<mark>不懂inference到底是哪个过程？？</mark>在神经网络中，inference通常指给定新的之前没有遇到的点来预测隐含的表示形式；而在概率模型中，inference意味着给定观察到的点来预测隐含变量的值。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a><center><strong>Reference</strong></center></h3><p>[1] <a href="https://jaan.io/unreasonable-confusion/" target="_blank" rel="external">The unreasonable confusion of variational autoencoders.</a><br>[2] <a href="https://github.com/altosaar/vae/blob/master/vae.py" target="_blank" rel="external">代码.</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;将deep learning和graphical model结合起来理解&lt;a href=&quot;https://jaan.io/unreasonable-confusion/&quot;&gt;VAE&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
      <category term="VAE" scheme="http://cuiyungao.github.io/tags/VAE/"/>
    
  </entry>
  
  <entry>
    <title>Learning Text Representation Using Recurrent Convolutional Neural Network with Highway Layers</title>
    <link href="http://cuiyungao.github.io/2016/08/17/RCNN/"/>
    <id>http://cuiyungao.github.io/2016/08/17/RCNN/</id>
    <published>2016-08-17T09:50:55.000Z</published>
    <updated>2016-08-17T12:58:23.000Z</updated>
    
    <content type="html"><![CDATA[<p>发表在SIGIR’16上。Wen, Y., Zhang, W., Luo, R., &amp; Wang, J. (2016). Learning text representation using recurrent convolutional neural network with highway layers. arXiv preprint arXiv:1606.06905.</p>
<a id="more"></a>
<p>作者提出了<mark>新的结合RNN和CNN的方式</mark>，即在中间加了一层highway，这层的作用是把RNN的输出进行特征选择，其结果作为CNN的输入。</p>
<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a><center><strong>Motivation</strong></center></h3><p>RNN跟CNN在NLP领域非常流行，但是两者各有优劣势。RNN虽然考虑了单词的顺序，但是它的问题在于后面的单词相比于前面单词的表示形式更能影响结果，对于情感分析这样的task，重要的单词预示着正确的情感，但是可能出现在文档的任何位置，所以RNN的结果并不是很好。而CNN可以很自然地解决这个问题，因为它通过max-pooling层平等地看待所有的词；但是CNN也有自己的问题，比如需要确定window的大小以及众多的filter参数。</p>
<p>RCNN模型旨在结合这两者的优点。模型对整个文档采用bi-directional recurrent neural network，并结合词跟它的上下文来表示这个词，filter用来计算latent semantic vectors。最终，max-pooling用来获取最重要的因素，并形成固定大小的句子表示。</p>
<p>这篇文章改进已有的RCNN模型，在bi-directional RNN和CNN之间插入了一个<mark>highway network</mark>，形成<mark>RCNN-HW</mark>模型，这个highway layer的作用是作为中间层来单独选择每个词的表示形式中的features。</p>
<h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a><center><strong>Model</strong></center></h3><p>整个模型框图如下所示。RNN能够处理当前的数据实例并且保留历史信息，常用的比如LSTM和GRU，后者比较简洁且<mark>性能较好（really??）</mark>，所以文章采用GRU。表达式如下：<br>$$<br>\mathbf{r}_t = \sigma(\mathbf{W}_r\mathbf{x}_t+\mathbf{U}_r\mathbf{h}_{t-1}+\mathbf{b}_r) \\<br>\mathbf{z}_t = \sigma(\mathbf{W}_z\mathbf{x}_t+\mathbf{U}_z\mathbf{h}_{t-1}+\mathbf{b}_z) \\<br>\mathbf{\tilde{h}}_t = tanh(\mathbf{W}_h\mathbf{x}_t+\mathbf{U}_h(\mathbf{r}_t\odot\mathbf{h}_{t-1})+\mathbf{b}_h) \\<br>\mathbf{h}_t = \mathbf{z}_t\odot\mathbf{h}_{t-1} + (1-\mathbf{z}_t)\odot\mathbf{\tilde{h}}_t,<br>$$<br>其中，$\odot$表示element-wise multiplication，$\mathbf{W,U,b}$表示输入、recurrent的权重和biases。所以，单词的表示形式$\mathbf{\tilde{x}}_t$可以结合上下文得到：<br>$$<br>\mathbf{\tilde{x}}_t = [\overleftarrow{\mathbf{h}_t}||\mathbf{x}_t||\overrightarrow{\mathbf{h}_t}].<br>$$</p>
<center><img src="/img/papers/RCNN1.png" width="70%"></center>

<p><code>One-layer</code> highway network是RNN和CNN的中间层，表示如下：<br>$$<br>\mathbf{y}_t=\tau\odot g(\mathbf{W}_H\mathbf{\tilde{x}}_t+\mathbf{b}_H)+(1-\tau)\odot\mathbf{\tilde{x}}_t,<br>$$<br>其中，$g$是非线性函数，$\tau$为<code>&quot;transform gate&quot;</code>，即$\tau=\sigma(\mathbf{W}_{\tau}\mathbf{x}_t+\mathbf{b}_{\tau})$。<mark>这个highway network的设计跟GRU的<code>update gate</code>$\mathbf{z}$非常相似，</mark>其作用是一部分输入信息被不变地传递到输出，其它的信息则需要进行非线性变换。<mark>实验说明这样的设计可以获取重要的信息。</mark></p>
<p>在CNN阶段，我们将window size设置为1，因为bidirectional RNN和highway layer已经获取了每个单词的上下文信息。</p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a><center><strong>Experiments</strong></center></h3><p>文章提供了<a href="https://github.com/wenying45/deep_learning_tutorial" target="_blank" rel="external">源代码</a>。情感分析实验阶段，比较了六种方法，分别是：Sum-of-Word-Vector (COW), LSTM, Bi-LSTM, CNN, CNN+LSTM,和RCNN，结果如下图。这些网络用来学习文本的表示形式，之后用softmax进行预测。实验没有采用pre-training model (e.g., word2vec)或者regularizer (e.g., Dropout)，这些使实验结果更好。情感分类的实验结果表明，CNN的效果要好于RNN，可能是由于RNN很难被训练，并且对超参数和后面的单词比较敏感。RCNN的效果最佳，因为它综合了CNN获取局部信息的优势和RNN获取全局信息的优势。CNN-LSTM是用CNN的局部信息作为RNN的输入，但是局部信息并没有时序关系。</p>
<center><img src="/img/papers/RCNN2.png" width="70%"></center>

<p>文章还比较了不同的文本长度对实验结果的影响，发现对于短文本，这几种方法都不是很好；而对于较长的文本，RCNN的优势就体现出来，如下图所示。RCNN不光可以保留较长的文本信息，还可以通过CNN的max-pooling减少噪声；RCNN-HW则通过highway layer进一步减少噪声，进行特征选择，所以在较长输入时效果最好。</p>
<center><img src="/img/papers/RCNN3.png" width="70%"></center>

<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a><center><strong>Reference</strong></center></h3><p>[1] <a href="http://arxiv.org/pdf/1606.06905v2.pdf" target="_blank" rel="external">Learning Text Representation Using Recurrent Convolutional Neural Network with Highway Layers</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;发表在SIGIR’16上。Wen, Y., Zhang, W., Luo, R., &amp;amp; Wang, J. (2016). Learning text representation using recurrent convolutional neural network with highway layers. arXiv preprint arXiv:1606.06905.&lt;/p&gt;
    
    </summary>
    
      <category term="papers" scheme="http://cuiyungao.github.io/categories/papers/"/>
    
    
      <category term="RNN" scheme="http://cuiyungao.github.io/tags/RNN/"/>
    
      <category term="CNN" scheme="http://cuiyungao.github.io/tags/CNN/"/>
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>What to do about Ad Blocking Impact on Customer Experience?</title>
    <link href="http://cuiyungao.github.io/2016/08/17/adblock/"/>
    <id>http://cuiyungao.github.io/2016/08/17/adblock/</id>
    <published>2016-08-17T09:02:25.000Z</published>
    <updated>2016-08-17T12:52:09.000Z</updated>
    
    <content type="html"><![CDATA[<p>这是一篇总结ad blocking对用户体验影响的<a href="http://apmblog.dynatrace.com/2016/08/09/ad-blocking-impact-customer-experience/" target="_blank" rel="external">报道</a>。</p>
<a id="more"></a>
<p>用户对ad比较反感的原因，总结有三点：</p>
<ol>
<li>Ads是获取网页的速度减慢；</li>
<li>Ads干扰了用户希望浏览的内容；</li>
<li>用户担心Ads会泄露个人隐私信息。</li>
</ol>
<p>用户使用Ad blocker已经非常普遍，据报道有20%的广告被拦截，从而造成了广告收入的损失。下面分析这三个方面的根本原因。<br>第一点的原因是广告的数量，插件和第三方服务太多。对于如何测量用户体验，作者提出了用response time。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是一篇总结ad blocking对用户体验影响的&lt;a href=&quot;http://apmblog.dynatrace.com/2016/08/09/ad-blocking-impact-customer-experience/&quot;&gt;报道&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="Ad" scheme="http://cuiyungao.github.io/tags/Ad/"/>
    
  </entry>
  
  <entry>
    <title>Consensus Attention-based Neural Networks for Chinese Reading Comprehension</title>
    <link href="http://cuiyungao.github.io/2016/08/17/CAS/"/>
    <id>http://cuiyungao.github.io/2016/08/17/CAS/</id>
    <published>2016-08-17T06:51:22.000Z</published>
    <updated>2016-08-17T08:29:05.000Z</updated>
    
    <content type="html"><![CDATA[<p>Cui, Y., Liu, T., Chen, Z., Wang, S., &amp; Hu, G. (2016). Consensus Attention-based Neural Networks for Chinese Reading Comprehension. arXiv preprint arXiv:1607.02250.</p>
<a id="more"></a>
<p>这篇文章是哈工大与科大讯飞合作的作品。文章的主要贡献在于：</p>
<ol>
<li>提供了中文阅读理解的数据集，包括人民日报新闻 (People Daily news)和儿童童话 (Children’s Fairy Tale, CFT)；</li>
<li>完善了已有的attention-based NN model，提出了consensus attention-based neural network architecture，即consensus attention sum reader (CAS Reader)。</li>
</ol>
<h3 id="Problem-Definition"><a href="#Problem-Definition" class="headerlink" title="Problem Definition"></a><center><strong>Problem Definition</strong></center></h3><p><code>Cloze-style reading comprehension problem</code>致力于理解给定的上下文或者文档，回答根据文档的属性所得的问题，并且回答是文档当中存在的一个词。可以由下面的三元组表示：<br>$&lt;\mathcal{D}, \mathcal{Q}, \mathcal{A}&gt;$，<br>其中$\mathcal{D}$为文档，$\mathcal{Q}$为query，$\mathcal{A}$为针对query的回答。</p>
<p>这种问题的解决通常采用attention-based neural network的方法，但是这种方法需要<mark>大量的训练数据</mark>来训练预测的可靠模型。现有的方法是自动地生成大量的训练数据。</p>
<h3 id="Consensus-Attention-Sum-Reader"><a href="#Consensus-Attention-Sum-Reader" class="headerlink" title="Consensus Attention Sum Reader"></a><center><strong>Consensus Attention Sum Reader</strong></center></h3><p>本文的算法是基于[2]的改进，目的是直接从文档中估计答案，而不是从所有的vocabularies估计。作者发现<mark>直接将query进行RNN之后的表示形式不足以表示query的整个信息</mark>，作者利用query的每一个<code>time slices</code>，建立了不同steps之间的<code>consensus attention</code>。<mark>感觉最近是不是attention上的LSTM很火？这个可以改善一些常见的topic么？</mark></p>
<p>一般形式是，给定一组训练三元组$&lt;\mathcal{D}, \mathcal{Q}, \mathcal{A}&gt;$，首先将文档$\mathcal{D}$和query $\mathcal{Q}$的one-hot表示形式转换成共享embedding matrix $W_e$的连续的表示形式。<mark>由于query通常比文档短，通过共享embedding weights，query的表示形式可以受益于文档的表示形式</mark>，这比分开的embedding matrices有效。</p>
<p>之后，用两个不同的<code>bi-directional RNNs</code>得到文档和query的上下文表示形式。<mark>这种方法可以获取之前和之后的上下文信息。</mark>文章采用bi-directional Gated Recurrent Unit (GRU)，如下：<br>$$<br>e(x) = W_e\times x, \; where \, x\in\mathcal{D,Q} \\<br>\overrightarrow{h_s} = \overrightarrow{GRU}(e(x)) \\<br>\overleftarrow{h_s} = \overleftarrow{GRU}(e(x)) \\<br>h_s = [\overrightarrow{h_s};\overleftarrow{h_s}].<br>$$</p>
<p>文档的<code>attention</code>即一个概率分布，这里用$h_{doc}$和$h_{query}$分别表示文档和query的上下文表示形式，都是三维的张量。这两个张量的内积表示每个文档单词在时间$t$对query单词的重要性。概率分布用softmax来获得，即：<br>$$<br>\alpha(t) = softmax(h_{doc}\odot h_{query}(t)).<br>$$</p>
<p>$\alpha(t)$即文档的<code>attention</code>，且$\alpha(t)=[\alpha(t)_1,\alpha(t)_2, …, \alpha(t)_n]$，$\alpha(t)_i$指的是文档第$i$个词在时间$t$的attention值。<code>Consensus attention</code>由merging function $f$得到，即：<br>$$<br>s=f(\alpha(1), …,\alpha(m)),<br>$$<br>其中，$s$是query最终的attention，$m$是query的长度。Merging function有以下几种heuristics，即：<br>$$<br>   s\propto<br>   \begin{cases}<br>    softmax(\sum_{t=1}^m \alpha(t)),\; if \, mode=sum; \\<br>    softmax(\frac{1}{m}\sum_{t=1}^m \alpha(t)), \; if \, mode=avg; \\<br>    softmax({max}_{t=1,…,m} \alpha(t)_i), \; if \, mode=max.<br>   \end{cases}<br>$$</p>
<p>最终，将结果$s$映射到vocabulary space $V$，并将在文档中同一个词出现在不同位置的attention value相加，如下式所示。（<mark>思考：这个方式跟直接从vocabulary中选取某个词有啥区别？因为文章的一个亮点是从文档当中选取，而不是从vocabularies当中选取。</mark>）</p>
<p>$$<br>P(w|\mathcal{D,Q})=\sum_{i\in I(w,\mathcal{D})} s_i, \, w\in V.<br>$$<br>其中，$I(w,\mathcal{D})$为单词$w$出现在文档$\mathcal{D}$中的位置。整个框架图由下图所示。</p>
<center><img src="/img/daily/CAS1.png" width="100%"></center><br><center>Fig.1 Architecture of the Proposed Consensus Attention Sum Reader (CAS Reader).</center>

<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a><center><strong>Experiments</strong></center></h3><p>框架实现采用Theano和Keras，在Tesla K40 GPU上训练。实验结果在某些数据集上要优于传统的不用Consensus的模型，结果不好的原因是以前的模型只是从entity当中挑选答案，而改进之后的模型是从文档当中选取。在中文分词上的结果表明，有Consensus的Model要优于传统的attention model，结果如下图所示。</p>
<center><img src="/img/daily/CAS2.png" width="80%"></center>

<p>文章另外一个有意思的点是，<mark>机器生成的问题跟人想问的问题不同，对人的问题产生效果比较差</mark>。比如，机器问题“I went to the <em>__</em> this morning .”，而人的问题是“Where did I go this morning ?”。<mark>我比较疑惑的是，这两个问题在中文里面不是一样么？只是加了一个语气词而已，那么效果怎么会差这么大？</mark></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a><center><strong>Reference</strong></center></h3><p>[1] <a href="https://arxiv.org/abs/1607.02250" target="_blank" rel="external">Consensus Attention-based Neural Networks for Chinese Reading Comprehension</a><br>[2] <a href="https://arxiv.org/abs/1603.01547" target="_blank" rel="external">Text Understanding with the Attention Sum Reader Network</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Cui, Y., Liu, T., Chen, Z., Wang, S., &amp;amp; Hu, G. (2016). Consensus Attention-based Neural Networks for Chinese Reading Comprehension. arXiv preprint arXiv:1607.02250.&lt;/p&gt;
    
    </summary>
    
      <category term="papers" scheme="http://cuiyungao.github.io/categories/papers/"/>
    
    
      <category term="RNN" scheme="http://cuiyungao.github.io/tags/RNN/"/>
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
      <category term="Attention Model" scheme="http://cuiyungao.github.io/tags/Attention-Model/"/>
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond</title>
    <link href="http://cuiyungao.github.io/2016/08/16/arxiv0810/"/>
    <id>http://cuiyungao.github.io/2016/08/16/arxiv0810/</id>
    <published>2016-08-16T09:16:51.000Z</published>
    <updated>2016-08-17T01:21:43.000Z</updated>
    
    <content type="html"><![CDATA[<p>IBM的作品。Nallapati, R., Zhou, B., glar Gulçehre, Ç., &amp; Xiang, B. Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond.</p>
<a id="more"></a>
<p>所谓的<code>abstractive text summarization</code>就是压缩原始的文档，并保留原文档的主要概念。</p>
<h3 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a><center><strong>Contribution</strong></center></h3><p>文章将在MT (Machine Learning)中的Attentional Encoder-Decoder RNN算法应用到abstractive text summarization，并针对在abstractive text summarization中存在的三个问题（即为<mark>key-words建模，获取sentence-to-word的结构层级，和在training阶段处理少见的或者没有见过的词</mark>）对模型进行改善。</p>
<p>文章的另外一个贡献是提出了新的包含多句子总结的数据集，建立了后面research的benchmarks。</p>
<h3 id="Models"><a href="#Models" class="headerlink" title="Models"></a><center><strong>Models</strong></center></h3><p>模型包含encoder和decoder，encoder是一个bidirectional GRU-RNN，而decoder是一个跟encoder有着相同大小hidden-state size的uni-directional GRU-RNN；在source-hidden states之间是<code>attention mechanism</code>，最终由一个<code>soft-max layer</code>为目标词汇生成单词。文章的model有如下三个改进的方面：</p>
<ol>
<li>Feature-rich encoder;</li>
<li>Switching generator-pointer model;</li>
<li>Hierarchical attention model.</li>
</ol>
<p>第一个改进，为encoder的输入增加新的features，比如parts-of-speech tags, named-entity tags (NER tags),和离散后的TF以及IDF数据。</p>
<p>第二个改进，为decoder增加一个<code>switch</code>，决定用generator还是用一个指针指向文档当中的某一个位置。如果switch打开，则模型还是用以前的方法从目标词库当中生成单词；如果关闭的话，则生成一个指针指向源的某一个位置，这个位置的word就会被添加到summary当中。每一个<code>time step</code>，<code>Switch</code>被定义为基于linear layer的sigmoid函数，如下：<br>$$<br>P(s_i=1)=\sigma(\mathbf{v}^s\cdot(\mathbf{W}_h^sh_i)+\mathbf{W}_e^s\mathbf{E}[o_{i-1}]+\mathbf{W}_c^s\mathbf{c}_i+\mathbf{b}^s)),<br>$$<br>其中$P(s_i=1)$表示在第$i$个time step开关被打开的概率，$\mathbf{c}_i$是attention-weighted context vector，$<br>mathbf{W}_h^s, \mathbf{W}_e^s，\mathbf{W}_c^s, \mathbf{b}^s$和$\mathbf{v}_s$都是开关的参数。<mark>这个是不是就相当于CNN中的<code>filters</code>的概念。</mark>改进后的模型图如下图所示。</p>
<center><img src="/img/daily/arxiv08101.png" width="70%"></center>

<p>第三个改进，在encoder采用<mark>两层</mark>概念，即word level和sentence level。Attention mechanism同时作用在这两层上，即<code>word-level</code>的attention被相应的<code>sentence-level</code>的attention重新校正，定义如下：<br>$$<br>P^a(j) = \frac{P_w^a(j)P_s^a(s(j))}{\sum_{k=1}^{N_d}P_w^a(k)P_s^a(s(k))}，<br>$$<br>其中，$P_w^a(j)$是word-level第j个位置的attention weight，$P_s^a(l)$是原文件中第l个位置sentence-level的attention weight。得到的re-scaled attention被用来计算decoder的hidden state的输入（attention-weighted context vector）。而且，将positional embeddings加到sentence-level RNN的hidden state，来表示文档中句子的位置重要性。这个模型因此能够将关键句子跟关键词联系在一起。</p>
<center><img src="/img/daily/arxiv08102.png" width="70%"></center>

<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a><center><strong>Experiments</strong></center></h3><p>分了好几个实验，比较感兴趣的实验是在CNN/Daily Mail上的实验，实验比较了三个模型，实验结果如图所示。最后一个模型”temp”表示<code>Temporal Attention Model</code>，<mark>解决了entity重复出现的问题，这个可以用在tag identification上面</mark>。另外，感觉这个<code>Temporal Attention Model</code>跟LSTM类似，只不过是在attention上的LSTM。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a><center><strong>Reference</strong></center></h3><p>[1] <a href="https://aclweb.org/anthology/K/K16/K16-1028.pdf" target="_blank" rel="external">Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;IBM的作品。Nallapati, R., Zhou, B., glar Gulçehre, Ç., &amp;amp; Xiang, B. Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond.&lt;/p&gt;
    
    </summary>
    
      <category term="papers" scheme="http://cuiyungao.github.io/categories/papers/"/>
    
    
      <category term="RNN" scheme="http://cuiyungao.github.io/tags/RNN/"/>
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Enriching Word Vectors with Subword information</title>
    <link href="http://cuiyungao.github.io/2016/08/16/enrich/"/>
    <id>http://cuiyungao.github.io/2016/08/16/enrich/</id>
    <published>2016-08-16T02:06:02.000Z</published>
    <updated>2016-08-16T03:30:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章是facebook AI research作品，是fastText的基础。Bojanowski, P., Grave, E., Joulin, A., &amp; Mikolov, T. (2016). Enriching Word Vectors with Subword Information. arXiv preprint arXiv:1607.04606.</p>
<a id="more"></a>
<p>大神Tomas Mikolov的有一篇作品，借鉴参考当中的trick。</p>
<h3 id="Idea"><a href="#Idea" class="headerlink" title="Idea"></a><center><strong>Idea</strong></center></h3><p>这篇文章的主要思路是让word之间可以共享信息，即利用subword的特征。这种表示方法同样对rare words以及不同的语言有效。</p>
<h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a><center><strong>Model</strong></center></h3><p>总的框架类似于Skip-Gram Model，给定一组训练词序集$w_1, …, w_T$，目标是最大化基于$w_t$预测出$w_c$的概率，即：<br>$$<br>\sum_{t=1}^T\sum_{c\in\mathcal{C}_t} log p(w_c|w_t),<br>$$<br>其中$\mathcal{C}_t$指$w_t$周围的词的indices。给定一个scoring function $s$，将paris (word, context)映射到$\mathbb{R}$空间，那么一个context word的概率可以定义为：<br>$$<br>p(w_c|w_t)=\frac{e^{s(w_t,w_c)}}{\sum_{j=1}^W e^{s(w_t,j)}}.<br>$$<br>这个scoring function通常定义为$s(w_t,w_c)=\mathbf{u}_{w_t}^T\mathbf{v}_{w_c}$。</p>
<p>考虑一个word当中的$n$-grams $\mathcal{G}_w\subset{1,…,G}$，每个$n$-gram $g$可以表示成一个向量$z_g$，这个word可以表示成其$n$-grams的和。所以scoring function可以表示成：<br>$$<br>s(w,c)=\sum_{g\in\mathcal{G}_w} \mathbf{z}_g^T\mathbf{v}_c.<br>$$<br>为了限制memory，所有的$n$-grams被映射到1~K的整数，论文中K为2 millions，最终每个word被表示成index和$n$-grams的hash values。词的开头和结果加一个special characters，以便于找到前缀和后缀。为了提高模型效率，<mark>最频繁的$P$个单词不用$n$-grams来表示</mark>。</p>
<p>实验结果在similarity tasks上的效果不错。测量方法是模型评分和人工判断之间的Spearman’s rank correlation coefficient。实现采用C++，在运行时间上比Skip-Gram baseline慢了1.5倍，比较的方法当中Skip-Gram和CBOW由C<a href="https://code.google.com/archive/p/word2vec/" target="_blank" rel="external">实现</a>。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章是facebook AI research作品，是fastText的基础。Bojanowski, P., Grave, E., Joulin, A., &amp;amp; Mikolov, T. (2016). Enriching Word Vectors with Subword Information. arXiv preprint arXiv:1607.04606.&lt;/p&gt;
    
    </summary>
    
      <category term="papers" scheme="http://cuiyungao.github.io/categories/papers/"/>
    
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Query Understanding</title>
    <link href="http://cuiyungao.github.io/2016/08/13/query/"/>
    <id>http://cuiyungao.github.io/2016/08/13/query/</id>
    <published>2016-08-13T02:36:37.000Z</published>
    <updated>2016-08-14T08:24:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>Query understanding是很多AI应用场景的基础，比如机器人对话，车载导航等等。</p>
<a id="more"></a>
<p>Query understanding的任务有三个，即纠正用户输入query的错误，精确地引导用户以及准确地理解用户query的目的。Query通常分为两种类型，一种是general query，不需要上下文情境；另外一种是<mark>contextual questions</mark>，比如”Where can I eat cheese cake right now”。第二种query不需要返回一系列的文档，而是需要解析query的目的，用户的地理位置和时间等，提供personal的回答。另外一种分类方法是从答案的角度，一种是<mark>精确需求</mark>，直接给出结果，比如”Who is the president of USA”；第二种则是给出泛泛的回答，让用户自己选择，即<mark>Interactive Question Answering</mark>，如下图所示。</p>
<center><img src="/img/daily/qu1.png" width="100%"></center><br><center>Fig.1 Interactive Question Answering</center>

<h3 id="General-Characteristics"><a href="#General-Characteristics" class="headerlink" title="General Characteristics"></a><center><strong>General Characteristics</strong></center></h3><p>Query understanding还有一个非常有意思的应用是个人助理（比如微软Cortana，苹果Siri，Google Now，百度度秘），可以为用户提供各种服务，比如搜索，会议安排，闹铃设置，打电话，发短信，播放音乐等等。这个可以被称为<code>Deep Search</code>，即Search with a Deep Understanding，应用NLP和knowledge context (用户之前的搜索和浏览活动，兴趣爱好，地点，询问的时间以及当时的天气等等)，通常有如下特征：</p>
<ol>
<li>对query的语义理解。不同语义层次的理解如下图所示，展示了不同level的语义理解。</li>
</ol>
<center><img src="/img/daily/qu2.png" width="90%"></center><br><center>Fig.2 Matching at Different Semantic Levels</center>

<ol>
<li>对上下文和前面任务的理解。这个可以有效地理解有歧义的词汇。Fig.3显示了含有interleaved tasks的一个搜索情景。<code>Reference Query</code>表示的是当前用户的query，<code>On-Task Queries</code>表示跟<code>Reference Query</code>相同任务的query；<code>Off-Task Queries</code>表示的是跟当前query任务不同的query。</li>
</ol>
<center><img src="/img/daily/qu3.png" width="90%"></center><br><center>Fig.3 A Search Context with Interleaved Tasks</center>

<ol>
<li>用户理解和个性化。不同的用户有不同的兴趣爱好等。</li>
</ol>
<h3 id="Phases-in-Deep-Query-Understanding"><a href="#Phases-in-Deep-Query-Understanding" class="headerlink" title="Phases in Deep Query Understanding"></a><center><strong>Phases in Deep Query Understanding</strong></center></h3><p>Query understanding主要有三个过程，如Fig.4所示，包括query refinement, query suggestion和query intent detection，最终给query打tag，有了tag之后进入<code>Answer Generation Module</code>。</p>
<center><img src="/img/daily/qu4.png" width="100%"></center><br><center> Fig.4 Query Understanding Module Broad Components</center>

<p><code>Query refinement</code>主要进行spelling correction, acronym expansion, word splitting, words merger和phrase segmentation。纠正之后的query进入<code>query suggestion</code>来发现和推荐可以得到更好搜索结果的queries。在循环一次或者多次<code>query correction-&gt;query suggestion-&gt;user query</code>这个过程后，完善的query进入到<code>query expansion</code>模块，补充更多近似的queries，以减少由于字符的错误匹配产生的文章结果。这个query set之后输入到<code>query intention detection</code>模块，精确地推测query的意图。<code>Query classification</code>将query粗分类到某一个field里面，比如运动、娱乐、天气等，结果被输入到<code>semantic tagging</code>部分进行意图检测。Fig.5描述了一个例子。</p>
<center><img src="/img/daily/qu5.png" width="100%"></center><br><center>Fig.5 Example of Task Done at Each Component</center>

<p><mark>Query classification比document text classification更具有挑战性，因为query相对较短，而且很多query有歧义性。</mark>Query classification对精确地判断用户意图非常重要。对于semantic tagging，经常用的features是n-grams, regular regression, POS (Part of Speech), lexicons和transit features,<mark>总的来说，分为两种，syntactic和semantic的features</mark>。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a><center><strong>Reference</strong></center></h3><p>[1] <a href="https://arxiv.org/pdf/1505.05187.pdf" target="_blank" rel="external">Techniques for deep query understanding.</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Query understanding是很多AI应用场景的基础，比如机器人对话，车载导航等等。&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
      <category term="QU" scheme="http://cuiyungao.github.io/tags/QU/"/>
    
  </entry>
  
  <entry>
    <title>CNN总结</title>
    <link href="http://cuiyungao.github.io/2016/08/12/cnn/"/>
    <id>http://cuiyungao.github.io/2016/08/12/cnn/</id>
    <published>2016-08-12T03:00:57.000Z</published>
    <updated>2016-08-12T03:29:50.000Z</updated>
    
    <content type="html"><![CDATA[<p>已经有很多大牛对CNN的算法进行了总结，这里就对这些资料进行汇总。</p>
<a id="more"></a>
<p>CNN已经被各种大牛解说得很好了，其中觉得不是很懂的地方主要还是在back propagation。所以这篇文章主要对back propagation方面进行总结，也汇总CNN的一些重要的点。一些很好的资料也在本文进行标注。</p>
<h3 id="Main-Steps"><a href="#Main-Steps" class="headerlink" title="Main Steps"></a><center><strong>Main Steps</strong></center></h3><p>下面这幅图非常经典，是ConvNet的过程图。在CNN中有4个关键操作：</p>
<ul>
<li>Convolution</li>
<li>Non Linearlity (ReLU)</li>
<li>Pooling or Sub Sampling</li>
<li>Classification (Fully Connected Layer).</li>
</ul>
<p>Convolution这一步是一个linear的操作，主要目的是获取输入图片的特征，通过移动<code>filter</code>（也叫<code>kernel</code>，<code>feature detector</code>）保留pixels之间的空间关系。dot product的结果叫做<code>Convolved Feature</code>或者<code>Activation Map</code>或者<code>Feature Map</code>。<code>filter</code>种类的数量表示convolution之后的深度，一种<code>filter</code>得到一种layer。Feature Map的大小由三个参数决定，即Depth, Stride和Zero-Padding。</p>
<center><img src="/img/daily/cnn1.png" width="100%"></center>

<p>ReLU是对Convolution的结果进行非线性变换，是element wise operation，将所有的负值替换成0。因为现实数据通常是非线性的，所以引入ReLU这个非线性函数，也可以采用tanh或者sigmoid，但是ReLU通常情况下性能较好。</p>
<p>Pooling操作主要目的是减少特征维度，同时保留重要信息，避免overfitting，这使得对输入图片的各种变换、扭曲和移动有效。到这一步，我们可以得到输入图片的high-level的feature。最后通过fully connected layer，我们对这些feature进行分类。比如说我们要使得结果分成$k$类，通过softmax这个activation function，使得这$k$类输出概率的和是1。最后通过back propagation，我们更新迭代参数使分类结果最佳。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a><center><strong>Reference</strong></center></h3><p>[1] <a href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/" target="_blank" rel="external">An Intuitive Explanation of Convolutional Neural Networks</a><br>[2] <a href="http://andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/" target="_blank" rel="external">Backpropagation</a><br>[3] <a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="external">CS231n Convolutional Neural Networks Stanford</a><br>[4] <a href="http://blog.algorithmia.com/introduction-natural-language-processing-nlp/" target="_blank" rel="external">Introduction to Natural Language Processing 2016</a>。这里面包含了各种资料的汇总。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;已经有很多大牛对CNN的算法进行了总结，这里就对这些资料进行汇总。&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="CNN" scheme="http://cuiyungao.github.io/tags/CNN/"/>
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>SVM模型小结</title>
    <link href="http://cuiyungao.github.io/2016/08/11/svmbaidu/"/>
    <id>http://cuiyungao.github.io/2016/08/11/svmbaidu/</id>
    <published>2016-08-11T12:06:31.000Z</published>
    <updated>2016-08-12T02:00:27.000Z</updated>
    
    <content type="html"><![CDATA[<p>在百度每周的NLP课程总结，感谢磊磊哥大神。这节课主要讲的是SVM算法，讲到了其中的精髓，感觉受益匪浅，还是记下来以备复习。</p>
<a id="more"></a>
<p>SVM (Support Vector Machine)是机器学习的基础，虽然基本，但是当中延伸出来一些很有意义的idea，比如kernel。</p>
<h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a><center><strong>SVM</strong></center></h3><p>如图所示，假定在$x$空间，存在两分类的点集，标记实心的点为$x^+$，空心的点为$x^-$，$w$为与$H_3$垂直的向量，则$H_3$表示为$w^Tx+b$。<mark>注意：$w$是与分界线垂直的向量。</mark>实心和空心区域的点集可以分别表示为：<br>$$<br>w^Tx^++b\geq 1 \\<br>w^Tx^-+b\leq -1.<br>$$<br>这里的1代表不确定常量（常量在不等式左右两边同除，结果为1）。</p>
<center><img src="/img/daily/svm1.png" width="60%"></center>

<p>现在我们想找到SVM的目标函数。我们的主要目的是maximize两个集合之间的距离，所以取在边界上的两个集合中的点，即$x^+$和$x^-$（<mark>注意：边界上的点即为support vector</mark>），则主要目的转变成maximize这两个点的距离，公式如下：<br>$$<br>\begin{cases}<br>w^Tx^+ + b = 1 \\<br>w^Tx^- + b = -1 \\<br>x^+ - x^- = \lambda.<br>\end{cases}<br>$$<br>在第三个式子左右两边同乘$w^T$，得到$\lambda = \frac{2}{w^Tw}$。所以，$||x^+ - x^-||=\frac{2}{w^Tw}\cdot\sqrt{w^Tw}=\frac{2}{\sqrt{w^Tw}}$，我们想要maximize这个margin值，即minimize这个margin值的倒数。即目标函数为：<br>$$<br>min \; \frac{1}{2}w^Tw \\<br>s.t. \; y_i(w^Tx_i+b) \geq 1, \; \forall i.<br>$$</p>
<p>其中$i$表示所有样本。这是<mark>primal</mark>的表示形式，我们现在用拉格朗日求其<mark>dual</mark>，则有：<br>$$<br>L(w,b,\lambda) = \frac{1}{2}w^Tw+\sum_i \lambda_i [1-y_i(w^Tx_i+b)], \\<br>s.t. \; \sum_i\lambda_i \geq 0.<br>$$<br>所以有，$\frac{\partial L}{\partial w} = w-\sum_i \lambda_iy_ix_i=0$，得到$w=\sum_i\lambda_iy_ix_i$。同理，我们对$b$进行求导，有$\frac{\partial L}{\partial b} = \sum_i\lambda_iy_i=0$。所以化简上面的公式，我们有：<br>$$<br>H({\lambda_i}) = -\frac{1}{2}(\sum_i\lambda_iy_ix_i)^T(\sum_j\lambda_jy_jx_j) + \sum_i\lambda_i, \\<br>s.t. \; \sum_i\lambda_i\geq 0.<br>$$</p>
<p>我们把这个式子写成向量形式，即$H=-\frac{1}{2}\overrightarrow{\lambda}^TK\overrightarrow{\lambda}+\overrightarrow{1}\lambda$，其中$K=(x_iy_i)^T(x_jy_j)$。<mark>注意：由定义可以看出，$\lambda$在不是边界上的点是为0，所以只有边界上的点定义了$w$。</mark>这里就引出来一个非常重要的概念<mark>kernel</mark>，什么样的矩阵可以写成kernel形式呢？<mark><a href="https://en.wikipedia.org/wiki/Mercer%27s_theorem" target="_blank" rel="external">Mercer’s theorem</a></mark>对其进行了阐述。简单来说，就是一个对称的半正定的矩阵就可以写成kernel的表示形式，即symetric, PSD matrix $\implies$ kernel ($x\to\phi(x)$)。这里的kernel有三种形式：</p>
<ul>
<li>linear</li>
<li>polynomial</li>
<li>RBF (类似Gaussian).</li>
</ul>
<p>最后一个RBF相当于映射到无穷维的空间$\phi(x)$，表示形式是$K(x_i, x_j)=\lambda exp(-\frac{||x_i-x_j||_2^2}{\sigma^2})$。总的kernel形式可以表示为$K(i,j)=\phi(x_i)^T\phi(x_j)$。</p>
<p>对于SVM，我们可以将目标函数引入一个slack variable $\zeta_i$，则目标函数为：<br>$$<br>min \; \frac{1}{2}w^Tw+c\sum_i\zeta_i \\<br>s.t. \; y_i(w^Tx_i+b)\geq 1-\zeta_i, \; \forall_i \zeta_i\geq 0.<br>$$</p>
<h3 id="SVR"><a href="#SVR" class="headerlink" title="SVR"></a><center><strong>SVR</strong></center></h3><p>SVR即support vector regression，目的跟regression一样，即用一条线拟合一堆点。其目标函数为：<br>$$<br>min \; \frac{1}{2}w^Tw \\<br>s.t. \; w^Tx_i + b -y \leq \epsilon, \; w^Tx_i+b-y\geq -\epsilon.<br>$$<br>推导过程同SVM。</p>
<h3 id="SVC"><a href="#SVC" class="headerlink" title="SVC"></a><center><strong>SVC</strong></center></h3><p>SVC即support vector clustering，目的是将点集聚类。它将点的类想象成不同的小山谷，分类即找出山谷的分布$P(x)$，投影到高维空间后，点集分布在一个半径为$R$的球内，球外的点即outliers，点集这时可以用一个高斯分布来描述。</p>
<p><center><img src="/img/daily/svm2.png"></center></p>
<p><center><img src="/img/daily/svm3.png"></center><br>目标函数为：<br>$$<br>min \; R^2 \\<br>s.t. \; ||\phi(x_i)-\mu||_2^2 \leq R^2.<br>$$<br>其中，$\mu=\frac{1}{N}\sum_i\phi(x_i)$。要注意两点：</p>
<ol>
<li>$dist(\hat{x}, \mu)$可以写成kernel的形式，即$\phi(x_i)^T\phi(x_i)$;</li>
<li>$R$的设置可以进行分类。</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在百度每周的NLP课程总结，感谢磊磊哥大神。这节课主要讲的是SVM算法，讲到了其中的精髓，感觉受益匪浅，还是记下来以备复习。&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="SVM" scheme="http://cuiyungao.github.io/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>fastText与Word2Vec之间的比较</title>
    <link href="http://cuiyungao.github.io/2016/08/09/fastvsword/"/>
    <id>http://cuiyungao.github.io/2016/08/09/fastvsword/</id>
    <published>2016-08-09T01:29:03.000Z</published>
    <updated>2016-08-09T02:08:42.000Z</updated>
    
    <content type="html"><![CDATA[<p>本篇文章总结了试验fastText跟Word2Vec在embedding上的performance，源于<a href="http://nbviewer.jupyter.org/github/jayantj/gensim/blob/683720515165a332baed8a2a46b6711cefd2d739/docs/notebooks/Word2Vec_FastText_Comparison.ipynb" target="_blank" rel="external">这篇文章</a>。</p>
<a id="more"></a>
<h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a><center><strong>Dataset</strong></center></h3><p>训练embedding的数据集有两个，一个是<code>text8</code> corpus，一个是nltk自带的<code>brown</code> corpus。Groundtruth是<code>questtions-words</code>文本，可以从<a href="https://raw.githubusercontent.com/arfon/word2vec/master/questions-words.txt" target="_blank" rel="external">这里</a>下载。</p>
<h4 id="text8-Corpus-Download"><a href="#text8-Corpus-Download" class="headerlink" title="text8 Corpus Download"></a>text8 Corpus Download</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">wget http://mattmahoney.net/dc/text8.zip</div></pre></td></tr></table></figure>
<h4 id="brown-Corpus-Download"><a href="#brown-Corpus-Download" class="headerlink" title="brown Corpus Download"></a>brown Corpus Download</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">import nltk</div><div class="line"># 从当中选择brown corpus进行下载</div><div class="line">nltk.download()</div><div class="line"></div><div class="line"># Generate brown corpus text file</div><div class="line">with open(&apos;brown_corp.txt&apos;, &apos;w+&apos;) as f:</div><div class="line">    for word in nltk.corpus.brown.words():</div><div class="line">        f.write(&apos;&#123;word&#125; &apos;.format(word=word))</div></pre></td></tr></table></figure>
<h3 id="Model-Training"><a href="#Model-Training" class="headerlink" title="Model Training"></a><center><strong>Model Training</strong></center></h3><p>用fastText和Word2Vec分别对上述两个数据集进行训练，得到word embeddings。</p>
<h4 id="fastText-Training"><a href="#fastText-Training" class="headerlink" title="fastText Training"></a>fastText Training</h4><p>下载<a href="https://github.com/facebookresearch/fastText" target="_blank" rel="external">fastText</a>源码，对上述两个数据集进行训练。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">./fasttext skipgram -input brown_corp.txt -output brown_ft</div><div class="line">./fasttext skipgram -input text8 -output text8_ft</div></pre></td></tr></table></figure></p>
<h4 id="Word2Vec-Training"><a href="#Word2Vec-Training" class="headerlink" title="Word2Vec Training"></a>Word2Vec Training</h4><p>Word2Vec的训练基于gensim，采用<code>logging</code>来对过程进行输出。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">from nltk.corpus import brown</div><div class="line">from gensim.models import Word2Vec</div><div class="line">from gensim.models.word2vec import Text8Corpus</div><div class="line">import logging</div><div class="line"></div><div class="line">logging.basicConfig(format=&apos;%(asctime)s : %(levelname)s : %(message)s&apos;)</div><div class="line">logging.root.setLevel(level=logging.INFO)</div><div class="line"></div><div class="line">MODELS_DIR = &apos;models/&apos;</div><div class="line"></div><div class="line">brown_gs = Word2Vec(brown.sents())</div><div class="line">brown_gs.save_word2vec_format(MODELS_DIR + &apos;brown_gs.vec&apos;)</div><div class="line"></div><div class="line">text8_gs = Word2Vec(Text8Corpus(&apos;text8&apos;))</div><div class="line">text8_gs.save_word2vec_format(MODELS_DIR + &apos;text8_gs.vec&apos;)</div></pre></td></tr></table></figure></p>
<h3 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a><center><strong>Comparison</strong></center></h3><p>用<code>questions-words.txt</code>提供的数据作为Groundtruth，从<code>semantic</code>和<code>syntactic</code>两方面来对两种embedding的方法进行比较。</p>
<h4 id="Based-on-Brown-Corpus"><a href="#Based-on-Brown-Corpus" class="headerlink" title="Based on Brown Corpus"></a>Based on Brown Corpus</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">from gensim.models import Word2Vec</div><div class="line"></div><div class="line">def print_accuracy(model, questions_file):</div><div class="line">    print(&apos;Evaluating...\n&apos;)</div><div class="line">    acc = model.accuracy(questions_file)</div><div class="line">    for section in acc:</div><div class="line">        correct = len(section[&apos;correct&apos;])</div><div class="line">        total = len(section[&apos;correct&apos;]) + len(section[&apos;incorrect&apos;])</div><div class="line">        total = total if total else 1</div><div class="line">        accuracy = 100*float(correct)/total</div><div class="line">        print(&apos;&#123;:d&#125;/&#123;:d&#125;, &#123;:.2f&#125;%, Section: &#123;:s&#125;&apos;.format(correct, total, accuracy, section[&apos;section&apos;]))</div><div class="line">    sem_correct = sum((len(acc[i][&apos;correct&apos;]) for i in range(5)))</div><div class="line">    sem_total = sum((len(acc[i][&apos;correct&apos;]) + len(acc[i][&apos;incorrect&apos;])) for i in range(5))</div><div class="line">    print(&apos;\nSemantic: &#123;:d&#125;/&#123;:d&#125;, Accuracy: &#123;:.2f&#125;%&apos;.format(sem_correct, sem_total, 100*float(sem_correct)/sem_total))</div><div class="line"></div><div class="line">    syn_correct = sum((len(acc[i][&apos;correct&apos;]) for i in range(5, len(acc)-1)))</div><div class="line">    syn_total = sum((len(acc[i][&apos;correct&apos;]) + len(acc[i][&apos;incorrect&apos;])) for i in range(5,len(acc)-1))</div><div class="line">    print(&apos;Syntactic: &#123;:d&#125;/&#123;:d&#125;, Accuracy: &#123;:.2f&#125;%\n&apos;.format(syn_correct, syn_total, 100*float(syn_correct)/syn_total))</div><div class="line"></div><div class="line">MODELS_DIR = &apos;models/&apos;</div><div class="line"></div><div class="line">word_analogies_file = &apos;questions-words.txt&apos;</div><div class="line">print(&apos;\nLoading FastText embeddings&apos;)</div><div class="line">ft_model = Word2Vec.load_word2vec_format(MODELS_DIR + &apos;brown_ft.vec&apos;)</div><div class="line">print(&apos;Accuracy for FastText:&apos;)</div><div class="line">print_accuracy(ft_model, word_analogies_file)</div><div class="line"></div><div class="line">print(&apos;\nLoading Gensim embeddings&apos;)</div><div class="line">gs_model = Word2Vec.load_word2vec_format(MODELS_DIR + &apos;brown_gs.vec&apos;)</div><div class="line">print(&apos;Accuracy for word2vec:&apos;)</div><div class="line">print_accuracy(gs_model, word_analogies_file)</div></pre></td></tr></table></figure>
<p>结果如下：</p>
<blockquote><br>Loading FastText embeddings<br>Accuracy for FastText:<br>Evaluating…<br><br>0/1, 0.00%, Section: capital-common-countries<br>0/1, 0.00%, Section: capital-world<br>0/1, 0.00%, Section: currency<br>0/1, 0.00%, Section: city-in-state<br>36/182, 19.78%, Section: family<br>498/702, 70.94%, Section: gram1-adjective-to-adverb<br>110/132, 83.33%, Section: gram2-opposite<br>675/1056, 63.92%, Section: gram3-comparative<br>140/210, 66.67%, Section: gram4-superlative<br>426/650, 65.54%, Section: gram5-present-participle<br>0/1, 0.00%, Section: gram6-nationality-adjective<br>153/1260, 12.14%, Section: gram7-past-tense<br>318/552, 57.61%, Section: gram8-plural<br>245/342, 71.64%, Section: gram9-plural-verbs<br>2601/5086, 51.14%, Section: total<br><br>Semantic: 36/182, Accuracy: <mark>19.78%</mark><br>Syntactic: 2565/4904, Accuracy: <mark>52.30%</mark><br><br><br>Loading Gensim embeddings<br>Accuracy for word2vec:<br>Evaluating…<br><br>0/1, 0.00%, Section: capital-common-countries<br>0/1, 0.00%, Section: capital-world<br>0/1, 0.00%, Section: currency<br>0/1, 0.00%, Section: city-in-state<br>54/182, 29.67%, Section: family<br>8/702, 1.14%, Section: gram1-adjective-to-adverb<br>0/132, 0.00%, Section: gram2-opposite<br>72/1056, 6.82%, Section: gram3-comparative<br>0/210, 0.00%, Section: gram4-superlative<br>14/650, 2.15%, Section: gram5-present-participle<br>0/1, 0.00%, Section: gram6-nationality-adjective<br>28/1260, 2.22%, Section: gram7-past-tense<br>4/552, 0.72%, Section: gram8-plural<br>8/342, 2.34%, Section: gram9-plural-verbs<br>188/5086, 3.70%, Section: total<br><br>Semantic: 54/182, Accuracy: <mark>29.67%</mark><br>Syntactic: 134/4904, Accuracy: <mark>2.73%</mark><br></blockquote>

<p>从运行结果可以看到，fastText的semantic accuracy比Word2Vec要稍微差一点儿，但是Syntactic accuracy的效果明显优于Word2Vec。这是因为<a href="https://arxiv.org/abs/1607.04606" target="_blank" rel="external">1</a>中提到，fastText中word embeddings是由他们的n-gram embeddings来表示，所以形态上相似的词的embeddings也会比较类似。比如：<br>$$<br>embedding(amazing)-embedding(amazingly) = embedding(calm)-embedding(calmly).<br>$$</p>
<h4 id="Based-on-text8-Corpus"><a href="#Based-on-text8-Corpus" class="headerlink" title="Based on text8 Corpus"></a>Based on text8 Corpus</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">print(&apos;Loading FastText embeddings&apos;)</div><div class="line">ft_model = Word2Vec.load_word2vec_format(MODELS_DIR + &apos;text8_ft.vec&apos;)</div><div class="line">print(&apos;Accuracy for FastText:&apos;)</div><div class="line">print_accuracy(ft_model, word_analogies_file)</div><div class="line"></div><div class="line">print(&apos;Loading Gensim embeddings&apos;)</div><div class="line">gs_model = Word2Vec.load_word2vec_format(MODELS_DIR + &apos;text8_gs.vec&apos;)</div><div class="line">print(&apos;Accuracy for word2vec:&apos;)</div><div class="line">print_accuracy(gs_model, word_analogies_file)</div></pre></td></tr></table></figure>
<p>结果如下：</p>
<blockquote><br>Loading FastText embeddings<br>Accuracy for FastText:<br>Evaluating…<br><br>322/506, 63.64%, Section: capital-common-countries<br>609/1452, 41.94%, Section: capital-world<br>36/268, 13.43%, Section: currency<br>286/1520, 18.82%, Section: city-in-state<br>134/306, 43.79%, Section: family<br>556/756, 73.54%, Section: gram1-adjective-to-adverb<br>186/306, 60.78%, Section: gram2-opposite<br>838/1260, 66.51%, Section: gram3-comparative<br>270/506, 53.36%, Section: gram4-superlative<br>556/992, 56.05%, Section: gram5-present-participle<br>1293/1371, 94.31%, Section: gram6-nationality-adjective<br>490/1332, 36.79%, Section: gram7-past-tense<br>888/992, 89.52%, Section: gram8-plural<br>365/650, 56.15%, Section: gram9-plural-verbs<br>6829/12217, 55.90%, Section: total<br><br>Semantic: 1387/4052, Accuracy: <mark>34.23%</mark><br>Syntactic: 5442/8165, Accuracy: <mark>66.65%</mark><br><br>Loading Gensim embeddings<br>Accuracy for word2vec:<br>Evaluating…<br><br>153/506, 30.24%, Section: capital-common-countries<br>248/1452, 17.08%, Section: capital-world<br>27/268, 10.07%, Section: currency<br>172/1571, 10.95%, Section: city-in-state<br>218/306, 71.24%, Section: family<br>88/756, 11.64%, Section: gram1-adjective-to-adverb<br>45/306, 14.71%, Section: gram2-opposite<br>716/1260, 56.83%, Section: gram3-comparative<br>179/506, 35.38%, Section: gram4-superlative<br>325/992, 32.76%, Section: gram5-present-participle<br>702/1371, 51.20%, Section: gram6-nationality-adjective<br>343/1332, 25.75%, Section: gram7-past-tense<br>401/992, 40.42%, Section: gram8-plural<br>219/650, 33.69%, Section: gram9-plural-verbs<br>3836/12268, 31.27%, Section: total<br><br>Semantic: 818/4103, Accuracy: <mark>19.94%</mark><br>Syntactic: 3018/8165, Accuracy: <mark>36.96%</mark><br></blockquote>

<p>实验结果可以看出，用在较大的数据集上，fastText的优势表现得更加明显，当然word2vec的Syntactic accuracy提高得也比较明显。所以总的来看，<mark>fastText比word2vec在word embedding上更好，特别是对于syntactic information。</mark></p>
<h4 id="实验中用到的Hyperparameters"><a href="#实验中用到的Hyperparameters" class="headerlink" title="实验中用到的Hyperparameters"></a>实验中用到的Hyperparameters</h4><p>Gensim word2vec和fastText用了相似的参数，dim_size = 100, window_size = 5, num_epochs = 5。但是它们的模型完全不同，尽管有很多相似性。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a><center><strong>Reference</strong></center></h3><p>[1]<a href="https://arxiv.org/abs/1607.04606" target="_blank" rel="external">Enriching Word Vectors with Subword Information</a><br>[2]<a href="https://arxiv.org/pdf/1301.3781v3.pdf" target="_blank" rel="external">Efficient Estimation of Word Representations in Vector Space</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇文章总结了试验fastText跟Word2Vec在embedding上的performance，源于&lt;a href=&quot;http://nbviewer.jupyter.org/github/jayantj/gensim/blob/683720515165a332baed8a2a46b6711cefd2d739/docs/notebooks/Word2Vec_FastText_Comparison.ipynb&quot;&gt;这篇文章&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
      <category term="Word Embedding" scheme="http://cuiyungao.github.io/tags/Word-Embedding/"/>
    
      <category term="fastText" scheme="http://cuiyungao.github.io/tags/fastText/"/>
    
  </entry>
  
  <entry>
    <title>用户在线广告点击行为的预测</title>
    <link href="http://cuiyungao.github.io/2016/08/04/adclick/"/>
    <id>http://cuiyungao.github.io/2016/08/04/adclick/</id>
    <published>2016-08-04T05:46:40.000Z</published>
    <updated>2016-08-17T09:21:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>本篇是对ad点击行为进行预测的总结，包括张伟楠在携程技术中心的一个<a href="http://mp.weixin.qq.com/s?__biz=MzAwNDI4ODcxNA==&amp;mid=2652243946&amp;idx=2&amp;sn=de8bb9a36fe9f2f0c97f30aaec87063f&amp;scene=2&amp;srcid=0727O4JXBdr5NU7sFOD48J69&amp;from=timeline&amp;isappinstalled=0#wechat_redirect" target="_blank" rel="external">演讲</a>以及北大硕士Kintocai的<a href="http://www.52cs.org/?p=1046" target="_blank" rel="external">总结</a>。集中于深度学习在Multi-field Categorical这类数据集上的应用。</p>
<a id="more"></a>
<p>总结了FM和FNN算法在处理多值分类数据方面的优势，并把这两种算法与神经网络在特征变量处理方面的差异做了对比，最后通过用户在线广告点击行为预测比较了LR、FM、FNN、CCPM、PNN-I等算法的效果。</p>
<h3 id="Multi-field-Categorical-Data"><a href="#Multi-field-Categorical-Data" class="headerlink" title="Multi-field Categorical Data"></a><center><strong>Multi-field Categorical Data</strong></center></h3><p>目前深度学习主要用在连续的数据集上，比如视觉、语音识别和自然语言处理上。但是用户点击率预测这种课题是一个离散的数据<code>Multi-field Categorical Data</code>，会有多种不同的字段，比如:[Weekday=Wednesday, Gender=Male, City=Shanghai, …]，我们想要<mark>识别这些特征之间的关系</mark>。传统的做法是应用<code>One-Hot Binary</code>的编码方式去处理这类数据，比如Weekday有7个取值，我们就将其编译为7维的二进制向量，其中只有Wednesday是1，其它都是0，因为它只有一个特征值；Gender只有两维，其中一维是1；如果有一万个城市的话，那City就有一万维，只有Shanghai取值为1，其它是0。</p>
<center><img src="/img/daily/adclick1.jpg" width="60%"></center>

<p>最终会得到一个高维稀疏向量。这个数据集不能直接用神经网络训练，因为如果直接用One-Hot Binary进行编码的话，那输入特征至少有一百万，第一层至少需要500个节点，那么第一层我们就需要训练5亿个参数，那就需要20亿或者50亿的数据集，而获得如此大的数据集比较困难。</p>
<h3 id="FM-FNN以及PNN模型"><a href="#FM-FNN以及PNN模型" class="headerlink" title="FM, FNN以及PNN模型"></a><center><strong>FM, FNN以及PNN模型</strong></center></h3><p>作者将FM跟Neural Network结合在一起，改进了之前的NN模型。FM (Factorization Machine)被认为是最有效的<code>embedding model</code>：</p>
<center><img src="/img/daily/adclick2.jpg" width="60%"></center>

<p>第一部分仍然是Logistic Regression，第二部分是特征之间跟目标变量之间的关系（通过两两向量之间的点积，点积大于0，表示这两个特征的组合跟目标值是正相关的）。这种算法在推荐系统领域应用比较广泛。作者用FM算法对底层输入field的one-hot binary编码进行embedding，把稀疏的二进制特征向量映射到<code>dense real</code>层，之后再把<code>dense real</code>层作为输入变量进行建模，这样就避免了高维二进制输入数据的计算复杂度。模型图如下：</p>
<center><img src="/img/daily/adclick3.jpg" width="80%"></center>

<p><mark>FNN跟一般的NN算法的区别是：</mark>大部分神经网络模型对向量之间的处理采用的是<mark>加法</mark>操作，相当于逻辑“或”；而FM则是通过向量之间的<mark>乘法</mark>来衡量两者之间的关系，这就相当于逻辑“且”。显然“且”比“或”更能严格区分目标变量。如下图所示。在第二层对向量的乘积处理中（比如上图蓝色节点直接为两个向量乘积，其连接边上没有参数需要学习），每一个field都只会被映射到一个low-dimensional vector，且field和field之间没有相互影响。</p>
<center><img src="/img/daily/adclick4.jpg" width="80%"></center>

<p>乘法关系的建模，可以采用内积或者外积，如下图。外积得到的矩阵中，只有对角线有值，即内积的结果，所以<mark>内积操作可以看做是外积操作的一种特殊情况</mark>。</p>
<center><img src="/img/daily/adclick5.jpg" width="80%"></center>

<p>PNN的神经网络图如下所示。进行embedding之后的输入数据有两种处理方法，一种是对该层的任意两个feature进行内积或者外积处理得到上图的蓝色节点，另外一种是把这些feature直接和1相乘复制到上一层的Z中，然后Z和P接在一起作为神经网络的输入层。</p>
<center><img src="/img/daily/adclick6.jpg" width="80%"></center>

<p><mark>对特征进行内积或者外积处理会产生一个复杂度的问题</mark>：weight矩阵比较庞大。解决方法：由于weight矩阵是个对称矩阵，可以用factorization来处理这个对称阵，把它转换成矩阵乘矩阵的转置，这样就会减少需要训练的参数，如下图所示。</p>
<center><img src="/img/daily/adclick7.jpg" width="60%"></center>

<h3 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a><center><strong>Metrics</strong></center></h3><p>评估模型主要看以下几个指标：</p>
<ul>
<li>Area under ROC curve (AUC)</li>
<li>Log loss</li>
<li>Root mean squared error (RMSE)</li>
<li>Relative Information Gain (RIG)</li>
</ul>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a><center><strong>Experiments</strong></center></h3><p>实验比较了dropout、hidden layer的层数、迭代次数、不同层级节点分布形态以及不同的Activation Function的效果。</p>
<h3 id="Summary-of-PNN"><a href="#Summary-of-PNN" class="headerlink" title="Summary of PNN"></a><center><strong>Summary of PNN</strong></center></h3><ol>
<li>深度学习在Multi-field的数据集上也有显著的应用效果；</li>
<li>通过外积和内积找到特征之间的相关关系；</li>
<li>在广告点击率的预测中，PNN效果优于其他模型。</li>
</ol>
<h3 id="Wide-and-Deep-Learning"><a href="#Wide-and-Deep-Learning" class="headerlink" title="Wide and Deep Learning"></a><center><strong>Wide and Deep Learning</strong></center></h3><p>这个<a href="https://arxiv.org/abs/1606.07792" target="_blank" rel="external">模型</a>结合了离散LR以及DNN的方法，category feature进行embedding输入到DNN，其它特征通过LR学习。LR是feature interaction，比较细粒度；而DNN则强调generalization，可以结合这两者的优势。把position bias特征放在最后一个隐层很有意义，可以避免一些不适合结合的features。</p>
<center><img src="/img/daily/adclick8.png"></center>

<h3 id="DNN-CTR-Prediction"><a href="#DNN-CTR-Prediction" class="headerlink" title="DNN CTR Prediction"></a><center><strong>DNN CTR Prediction</strong></center></h3><p>通过embedding的方式把离散特征转化为Dense Feature， 输入层同时可以增加其他Dense 特征， 比如CTR统计特征、similarity 特征、pv、click等等。每一个隐层都可以增加新的特征输入， 尤其是最后一个隐层。</p>
<center><img src="/img/daily/adclick9.png"></center>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇是对ad点击行为进行预测的总结，包括张伟楠在携程技术中心的一个&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzAwNDI4ODcxNA==&amp;amp;mid=2652243946&amp;amp;idx=2&amp;amp;sn=de8bb9a36fe9f2f0c97f30aaec87063f&amp;amp;scene=2&amp;amp;srcid=0727O4JXBdr5NU7sFOD48J69&amp;amp;from=timeline&amp;amp;isappinstalled=0#wechat_redirect&quot;&gt;演讲&lt;/a&gt;以及北大硕士Kintocai的&lt;a href=&quot;http://www.52cs.org/?p=1046&quot;&gt;总结&lt;/a&gt;。集中于深度学习在Multi-field Categorical这类数据集上的应用。&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
      <category term="Ad" scheme="http://cuiyungao.github.io/tags/Ad/"/>
    
  </entry>
  
  <entry>
    <title>Tweet2Vec:Learning Tweet Embeddings Using Character-Level CNN-LSTM Encoder-Decoder</title>
    <link href="http://cuiyungao.github.io/2016/08/03/tweet2vec/"/>
    <id>http://cuiyungao.github.io/2016/08/03/tweet2vec/</id>
    <published>2016-08-03T12:41:08.000Z</published>
    <updated>2016-08-03T14:15:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>SIGIR’16的文章，来自MIT Media Lab。Vosoughi, Soroush, Prashanth Vijayaraghavan, and Deb Roy. “Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM Encoder-Decoder.”</p>
<a id="more"></a>
<h3 id="Major-Idea"><a href="#Major-Idea" class="headerlink" title="Major Idea"></a><center><strong>Major Idea</strong></center></h3><p>作者利用tweet用户的hashtag作为groundtruth，基于Character-level对每一条的embeddings进行学习，对hashtag进行推测，并与已有的word2vec跟Glove进行对比，发现Tweet2Vec的准确率最高。</p>
<h3 id="Techniques"><a href="#Techniques" class="headerlink" title="Techniques"></a><center><strong>Techniques</strong></center></h3><p>基于Character-level，以及作者曾经提出Bi-directional gated recurrent unit (Bi-GRU)’14来学习tweet的表示形式。</p>
<h4 id="Character-level-CNN-Tweet-Model"><a href="#Character-level-CNN-Tweet-Model" class="headerlink" title="Character-level CNN Tweet Model"></a>Character-level CNN Tweet Model</h4><p>作者采用temporal convolutional和temporal max-pooling operations，即计算一维的输入和输出之间的convolution和pooling函数。给定一个离散的输入函数$f(x)\in [1,l]\mapsto \mathbb{R}$，一个离散的kernel函数$k(x)\in [1,m]\mapsto \mathbb{R}$，stride $s$，$k(x)$和$f(x)$之间的卷积$g(y)\in [1,(l-m+1)/s]\mathbb{R}$, 对$f(x)$的pooling操作$h(y)\in [1, (l-m+1)/s]\mapsto \mathbb{R}$：<br>$$<br>g(y) = \sum_{x=1}^m k(x)\cdot f(y\cdot s - x + c) \\<br>h(y) = max_{x=1}^m f(y\cdot s - x + c)<br>$$<br>其中，$c=m-s+1$是一个补偿常量。tweet的character set包含英语字母，数字，特殊字符和不明确的字符，总共统计了70个字符。每个字符被encode成一个one-hot vector $x_i\in {0,1}^{70}$，单个tweet的最大长度是150，所以每个tweet被表示成了一个binary matrix $x_{1…150}\in {0,1}^{150\times70}$。表示成matix的tweet输入到一个包含4层一维卷基层的deep model。每个卷基层操作采用filter $w\in \mathbb{R}^l$来获取n-gram的character feature。一般来说，对于一个tweet $s$，在层$h$的一个特征$c_i$由下面式子生成：<br>$$<br>c_i^{(h)} (s) = g(w^{(h)}\cdot \hat{c}_i^{(h-1)} + b^{(h)})<br>$$<br>其中，$\hat{c}_i^{(0)}=x_{i…i+l-1}$, $b^{(h)}\in \mathbb{R}$是h层的bias，$g$是一个rectified linear unit。整体框架如下图所示：</p>
<center><img src="/img/papers/tweet2vec.png"></center>

<h4 id="Long-Short-Term-Memory-LSTM"><a href="#Long-Short-Term-Memory-LSTM" class="headerlink" title="Long-Short Term Memory (LSTM)"></a>Long-Short Term Memory (LSTM)</h4><p>给定一个输入序列$X=(x_1, x_2, …, x_N)$，LSTM计算hidden vector sequence $h=(h_1, h_2, …, h_N)$和output vector sequence $Y=(y_1, y_2, …, y_N)$。每一个时间步骤，一个模块的输出是由一组gates (前一个hidden state $h_{t1}$组成的函数)和当前时间步骤的输入控制的。forget gate $f_t$，input gate $i_t$和output gate $o_t$。这些gates集中决定了当前memory cess $c_t$的过渡和当前的hidden state $h_t$。LSTM的过渡函数定义如下：<br>$$<br>i_t = \sigma(W_i\cdot [h_{t-1}, x_t]+b_i) \\<br>f_t = \sigma(W_f\cdot [h_{t-1}, x_t]+b_f) \\<br>l_t = tanh(W_l\cdot [h_{t-1}, x_t]+b_l) \\<br>o_t = \sigma(W_o\cdot [h_{t-1}, x_t]+b_o) \\<br>c_t = f_t\odot c_{t-1} + i_t\odot l_t \\<br>h_t = o_t\odot tanh(c_t)<br>$$<br>其中，$\odot$表示component-wise multiplicaiton。$f_t$控制过去的memory cell要舍弃的信息，而$i_t$控制新的信息储存在current memory cell的程度，$o_t$是基于memory cell $c_t$的输出。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">LSTM是学习`long-term denpendencies`，所以在卷基层之后用LSTM学习获取特征的序列中存在的依赖。</div></pre></td></tr></table></figure></p>
<p>在seq-to-seq中，LSTM定义了output上的分布，之后用softmax来序列预测tokens。<br>$$<br>P(Y|X)=\prod_{t\in [1,N]} \frac{exp(g(h_{t-1}, y_t))}{\sum_{y^{\prime} exp(g(h_{t1}, y_t^{\prime}))}}<br>$$<br>其中，g是activation function。</p>
<h4 id="Combined-Model"><a href="#Combined-Model" class="headerlink" title="Combined Model"></a>Combined Model</h4><p>Figure 1呈现了整个encoder-decoder过程。输入是由matrix呈现的tweet，字符由one-hot vector表示。<br><mark>Encoder部分</mark>，在Character-level CNN的较高层卷积之后不经过pooling，直接作为LSTM的输入，encoder过程可以表示为：<br>$$<br>H^{conv} = CharCNN(T) \\<br>h_t = LSTM(g_t, h_{t-1}))<br>$$<br>其中，g=H^{conv}是一个特征矩阵，每一行代表LSTM的一个time step，$h_t$是$t$时刻的hidden representation。LSTM作用于$H^{conv}$的每一层，生成下一个序列的embedding。最终输出结果$enc_N$用来表示整条tweet。</p>
<p><mark>Decoder部分</mark>，用两层的LSTM作用于encoded representation。每一时间步骤中，字符的预测是：<br>$$<br>P(C_t|\cdot) = softmax(T_t, h_{t-1})<br>$$<br>其中，$C_t$指的是时间$t$的字符，$T_t$表示时刻$t$的one-hot vector。最终的结果是一个decoded tweet matrix $T^{dec}$，与实际的tweet进行比较，并学习模型的参数。</p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>实验用于两个分类任务： tweet semantic relatedness和tweet sentiment classification。3 million tweets。感觉数据量也挺小。准确率在0.6~0.7左右。</p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p><mark>Character level的deep learning的优势:</mark>占用内存少，不需要存储所有word的表示，只需要存储已有的character；不依赖于语言，只需要字符；而且不需要NLP的预处理，比如word segmentation。<br><mark>劣势是：</mark>运行速度慢，在文章的实验中，word2vec的速度是tweet2vec的6~7倍，原因是从word变成character输入意味着GRU的输入量变大。<br>Character level可以用在NLP的很多方面，比如named entity recognition, POS tagging, text classification, and language modeling。</p>
<blockquote><br>文章highlight了tweet2vec的优势，对word segmentation error, spelling mistakes, special characters, interpret emojis, in-vocab tokens非常有效。<br></blockquote>

<p><a href="http://soroush.mit.edu/publications/tweet2vec_vvr.pdf" target="_blank" rel="external">Published paper</a>.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;SIGIR’16的文章，来自MIT Media Lab。Vosoughi, Soroush, Prashanth Vijayaraghavan, and Deb Roy. “Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM Encoder-Decoder.”&lt;/p&gt;
    
    </summary>
    
      <category term="papers" scheme="http://cuiyungao.github.io/categories/papers/"/>
    
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Impactful ML Topics at ICML&#39;16</title>
    <link href="http://cuiyungao.github.io/2016/07/04/memory-network/"/>
    <id>http://cuiyungao.github.io/2016/07/04/memory-network/</id>
    <published>2016-07-04T07:32:54.000Z</published>
    <updated>2016-08-03T03:44:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>Summary from a talk on ICML’16.</p>
<a id="more"></a>
<h3 id="Deep-Residual-Networks"><a href="#Deep-Residual-Networks" class="headerlink" title="Deep Residual Networks"></a><center><strong>Deep Residual Networks</strong></center></h3><p>Residual networks可以在深度增加的同时提高准确率，可以产生许多问题的representations。适当的weight initialization和batch normalization使得网络可以。</p>
<h3 id="Memory-Networks-for-Language-Understanding"><a href="#Memory-Networks-for-Language-Understanding" class="headerlink" title="Memory Networks for Language Understanding"></a><center><strong>Memory Networks for Language Understanding</strong></center></h3><p><code>Memory Networks</code>是结合large memory跟可读可写的learning component的一类模型。结合了<code>Reasoning</code> with <code>attention</code> over <code>memory</code> (RAM)。大部分ML有limited memory，针对”low level” tasks，比如object detection。</p>
<p>得到word Embedding的过程是一个<code>encoding</code>的过程。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Summary from a talk on ICML’16.&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Find Researchers Here</title>
    <link href="http://cuiyungao.github.io/2016/07/04/researcher/"/>
    <id>http://cuiyungao.github.io/2016/07/04/researcher/</id>
    <published>2016-07-04T07:27:10.000Z</published>
    <updated>2016-07-04T07:32:39.000Z</updated>
    
    <content type="html"><![CDATA[<p>With researchers in some fields related to NLP and deep learning contained.</p>
<a id="more"></a>
<h3 id="Natural-Language-Processing"><a href="#Natural-Language-Processing" class="headerlink" title="Natural Language Processing"></a><center><strong>Natural Language Processing</strong></center></h3><ul>
<li>Jason Weston<br><a href="http://www.thespermwhale.com/jaseweston/" target="_blank" rel="external">Link.</a> Research Scientist at Facebook.</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;With researchers in some fields related to NLP and deep learning contained.&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Predicting Amazon Ratings Using Neural Networks</title>
    <link href="http://cuiyungao.github.io/2016/06/30/predictingamazon/"/>
    <id>http://cuiyungao.github.io/2016/06/30/predictingamazon/</id>
    <published>2016-06-30T09:41:17.000Z</published>
    <updated>2016-06-30T09:51:31.000Z</updated>
    
    <content type="html"><![CDATA[<p>The paper compares 3 different methods, bag-of-words model, word2vec (using RandomForests) and word2vec (using K Nearest Neighbors).</p>
<a id="more"></a>
<p>Word vectors经常被用作features应用于许多NLP领域，有两种模型CBOW和Skip-Gram。<code>Doc2vec</code>是word2vec的改进版，区别在于<code>model architecture</code>。在<code>doc2vec</code>中，algorithms是distributed memory (dm) and distributed bag of word (dbow)，但是其分类效果并不如word2vec。</p>
<p>结果显示word2vec的准确率是最高的，而且运行时间最短。</p>
<p>Original paper can be download <a href="http://cseweb.ucsd.edu/~jmcauley/cse190/reports/sp15/014.pdf" target="_blank" rel="external">here</a>.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The paper compares 3 different methods, bag-of-words model, word2vec (using RandomForests) and word2vec (using K Nearest Neighbors).&lt;/p&gt;
    
    </summary>
    
      <category term="papers" scheme="http://cuiyungao.github.io/categories/papers/"/>
    
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
      <category term="Rating Prediction" scheme="http://cuiyungao.github.io/tags/Rating-Prediction/"/>
    
  </entry>
  
  <entry>
    <title>User Modeling with Neural Network for Review Rating Prediction</title>
    <link href="http://cuiyungao.github.io/2016/06/30/usermodeling/"/>
    <id>http://cuiyungao.github.io/2016/06/30/usermodeling/</id>
    <published>2016-06-30T03:38:13.000Z</published>
    <updated>2016-06-30T09:41:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>[IJCAI’15] User Modeling with Neural Network for Review Rating Prediction. <a href="http://ir.hit.edu.cn/~dytang/" target="_blank" rel="external">Duyu Tang</a>, Bing Qin, Ting Liu<br>Proceeding of The 24th International Joint Conference on Artificial Intelligence.</p>
<a id="more"></a>
<p>作者Duyu Tang的分析集中于sentiment analysis，一年发了很多NLP的顶会文章（EMNLP, ACL, IJCAI, etc.）。</p>
<p>本文通过考虑user information来改善review rating prediction。启发源于lexical composition model，将<code>compositional modifier</code>作为一个matrix, 用<code>matrix-vector mulplication</code>作为compositon function。本文延伸lexical semantic compositio models，建立了<code>user-word composition vector model</code> (UWCVM)，并将结果作为feature用supervised learning framework来进行review rating prediction。</p>
<h3 id="Problem-Definition"><a href="#Problem-Definition" class="headerlink" title="Problem Definition"></a><center><strong>Problem Definition</strong></center></h3><p>Review rating prediction可以看做是a classificaiton/regression problem，由Pang and Lee <a href="http://www.cs.cornell.edu/home/llee/papers/pang-lee-stars.pdf" target="_blank" rel="external">2005</a>最开始做，采用<code>metric labeling</code> framework。Rating的performance很大程度上依赖于<code>feature representation</code>的选择。</p>
<p>Problem: Given a review $r_{k_j}$ comprised of $n$ words $\{w_1,w_2,…,w_n\}$ written by user $u_k$ as input, review rating prediction aims at infering the numeric rating (1~4 or 1~5 stars) of $r_{k_j}$. The problem can be regarded as a multi-class classification problem by inferring a discrete rating score.</p>
<h3 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a><center><strong>Methodology</strong></center></h3><p>The method includes two <code>composition models</code>, the user-word composition vector model (<strong>DWCVM</strong>) and the document composition vector model (<strong>DCVM</strong>). The former model将原始的word vectors加入user information，the latter model则将modified word vectors转化成review representation，并用其作为feature来进行rating prediction。Training的过程采用Pang and Lee提出的<code>supervised metric labeling</code>的方法。整个过程如下图表示：<br><br></p>
<center><img src="/img/papers/usermodeling.png" width="50%"></center>

<h4 id="User-Word-Composition-Vector-Model"><a href="#User-Word-Composition-Vector-Model" class="headerlink" title="User-Word Composition Vector Model"></a>User-Word Composition Vector Model</h4><p>整个model将original word vectors融入user information。融入的方法有两种：<code>additive</code>和<code>multiplicative</code>。给定两个向量$v_1$和$v_2$，<strong>Additive</strong>的结合方式认为输出向量$p$是a linear function of Cartesian product of $v_1$ and $v_2$，如下：</p>
<p>$$<br>p = \mathbf{A}\times v_1 + \mathbf{B}\times v_2<br>$$<br>其中$\mathbf{A}$和$\mathbf{B}$是matrices parameters，用来encode $v_1$, $v_2$对$p$的贡献。<strong>Multiplicative</strong>的结合方式认为输出向量$p$是a linear function of the tensor product of $v_1$ and $v_2$，如下：</p>
<p>$$<br>p = \mathbf{T}\times v_1 \times v_2 = \mathbf{U}_1 \times v_2<br>$$<br>其中$\mathbf{T}$是一个rank为3的tensor，将$v_1,v_2$的tensor product映射到$p$上。$\mathbf{T}$和$\v_1$的Partial product可以被看做生成新的矩阵$\mathbf{U}_1$，这个矩阵可以modify原始的word vectors。这两种结合方式可以用下图表示：<br><br></p>
<center><img src="/img/papers/usermodeling2.png" width="50%"></center>

<p>文章选用<code>multiplicative composition</code>，因为这种结合方式符合最初的用user information来改善word vectors的想法。后面的实验也验证了multiplicative composition的结合方式准确率要高于additive composition。</p>
<p>为了减少parameter size，user representation被用low-rank plus diagonal approximation来表示：$\mathbf{U}_k=\mathbf{U}_{k1} \times \mathbf{U}_{k2} + diag(u^{\prime})$，其中$\mathbf{U}_{k1}\in \mathbb{R}^{d\times r}$, $\mathbf{U}_{k2}\in \mathbb{R}^{r\times d}$, $u^{\prime}\in \mathbb{R}^d$。$u^{\prime}$是每个user共享的background representation，以应对某些在test set中有而在training set中没有的users，即<code>Out-Of-Vocabulary</code> situation。最终modified word vectors $p_i$：</p>
<p>$$<br>p_i = tanh(\mathbf{e}_{ik}) = tanh(\mathbf{U}_k\times \mathbf{e}_i)<br>\; = tanh((\mathbf{U}_{k1}\times \mathbf{U}_{k2} + diag(u^{\prime})) \times \mathbf{e}_i)<br>$$</p>
<h4 id="Document-Composition-Vector-Model"><a href="#Document-Composition-Vector-Model" class="headerlink" title="Document Composition Vector Model"></a>Document Composition Vector Model</h4><p>文中采用一个简单而有效的方法<a href="http://anthology.aclweb.org/P/P14/P14-1006.pdf" target="_blank" rel="external">paper</a>, recursivley uses <code>biTanh</code> function来生成document representation:</p>
<p>$$<br>biTanh(p) = \sum_{i=1}^n tanh(p_{i-1} + p_i)<br>$$<br>Each sentence将user-modified word vectors作为输入，得到sentence vectors；然后再将sentence vectors输入到<code>biTanh</code>得到最终的document vector $vec(doc)$。<code>biTanh</code>的Recursive use可以看做是two pairs of bag-of-word convolutional neural network，其中window size是2，parameters通过<code>addition</code>和<code>tanh</code>来定义。</p>
<h4 id="Rating-Prediction-with-Metric-Labeling"><a href="#Rating-Prediction-with-Metric-Labeling" class="headerlink" title="Rating Prediction with Metric Labeling"></a>Rating Prediction with Metric Labeling</h4><p>Review representation被用来进行review rating prediction，方法是<code>metric labeling</code> framework。</p>
<h4 id="Model-Training"><a href="#Model-Training" class="headerlink" title="Model Training"></a>Model Training</h4><p>获取back-propagation中对于whole set of parameters的derivative of the loss, 然后用stochastic gradient descent with mini-batch来更新这些parameters。文中用<code>dropout</code>来避免neural network being over-fitting。</p>
<p><strong>Note:</strong> Cases that rating does not match with review texts is not considered. [Zhang et al., SIGIR’14]  <a href="http://dl.acm.org/citation.cfm?id=2609501" target="_blank" rel="external">Paper</a></p>
<h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a><center><strong>Experiment</strong></center></h3><p>实验数据集有两个：<code>Yelp13</code>和<code>RT05</code>。实验结果说明:</p>
<ul>
<li>Semantic composition可以提高预测的准确度，并且结合<span style="color:white">metric labeling</span>可以改善结果，因为它基于”similar items, similar labels”的idea。</li>
<li>一个用户有越多的reviews，则这个用户的rating可以被很好地估计。</li>
<li>SSPE [Tang et al., 2014a]  <a href="http://anthology.aclweb.org/C/C14/C14-1018.pdf" target="_blank" rel="external">paper</a>更适用于short review.</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[IJCAI’15] User Modeling with Neural Network for Review Rating Prediction. &lt;a href=&quot;http://ir.hit.edu.cn/~dytang/&quot;&gt;Duyu Tang&lt;/a&gt;, Bing Qin, Ting Liu&lt;br&gt;Proceeding of The 24th International Joint Conference on Artificial Intelligence.&lt;/p&gt;
    
    </summary>
    
      <category term="papers" scheme="http://cuiyungao.github.io/categories/papers/"/>
    
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
      <category term="Rating Prediction" scheme="http://cuiyungao.github.io/tags/Rating-Prediction/"/>
    
      <category term="Neural Network" scheme="http://cuiyungao.github.io/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>UFLDL Tutorial</title>
    <link href="http://cuiyungao.github.io/2016/06/29/UFLDL/"/>
    <id>http://cuiyungao.github.io/2016/06/29/UFLDL/</id>
    <published>2016-06-29T14:39:40.000Z</published>
    <updated>2016-06-29T14:43:23.000Z</updated>
    
    <content type="html"><![CDATA[<p>The page records the knowledge learnt from UFLDL tutorial provided by Stanford. The resource can be found <a href="http://ufldl.stanford.edu/tutorial/" target="_blank" rel="external">here</a>.</p>
<a id="more"></a>
<p>The tutorial主要介绍unsupervised feature learning和deep learning，以及如何实现相关的算法。由于算法实现主要是matlab，所以主要记录里面的知识要点。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The page records the knowledge learnt from UFLDL tutorial provided by Stanford. The resource can be found &lt;a href=&quot;http://ufldl.stanford.edu/tutorial/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Collaborative Filtering (CF)</title>
    <link href="http://cuiyungao.github.io/2016/06/29/cf/"/>
    <id>http://cuiyungao.github.io/2016/06/29/cf/</id>
    <published>2016-06-29T09:22:55.000Z</published>
    <updated>2016-07-19T02:31:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>This webpage contains information and implementation related to collaborative filtering.</p>
<a id="more"></a>
<p>对于recommender system，最常用的两个方法是<code>Content-Based</code>和<code>Collaborative Filtering (CF)</code>。CF基于users对items的态度来推荐items，即<code>&quot;wisdom of the crows&quot;</code>。相反，CB关注items的属性，用items之间的相似度来进行推荐。</p>
<p>CF分为<code>Memory-based CF</code>和<code>Model-based CF</code>。</p>
<h3 id="Implementation-Steps"><a href="#Implementation-Steps" class="headerlink" title="Implementation Steps"></a><center><strong>Implementation Steps</strong></center></h3><p>We use <code>scikit-learn</code> library将数据分成testing set和training set。For example,</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">import pandas as pd</div><div class="line">from sklearn import cross_validation as cv</div><div class="line">header = [&apos;user_id&apos;,&apos;item_id&apos;,&apos;rating&apos;,&apos;timestamp&apos;]</div><div class="line">data = pd.read_csv(&apos;ml-100k/u.data&apos;, sep=&apos;\t&apos;, names=header)</div><div class="line">train_data, test_data = cv.train_test_split(data, test_size=0.25)</div></pre></td></tr></table></figure>
<p>基于<code>cosine similarity</code>，我们得到<code>user_similarity</code>和<code>item_similarity</code>，对于<code>user-based CF</code>,我们用下面的公式：</p>
<p>$$<br>\hat{x}_{k,m} = \bar{x}_k + \frac{\sum_{u_a} sim_u(u_k, u_a)(x_{a,m}-\bar{x}_{u_a})}{\sum_{u_a} \|sim_u(u_k, u_a)\|}<br>$$</p>
<p>Users $k$ and $a$可以被当做是weights。Normalize确保评分在1~5之间。由于有相似taste的两个users可能在评分上一个偏高，一个偏低，所以前面需要加上user的平均rating作为bias。对于<code>item-based CF</code>,公式如下：</p>
<p>$$<br>\hat{x}_{k,m} = \frac{\sum_{i_b} sim_i(i_m,i_b)(x_{k,b})}{\sum_{i_b} \|sim_i(i_m,i_b)\|}<br>$$</p>
<h3 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a><center><strong>Evaluation</strong></center></h3><p>Metric采用<code>Root Mean Squared Error (RMSE)</code>,公式如下：</p>
<p>$$<br>RMSE = \sqrt{\frac{1}{N}\sum (x_i-\hat{x}_i)^2}<br>$$</p>
<p>实现采用<code>sklearn</code>提供的<code>mean_square_error</code> (MSE), RMSE是MSE的平方根。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">from sklearn.metrics import mean_squared_error</div><div class="line">from math import sqrt</div><div class="line">def rmse(prediction, ground_truth):</div><div class="line">  # find nonzeros places in groundtruth and extract from the prediction accordingly</div><div class="line">  prediction = prediction[ground_truth.nonzero()].flatten() </div><div class="line">  ground_truth = ground_truth[ground_truth.nonzero()].flatten()</div><div class="line">  return sqrt(mean_squared_error(prediction, ground_truth))</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">print &apos;User-based CF RMSE: &apos; + str(rmse(user_prediction, test_data_matrix))</div><div class="line">print &apos;Item-based CF RMSE: &apos; + str(rmse(item_prediction, test_data_matrix))</div></pre></td></tr></table></figure>
<p><span style="color:red">Drawback:</span> Memory-based CF doesn’t scale to real-world scenarios and cannot solve cold-start problem well. <code>Model-based</code> CF methods are scalabel and can deal with higher sparsity level than memory-based, but also suffers from cold-start problem.</p>
<p>Original <a href="http://online.cambridgecoding.com/notebooks/eWReNYcAfB/implementing-your-own-recommender-systems-in-python-2" target="_blank" rel="external">Link</a>. Also implemented locally.</p>
<h3 id="Recosystem"><a href="#Recosystem" class="headerlink" title="Recosystem"></a>Recosystem</h3><p>Recommender system using parallel matrix factorization <a href="https://cran.r-project.org/web/packages/recosystem/index.html" target="_blank" rel="external">Link.</a>. The author’s <a href="http://statr.me/2016/07/recommender-system-using-parallel-matrix-factorization/" target="_blank" rel="external">intro</a>.</p>
<h3 id="Fast-Recommendation-for-Activity-Stream"><a href="#Fast-Recommendation-for-Activity-Stream" class="headerlink" title="Fast Recommendation for Activity Stream"></a>Fast Recommendation for Activity Stream</h3><p>Fast Recommendations for Activity Streams Using Vowpal Wabbit <a href="http://blog.getstream.io/fast-recommendations-for-activity-streams-using-vowpal-wabbit" target="_blank" rel="external">Link</a>.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This webpage contains information and implementation related to collaborative filtering.&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="Recommend System" scheme="http://cuiyungao.github.io/tags/Recommend-System/"/>
    
  </entry>
  
  <entry>
    <title>Good-Turing Estimation 大数据处理平滑算法</title>
    <link href="http://cuiyungao.github.io/2016/06/29/goodturing/"/>
    <id>http://cuiyungao.github.io/2016/06/29/goodturing/</id>
    <published>2016-06-29T09:22:55.000Z</published>
    <updated>2016-08-03T04:34:19.000Z</updated>
    
    <content type="html"><![CDATA[<p>参数平滑算法</p>
<a id="more"></a>
<p>参数平滑算法，是在训练数据不足时，采用某种方式对统计结果和概率估计进行必要的调整和修补，以降低由于数据稀疏现象带来的统计误差。</p>
<p>在实际应用中，数据稀疏会产生大量空值，影响后续处理的性能和效果，所以需要平滑算法。</p>
<p><code>Good-Turing</code>估计经常用在数据平滑方面。基本思想是：将统计参数按出现次数聚类，然后用出现次数加1的类来估计当前类。假定，一个元素在文本$W$中出现$r$次的概率是$\theta(r)$，$N_r$表示元素在$W$中正好出现$r$次的个数，即$N_r=|{x_j: \sharp(x_j)=r}|$，满足：</p>
<p>$$<br>N = \sum_{r} rN_r.<br>$$</p>
<p>Good-Turing估计$\theta(r)$为：$\hat{\theta}(r) = \frac{1}{N} (r+1) \frac{N_{r+1}}{N_r}$.</p>
<p>具体的分析过程可以看<a href="http://mp.weixin.qq.com/s?__biz=MjM5ODIzNDQ3Mw==&amp;mid=2649965669&amp;idx=1&amp;sn=297fa2d2b48c4a46135aeaa73d2d437a&amp;scene=0#wechat_redirect" target="_blank" rel="external">这里</a>。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;参数平滑算法&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="Data Mining" scheme="http://cuiyungao.github.io/tags/Data-Mining/"/>
    
  </entry>
  
  <entry>
    <title>Memorization and Exploration in Recurrent Neural Language Models</title>
    <link href="http://cuiyungao.github.io/2016/06/28/memorization/"/>
    <id>http://cuiyungao.github.io/2016/06/28/memorization/</id>
    <published>2016-06-28T14:27:33.000Z</published>
    <updated>2016-06-28T14:38:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>Deep Learning | Los Angeles Meetup</p>
<a id="more"></a>
<p>The speaker explains <code>recurrent memory networks</code>.<a href="https://www.youtube.com/watch?v=nPUW0akp5s0" target="_blank" rel="external">Link</a></p>
<h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><ul>
<li>LSTM is a powerful sequential model.</li>
<li>It has complicated design of input gate, forget gate, update gate and cell.</li>
<li>LSTM achieves SOTA in many NLP tasks: sentence completion, sentiment analysis, parsing….</li>
</ul>
<h3 id="Recurrent-Memory-Networks"><a href="#Recurrent-Memory-Networks" class="headerlink" title="Recurrent Memory Networks"></a>Recurrent Memory Networks</h3><ol>
<li><p>amplify the power of RNN</p>
</li>
<li><p>offer a way to interpret and discover <code>dependencies</code> in data</p>
</li>
<li><p>outperform TreeLSTM models that explicitly use syntactic information in Sentence Completion Challenge</p>
</li>
</ol>
<h3 id="First-Attempt"><a href="#First-Attempt" class="headerlink" title="First Attempt"></a>First Attempt</h3><p>Combine linearly source context vector, target context (Memory block) and target hidden state LSTM</p>
<ol>
<li><p>Did not see any gain in BLEU</p>
</li>
<li><p>Potentially bias toward target LM</p>
</li>
<li><p><code>Gating combination</code> might be essential for the model</p>
</li>
</ol>
<h3 id="Better-Memorization"><a href="#Better-Memorization" class="headerlink" title="Better Memorization"></a>Better Memorization</h3><p>Better memorization leads to better translation</p>
<ol>
<li>LSTM does not offer any representation advantage compared to vanilla RNN</li>
<li>Gradient in LSTM just flows nicer</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Deep Learning | Los Angeles Meetup&lt;/p&gt;
    
    </summary>
    
      <category term="daily" scheme="http://cuiyungao.github.io/categories/daily/"/>
    
    
      <category term="NLP" scheme="http://cuiyungao.github.io/tags/NLP/"/>
    
      <category term="Deep Learning" scheme="http://cuiyungao.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
</feed>
